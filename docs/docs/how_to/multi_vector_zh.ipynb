{"cells": [{"cell_type": "markdown", "id": "d9172545", "metadata": {}, "source": ["# 如何通过每个文档的多个向量进行检索", "\n", "为每个文档存储多个[向量](/docs/concepts/vectorstores/)通常非常实用。这种做法在多种应用场景中都具有优势。例如，我们可以对文档的多个片段进行[嵌入](/docs/concepts/embedding_models/)，并将这些嵌入向量与父文档关联起来，这样当[检索器](/docs/concepts/retrievers/)命中某个片段时，就能返回完整的文档。", "\n", "LangChain 实现了一个基础版 [MultiVectorRetriever](https://python.langchain.com/api_reference/langchain/retrievers/langchain.retrievers.multi_vector.MultiVectorRetriever.html)，该检索器能简化这一流程。大部分复杂性在于如何为每个文档创建多重向量。本笔记本将介绍几种常见的向量生成方法及 `MultiVectorRetriever` 的使用方式。", "\n", "为每个文档创建多个向量的方法包括：", "\n", "- 更小的分块：将文档分割成更小的块，并对这些块进行嵌入（这是 [ParentDocumentRetriever](https://python.langchain.com/api_reference/langchain/retrievers/langchain_retrievers_parent_document_retriever_ParentDocumentRetriever.html)）。", "- 摘要：为每份文档创建摘要，并将该摘要与文档一同嵌入（或替代文档）。", "- 假设性问题：创建每个文档适合回答的假设性问题，将这些问題嵌入文档中（或替代文档）。", "\n", "请注意，这也启用了另一种添加嵌入的方法——手动添加。这种方法很有用，因为你可以明确添加应该导致文档被检索到的问题或查询，从而获得更多控制权。", "\n", "以下我们通过一个示例进行说明。首先，我们实例化一些文档。我们将使用[OpenAI](https://python.langchain.com/docs/integrations/text_embedding/openai/)嵌入模型将它们索引到内存中的[Chroma](/docs/integrations/providers/chroma/)向量存储中，但任何LangChain向量存储或嵌入模型均可适用。"]}, {"cell_type": "code", "execution_count": null, "id": "09cecd95-3499-465a-895a-944627ffb77f", "metadata": {}, "outputs": [], "source": ["%pip install --upgrade --quiet  langchain-chroma langchain langchain-openai > /dev/null"]}, {"cell_type": "code", "execution_count": 1, "id": "18c1421a", "metadata": {}, "outputs": [], "source": ["from langchain.storage import InMemoryByteStore\n", "from langchain_chroma import Chroma\n", "from langchain_community.document_loaders import TextLoader\n", "from langchain_openai import OpenAIEmbeddings\n", "from langchain_text_splitters import RecursiveCharacterTextSplitter\n", "\n", "loaders = [\n", "    TextLoader(\"paul_graham_essay.txt\"),\n", "    TextLoader(\"state_of_the_union.txt\"),\n", "]\n", "docs = []\n", "for loader in loaders:\n", "    docs.extend(loader.load())\n", "text_splitter = RecursiveCharacterTextSplitter(chunk_size=10000)\n", "docs = text_splitter.split_documents(docs)\n", "\n", "# The vectorstore to use to index the child chunks\n", "vectorstore = Chroma(\n", "    collection_name=\"full_documents\", embedding_function=OpenAIEmbeddings()\n", ")"]}, {"cell_type": "markdown", "id": "fa17beda", "metadata": {}, "source": ["## 更小的分块", "\n", "很多时候，检索较大的信息块但嵌入较小的信息块会很有用。这样可以让嵌入尽可能准确地捕捉语义含义，同时又能将尽可能多的上下文传递给下游。请注意，这正是 [ParentDocumentRetriever](https://python.langchain.com/api_reference/langchain/retrievers/langchain.retrievers.parent_document_retriever.ParentDocumentRetriever.html) 的功能。下面我们将展示其背后的实现原理。", "\n", "我们将对向量存储和文档存储进行区分：  \n\n- **向量存储**：负责索引（子）文档的嵌入向量。  \n- **文档存储**：存储“父”文档，并将其与标识符关联起来。"]}, {"cell_type": "code", "execution_count": 2, "id": "0e7b6b45", "metadata": {}, "outputs": [], "source": ["import uuid\n", "\n", "from langchain.retrievers.multi_vector import MultiVectorRetriever\n", "\n", "# The storage layer for the parent documents\n", "store = InMemoryByteStore()\n", "id_key = \"doc_id\"\n", "\n", "# The retriever (empty to start)\n", "retriever = MultiVectorRetriever(\n", "    vectorstore=vectorstore,\n", "    byte_store=store,\n", "    id_key=id_key,\n", ")\n", "\n", "doc_ids = [str(uuid.uuid4()) for _ in docs]"]}, {"cell_type": "markdown", "id": "d4feded4-856a-4282-91c3-53aabc62e6ff", "metadata": {}, "source": ["接下来，我们通过分割原始文档生成“子”文档。请注意，我们将文档标识符存储在对应 [Document](https://python.langchain.com/api_reference/core/documents/langchain_core.documents.base.Document.html) 对象的 `metadata` 中。"]}, {"cell_type": "code", "execution_count": 3, "id": "5d23247d", "metadata": {}, "outputs": [], "source": ["# The splitter to use to create smaller chunks\n", "child_text_splitter = RecursiveCharacterTextSplitter(chunk_size=400)\n", "\n", "sub_docs = []\n", "for i, doc in enumerate(docs):\n", "    _id = doc_ids[i]\n", "    _sub_docs = child_text_splitter.split_documents([doc])\n", "    for _doc in _sub_docs:\n", "        _doc.metadata[id_key] = _id\n", "    sub_docs.extend(_sub_docs)"]}, {"cell_type": "markdown", "id": "8e0634f8-90d5-4250-981a-5257c8a6d455", "metadata": {}, "source": ["最后，我们在向量存储和文档存储中对文档建立索引："]}, {"cell_type": "code", "execution_count": 4, "id": "92ed5861", "metadata": {}, "outputs": [], "source": ["retriever.vectorstore.add_documents(sub_docs)\n", "retriever.docstore.mset(list(zip(doc_ids, docs)))"]}, {"cell_type": "markdown", "id": "14c48c6d-850c-4317-9b6e-1ade92f2f710", "metadata": {}, "source": ["仅凭向量存储将检索出小片段："]}, {"cell_type": "code", "execution_count": 5, "id": "8afed60c", "metadata": {}, "outputs": [{"data": {"text/plain": ["Document(page_content='Tonight, I’d like to honor someone who has dedicated his life to serve this country: Justice Stephen Breyer—an Army veteran, Constitutional scholar, and retiring Justice of the United States Supreme Court. Justice Breyer, thank you for your service. \\n\\nOne of the most serious constitutional responsibilities a President has is nominating someone to serve on the United States Supreme Court.', metadata={'doc_id': '064eca46-a4c4-4789-8e3b-583f9597e54f', 'source': 'state_of_the_union.txt'})"]}, "execution_count": 5, "metadata": {}, "output_type": "execute_result"}], "source": ["retriever.vectorstore.similarity_search(\"justice breyer\")[0]"]}, {"cell_type": "markdown", "id": "717097c7-61d9-4306-8625-ef8f1940c127", "metadata": {}, "source": ["而检索器将返回更大的父文档："]}, {"cell_type": "code", "execution_count": 6, "id": "3c9017f1", "metadata": {}, "outputs": [{"data": {"text/plain": ["9875"]}, "execution_count": 6, "metadata": {}, "output_type": "execute_result"}], "source": ["len(retriever.invoke(\"justice breyer\")[0].page_content)"]}, {"cell_type": "markdown", "id": "cdef8339-f9fa-4b3b-955f-ad9dbdf2734f", "metadata": {}, "source": ["检索器在向量数据库上执行的默认搜索类型是相似性搜索。LangChain向量存储还支持通过[最大边际相关性](https://python.langchain.com/api_reference/core/vectorstores/langchain_core.vectorstores.base.VectorStore.html#langchain_core.vectorstores.base.VectorStore.max_marginal_relevance_search)进行搜索。这可以通过检索器的`search_type`参数进行控制："]}, {"cell_type": "code", "execution_count": 7, "id": "36739460-a737-4a8e-b70f-50bf8c8eaae7", "metadata": {}, "outputs": [{"data": {"text/plain": ["9875"]}, "execution_count": 7, "metadata": {}, "output_type": "execute_result"}], "source": ["from langchain.retrievers.multi_vector import SearchType\n", "\n", "retriever.search_type = SearchType.mmr\n", "\n", "len(retriever.invoke(\"justice breyer\")[0].page_content)"]}, {"cell_type": "markdown", "id": "d6a7ae0d", "metadata": {}, "source": ["## 将摘要与文档关联以便检索", "\n", "摘要或许能更准确地提炼出一个文本块的核心内容，从而提升检索效果。以下我们将展示如何创建摘要，并对这些摘要进行嵌入处理。", "\n", "我们构建了一个简单的[链](/docs/how_to/sequence)，该链将接收输入[文档](https://python.langchain.com/api_reference/core/documents/langchain_core.documents.base.Document.html)对象，并使用大语言模型生成摘要。", "\n", "import ChatModelTabs from \"@theme/ChatModelTabs\";", "\n", "<ChatModelTabs customVarName=\"llm\" />\n"]}, {"cell_type": "code", "execution_count": 8, "id": "6589291f-55bb-4e9a-b4ff-08f2506ed641", "metadata": {}, "outputs": [], "source": ["# | output: false\n", "# | echo: false\n", "\n", "from langchain_openai import ChatOpenAI\n", "\n", "llm = ChatOpenAI()"]}, {"cell_type": "code", "execution_count": 9, "id": "1433dff4", "metadata": {}, "outputs": [], "source": ["import uuid\n", "\n", "from langchain_core.documents import Document\n", "from langchain_core.output_parsers import StrOutputParser\n", "from langchain_core.prompts import ChatPromptTemplate\n", "\n", "chain = (\n", "    {\"doc\": lambda x: x.page_content}\n", "    | ChatPromptTemplate.from_template(\"Summarize the following document:\\n\\n{doc}\")\n", "    | llm\n", "    | StrOutputParser()\n", ")"]}, {"cell_type": "markdown", "id": "3faa9fde-1b09-4849-a815-8b2e89c30a02", "metadata": {}, "source": ["请注意，我们可以[批量处理](https://python.langchain.com/api_reference/core/runnables/langchain_core.runnables.base.Runnable.html#langchain_core.runnables.base.Runnable)跨文档的链："]}, {"cell_type": "code", "execution_count": 10, "id": "41a2a738", "metadata": {}, "outputs": [], "source": ["summaries = chain.batch(docs, {\"max_concurrency\": 5})"]}, {"cell_type": "markdown", "id": "73ef599e-140b-4905-8b62-6c52cdde1852", "metadata": {}, "source": ["随后我们可以像之前一样初始化一个`MultiVectorRetriever`，将摘要索引到向量存储中，同时将原始文档保留在文档存储中："]}, {"cell_type": "code", "execution_count": 11, "id": "7ac5e4b1", "metadata": {}, "outputs": [], "source": ["# The vectorstore to use to index the child chunks\n", "vectorstore = Chroma(collection_name=\"summaries\", embedding_function=OpenAIEmbeddings())\n", "# The storage layer for the parent documents\n", "store = InMemoryByteStore()\n", "id_key = \"doc_id\"\n", "# The retriever (empty to start)\n", "retriever = MultiVectorRetriever(\n", "    vectorstore=vectorstore,\n", "    byte_store=store,\n", "    id_key=id_key,\n", ")\n", "doc_ids = [str(uuid.uuid4()) for _ in docs]\n", "\n", "summary_docs = [\n", "    Document(page_content=s, metadata={id_key: doc_ids[i]})\n", "    for i, s in enumerate(summaries)\n", "]\n", "\n", "retriever.vectorstore.add_documents(summary_docs)\n", "retriever.docstore.mset(list(zip(doc_ids, docs)))"]}, {"cell_type": "code", "execution_count": 17, "id": "862ae920", "metadata": {}, "outputs": [], "source": ["# # We can also add the original chunks to the vectorstore if we so want\n", "# for i, doc in enumerate(docs):\n", "#     doc.metadata[id_key] = doc_ids[i]\n", "# retriever.vectorstore.add_documents(docs)"]}, {"cell_type": "markdown", "id": "f0274892-29c1-4616-9040-d23f9d537526", "metadata": {}, "source": ["查询向量存储将返回摘要："]}, {"cell_type": "code", "execution_count": 12, "id": "299232d6", "metadata": {}, "outputs": [{"data": {"text/plain": ["Document(page_content=\"President Biden recently nominated Judge Ketanji Brown Jackson to serve on the United States Supreme Court, emphasizing her qualifications and broad support. The President also outlined a plan to secure the border, fix the immigration system, protect women's rights, support LGBTQ+ Americans, and advance mental health services. He highlighted the importance of bipartisan unity in passing legislation, such as the Violence Against Women Act. The President also addressed supporting veterans, particularly those impacted by exposure to burn pits, and announced plans to expand benefits for veterans with respiratory cancers. Additionally, he proposed a plan to end cancer as we know it through the Cancer Moonshot initiative. President Biden expressed optimism about the future of America and emphasized the strength of the American people in overcoming challenges.\", metadata={'doc_id': '84015b1b-980e-400a-94d8-cf95d7e079bd'})"]}, "execution_count": 12, "metadata": {}, "output_type": "execute_result"}], "source": ["sub_docs = retriever.vectorstore.similarity_search(\"justice breyer\")\n", "\n", "sub_docs[0]"]}, {"cell_type": "markdown", "id": "e4f77ac5-2926-4f60-aad5-b2067900dff9", "metadata": {}, "source": ["检索器将返回更大的源文档："]}, {"cell_type": "code", "execution_count": 13, "id": "e4cce5c2", "metadata": {}, "outputs": [{"data": {"text/plain": ["9194"]}, "execution_count": 13, "metadata": {}, "output_type": "execute_result"}], "source": ["retrieved_docs = retriever.invoke(\"justice breyer\")\n", "\n", "len(retrieved_docs[0].page_content)"]}, {"cell_type": "markdown", "id": "097a5396", "metadata": {}, "source": ["## 假设性查询", "\n", "大型语言模型（LLM）还可用于生成针对特定文档的假设性问题列表，这些问题可能与[RAG](/docs/tutorials/rag)应用中的相关查询具有高度语义相似性。随后可将这些问题嵌入并与文档关联，从而提升检索效果。", "\n", "以下，我们使用 [with_structured_output](/docs/how_to/structured_output/) 方法将大语言模型的输出结构化为一组字符串列表。"]}, {"cell_type": "code", "execution_count": 16, "id": "03d85234-c33a-4a43-861d-47328e1ec2ea", "metadata": {}, "outputs": [], "source": ["from typing import List\n", "\n", "from pydantic import BaseModel, Field\n", "\n", "\n", "class HypotheticalQuestions(BaseModel):\n", "    \"\"\"Generate hypothetical questions.\"\"\"\n", "\n", "    questions: List[str] = Field(..., description=\"List of questions\")\n", "\n", "\n", "chain = (\n", "    {\"doc\": lambda x: x.page_content}\n", "    # Only asking for 3 hypothetical questions, but this could be adjusted\n", "    | ChatPromptTemplate.from_template(\n", "        \"Generate a list of exactly 3 hypothetical questions that the below document could be used to answer:\\n\\n{doc}\"\n", "    )\n", "    | ChatOpenAI(max_retries=0, model=\"gpt-4o\").with_structured_output(\n", "        HypotheticalQuestions\n", "    )\n", "    | (lambda x: x.questions)\n", ")"]}, {"cell_type": "markdown", "id": "6dddc40f-62af-413c-b944-f94a5e1f2f4e", "metadata": {}, "source": ["对单个文档调用链式处理过程表明，其输出结果为一组问题列表："]}, {"cell_type": "code", "execution_count": 17, "id": "11d30554", "metadata": {}, "outputs": [{"data": {"text/plain": ["[\"What impact did the IBM 1401 have on the author's early programming experiences?\",\n", " \"How did the transition from using the IBM 1401 to microcomputers influence the author's programming journey?\",\n", " \"What role did Lisp play in shaping the author's understanding and approach to AI?\"]"]}, "execution_count": 17, "metadata": {}, "output_type": "execute_result"}], "source": ["chain.invoke(docs[0])"]}, {"cell_type": "markdown", "id": "dcffc572-7b20-4b77-857a-90ec360a8f7e", "metadata": {}, "source": ["我们可以批量处理所有文档的链式操作，并像之前一样组装向量存储和文档存储："]}, {"cell_type": "code", "execution_count": 18, "id": "b2cd6e75", "metadata": {}, "outputs": [], "source": ["# Batch chain over documents to generate hypothetical questions\n", "hypothetical_questions = chain.batch(docs, {\"max_concurrency\": 5})\n", "\n", "\n", "# The vectorstore to use to index the child chunks\n", "vectorstore = Chroma(\n", "    collection_name=\"hypo-questions\", embedding_function=OpenAIEmbeddings()\n", ")\n", "# The storage layer for the parent documents\n", "store = InMemoryByteStore()\n", "id_key = \"doc_id\"\n", "# The retriever (empty to start)\n", "retriever = MultiVectorRetriever(\n", "    vectorstore=vectorstore,\n", "    byte_store=store,\n", "    id_key=id_key,\n", ")\n", "doc_ids = [str(uuid.uuid4()) for _ in docs]\n", "\n", "\n", "# Generate Document objects from hypothetical questions\n", "question_docs = []\n", "for i, question_list in enumerate(hypothetical_questions):\n", "    question_docs.extend(\n", "        [Document(page_content=s, metadata={id_key: doc_ids[i]}) for s in question_list]\n", "    )\n", "\n", "\n", "retriever.vectorstore.add_documents(question_docs)\n", "retriever.docstore.mset(list(zip(doc_ids, docs)))"]}, {"cell_type": "markdown", "id": "75cba8ab-a06f-4545-85fc-cf49d0204b5e", "metadata": {}, "source": ["请注意，查询底层向量存储将检索出与输入查询在语义上相似的假设性问题："]}, {"cell_type": "code", "execution_count": 19, "id": "7b442b90", "metadata": {}, "outputs": [{"data": {"text/plain": ["[Document(page_content='What might be the potential benefits of nominating Circuit Court of Appeals Judge Ketanji Brown Jackson to the United States Supreme Court?', metadata={'doc_id': '43292b74-d1b8-4200-8a8b-ea0cb57fbcdb'}),\n", " Document(page_content='How might the Bipartisan Infrastructure Law impact the economic competition between the U.S. and China?', metadata={'doc_id': '66174780-d00c-4166-9791-f0069846e734'}),\n", " Document(page_content='What factors led to the creation of Y Combinator?', metadata={'doc_id': '72003c4e-4cc9-4f09-a787-0b541a65b38c'}),\n", " Document(page_content='How did the ability to publish essays online change the landscape for writers and thinkers?', metadata={'doc_id': 'e8d2c648-f245-4bcc-b8d3-14e64a164b64'})]"]}, "execution_count": 19, "metadata": {}, "output_type": "execute_result"}], "source": ["sub_docs = retriever.vectorstore.similarity_search(\"justice breyer\")\n", "\n", "sub_docs"]}, {"cell_type": "markdown", "id": "63c32e43-5f4a-463b-a0c2-2101986f70e6", "metadata": {}, "source": ["调用检索器将返回相应的文档："]}, {"cell_type": "code", "execution_count": 20, "id": "7594b24e", "metadata": {}, "outputs": [{"data": {"text/plain": ["9194"]}, "execution_count": 20, "metadata": {}, "output_type": "execute_result"}], "source": ["retrieved_docs = retriever.invoke(\"justice breyer\")\n", "len(retrieved_docs[0].page_content)"]}], "metadata": {"kernelspec": {"display_name": "Python 3 (ipykernel)", "language": "python", "name": "python3"}, "language_info": {"codemirror_mode": {"name": "ipython", "version": 3}, "file_extension": ".py", "mimetype": "text/x-python", "name": "python", "nbconvert_exporter": "python", "pygments_lexer": "ipython3", "version": "3.10.4"}}, "nbformat": 4, "nbformat_minor": 5}