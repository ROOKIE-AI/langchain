{"cells": [{"cell_type": "raw", "id": "0bdb3b97-4989-4237-b43b-5943dbbd8302", "metadata": {"vscode": {"languageId": "raw"}}, "source": ["---\n", "keywords: [stream]\n", "---"]}, {"cell_type": "markdown", "id": "bb7d49db-04d3-4399-bfe1-09f82bbe6015", "metadata": {}, "source": ["# 如何流式运行可执行程序", "\n", ":::info 前提条件", "\n", "本指南假定您熟悉以下概念：", "- [聊天模型](/docs/concepts/chat_models)", "- [LangChain 表达式语言](/docs/concepts/lcel)", "- [输出解析器](/docs/concepts/output_parsers)", "\n", ":::", "\n", "流式传输对于让基于LLM的应用程序对终端用户具有响应感至关重要。", "\n", "重要的LangChain基础组件如[聊天模型](/docs/concepts/chat_models)、[输出解析器](/docs/concepts/output_parsers)、[提示模板](/docs/concepts/prompt_templates)、[检索器](/docs/concepts/retrievers)和[智能体](/docs/concepts/agents)均实现了LangChain的[可运行接口](/docs/concepts/runnables)。", "\n", "此接口提供了两种通用的流式内容处理方法：", "\n", "1. 同步 `stream` 与异步 `astream`：一种**默认实现**的流式传输方式，用于从链中流式传输**最终输出**。", "2. 异步 `astream_events` 与异步 `astream_log`：这些方法提供了从链中流式传输**中间步骤**和**最终输出**的能力。", "\n", "让我们来看看这两种方法，并尝试理解如何使用它们。", "\n", ":::信息", "要获取LangChain中流式技术的更高级概述，请参阅[概念指南的这一部分](/docs/concepts/streaming)。", ":::", "\n", "## 使用流", "\n", "所有 `Runnable` 对象都实现了一个名为 `stream` 的同步方法，以及一个名为 `astream` 的异步变体。", "\n", "这些方法旨在以分块形式流式传输最终输出，一旦每个块可用就立即生成。", "\n", "只有当程序中的所有步骤都知晓如何处理**输入流**时，才能实现流式处理；即每次处理一个输入数据块，并生成相应的输出数据块。", "\n", "这种处理的复杂性可能各不相同，从直接输出大语言模型生成的令牌这类简单任务，到在完整JSON生成前流式传输部分JSON结果这类更具挑战性的任务。", "\n", "探索流式处理的最佳起点是大型语言模型（LLM）应用中最核心的组件——LLM模型本身！", "\n", "### 大语言模型与聊天模型", "\n", "大型语言模型及其聊天变体是基于LLM应用的主要瓶颈。", "\n", "大型语言模型可能需要**几秒钟**才能生成对查询的完整响应。这远远慢于**约200-300毫秒**的阈值——在该响应时间内，应用程序才会让终端用户感觉反应灵敏。", "\n", "使应用程序感觉更灵敏的关键策略是展示中间进度；即**逐词**流式传输模型的输出。", "\n", "我们将展示使用聊天模型进行流式处理的示例。请从以下选项中选择一个：", "\n", "import ChatModelTabs from \"@theme/ChatModelTabs\";", "\n", "<ChatModelTabs\n", "customVarName=\"模型\"", "/>"]}, {"cell_type": "code", "execution_count": 1, "id": "f123bdcb-8c8b-440c-9bbd-aa5ed4e9cd17", "metadata": {}, "outputs": [{"name": "stdout", "output_type": "stream", "text": ["\n", "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.2.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.0\u001b[0m\n", "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython -m pip install --upgrade pip\u001b[0m\n", "Note: you may need to restart the kernel to use updated packages.\n"]}], "source": ["# | output: false\n", "# | echo: false\n", "\n", "import os\n", "from getpass import getpass\n", "\n", "keys = [\n", "    \"ANTHROPIC_API_KEY\",\n", "    \"OPENAI_API_KEY\",\n", "]\n", "\n", "for key in keys:\n", "    if key not in os.environ:\n", "        os.environ[key] = getpass(f\"Enter API Key for {key}=?\")\n", "\n", "\n", "from langchain_anthropic import ChatAnthropic\n", "\n", "model = ChatAnthropic(model=\"claude-3-sonnet-20240229\", temperature=0)"]}, {"cell_type": "markdown", "id": "a2464c57-0e89-4159-b21f-5859a21be658", "metadata": {}, "source": ["让我们从同步 `stream` API 开始："]}, {"cell_type": "code", "execution_count": 2, "id": "8b44dfb2-0749-487a-8918-f8b6b8233093", "metadata": {}, "outputs": [{"name": "stdout", "output_type": "stream", "text": ["The| sky| appears| blue| during| the| day|.|"]}], "source": ["chunks = []\n", "for chunk in model.stream(\"what color is the sky?\"):\n", "    chunks.append(chunk)\n", "    print(chunk.content, end=\"|\", flush=True)"]}, {"cell_type": "markdown", "id": "8d835b5c-cbb7-41ab-8905-bdc24d515d29", "metadata": {}, "source": ["或者，如果您在异步环境中工作，可以考虑使用异步的 `astream` API："]}, {"cell_type": "code", "execution_count": 3, "id": "f180b6a0-0027-4bd8-8bab-fde76e282609", "metadata": {}, "outputs": [{"name": "stdout", "output_type": "stream", "text": ["The| sky| appears| blue| during| the| day|.|"]}], "source": ["chunks = []\n", "async for chunk in model.astream(\"what color is the sky?\"):\n", "    chunks.append(chunk)\n", "    print(chunk.content, end=\"|\", flush=True)"]}, {"cell_type": "markdown", "id": "66730a87-77d5-40d6-a68f-315121989bd1", "metadata": {}, "source": ["让我们检查其中一个数据块"]}, {"cell_type": "code", "execution_count": 4, "id": "dade3000-1ac4-4f5c-b5c6-a0217f9f8a6b", "metadata": {}, "outputs": [{"data": {"text/plain": ["AIMessageChunk(content='The', id='run-b36bea64-5511-4d7a-b6a3-a07b3db0c8e7')"]}, "execution_count": 4, "metadata": {}, "output_type": "execute_result"}], "source": ["chunks[0]"]}, {"cell_type": "markdown", "id": "a3a47193-2bd1-46bc-9c7e-ea0f6b08c4a5", "metadata": {}, "source": ["我们得到的是一个名为 `AIMessageChunk` 的返回内容。这个块（chunk）代表 `AIMessage` 的一部分。", "\n", "消息块的设计是可叠加的——只需将它们简单相加，就能得到当前响应的状态！"]}, {"cell_type": "code", "execution_count": 5, "id": "d3cf5f38-249c-4da0-94e6-5e5203fad52e", "metadata": {}, "outputs": [{"data": {"text/plain": ["AIMessageChunk(content='The sky appears blue during', id='run-b36bea64-5511-4d7a-b6a3-a07b3db0c8e7')"]}, "execution_count": 5, "metadata": {}, "output_type": "execute_result"}], "source": ["chunks[0] + chunks[1] + chunks[2] + chunks[3] + chunks[4]"]}, {"cell_type": "markdown", "id": "59ffbd9a-3b79-44b6-8883-1371f9460c77", "metadata": {}, "source": ["### 链式结构", "\n", "几乎所有LLM应用都涉及多个步骤，而不仅仅是对语言模型的一次调用。", "\n", "让我们使用`LangChain表达式语言`(`LCEL`)构建一个简单的链，该链结合了提示词、模型和解析器，并验证流式传输是否正常工作。", "\n", "我们将使用 [`StrOutputParser`](https://python.langchain.com/api_reference/core/output_parsers/langchain_core.output_parsers.string.StrOutputParser.html) 来解析模型的输出。这是一个简单的解析器，它会从 `AIMessageChunk` 中提取 `content` 字段，从而获取模型返回的 `token`。", "\n", ":::提示", "LCEL 是一种通过将不同LangChain原语链接在一起来定义「程序」的*声明式*方法。使用LCEL创建的链能自动实现`stream`和`astream`方法，从而支持最终输出的流式传输。实际上，基于LCEL构建的链完整实现了标准Runnable接口的全部功能。", ":::"]}, {"cell_type": "code", "execution_count": 6, "id": "a8562ae2-3fd1-4829-9801-a5a732b1798d", "metadata": {}, "outputs": [{"name": "stdout", "output_type": "stream", "text": ["Here|'s| a| joke| about| a| par|rot|:|\n", "\n", "A man| goes| to| a| pet| shop| to| buy| a| par|rot|.| The| shop| owner| shows| him| two| stunning| pa|rr|ots| with| beautiful| pl|um|age|.|\n", "\n", "\"|There|'s| a| talking| par|rot| an|d a| non|-|talking| par|rot|,\"| the| owner| says|.| \"|The| talking| par|rot| costs| $|100|,| an|d the| non|-|talking| par|rot| is| $|20|.\"|\n", "\n", "The| man| says|,| \"|I|'ll| take| the| non|-|talking| par|rot| at| $|20|.\"|\n", "\n", "He| pays| an|d leaves| with| the| par|rot|.| As| he|'s| walking| down| the| street|,| the| par|rot| looks| up| at| him| an|d says|,| \"|You| know|,| you| really| are| a| stupi|d man|!\"|\n", "\n", "The| man| is| stun|ne|d an|d looks| at| the| par|rot| in| dis|bel|ief|.| The| par|rot| continues|,| \"|Yes|,| you| got| r|ippe|d off| big| time|!| I| can| talk| just| as| well| as| that| other| par|rot|,| an|d you| only| pai|d $|20| |for| me|!\"|"]}], "source": ["from langchain_core.output_parsers import StrOutputParser\n", "from langchain_core.prompts import ChatPromptTemplate\n", "\n", "prompt = ChatPromptTemplate.from_template(\"tell me a joke about {topic}\")\n", "parser = StrOutputParser()\n", "chain = prompt | model | parser\n", "\n", "async for chunk in chain.astream({\"topic\": \"parrot\"}):\n", "    print(chunk, end=\"|\", flush=True)"]}, {"cell_type": "markdown", "id": "868bc412", "metadata": {}, "source": ["请注意，尽管我们在上述链的末尾使用了 `parser`，但仍然获得了流式输出。这是因为 `parser` 会对每个流式数据块单独进行处理。许多 [LCEL 基础组件](/docs/how_to#langchain-expression-language-lcel) 也支持这种转换式直通流处理，这在构建应用程序时非常方便。", "\n", "自定义函数可以[设计为返回生成器](/docs/how_to/functions#streaming)，这些生成器能够对流进行操作。", "\n", "某些可运行对象，例如[提示模板](/docs/how_to#prompt-templates)和[聊天模型](/docs/how_to#chat-models)，无法处理单个数据块，而是会聚合所有先前的步骤。这类可运行对象可能会中断流式处理过程。"]}, {"cell_type": "markdown", "id": "1b399fb4-5e3c-4581-9570-6df9b42b623d", "metadata": {}, "source": [":::note", "LangChain表达式语言允许你将链的构建与其使用模式（如同步/异步、批处理/流处理等）分离开来。如果这与你的构建内容无关，你也可以通过标准的**命令式**编程方法来实现。", "对每个组件单独调用 `invoke`、`batch` 或 `stream`，将结果赋值给变量，然后根据需求在下游使用这些变量。", "\n", "好的,我将以标准markdown格式进行翻译,只输出具体内容:\n\n# 欢迎使用翻译助手\n\n这是一个将英文翻译成中文的助手工具。以下是使用说明:\n\n## 功能特点\n\n- 支持多种文件格式翻译\n- 保持原文格式和排版\n- 快速准确的翻译结果\n- 支持专业术语库\n\n## 使用方法\n\n1. 输入或粘贴需要翻译的英文文本\n2. 点击\"翻译\"按钮\n3. 获取准确的中文翻译结果\n\n## 注意事项\n\n- 请确保输入文本为英文\n- 复杂格式可能需要人工校对\n- 专业领域建议使用定制术语库\n\n如需帮助,请联系我们: contact@transhelper.com"]}, {"cell_type": "markdown", "id": "dfff2701-8887-486f-8b3b-eb26383d4bb6", "metadata": {}, "source": ["### 处理输入流", "\n", "如果你想在生成过程中实时流式传输JSON输出该怎么办？", "\n", "如果你依赖 `json.loads` 来解析不完整的 JSON，解析将会失败，因为不完整的 JSON 不是有效的 JSON。", "\n", "你可能会完全不知所措，声称无法流式传输JSON数据。", "\n", "原来，确实有一种方法可以实现——解析器需要操作**输入流**，并尝试将不完整的 JSON \"自动补全\"为有效状态。", "\n", "让我们通过实际操作来观察这样一个解析器，以理解其含义。"]}, {"cell_type": "code", "execution_count": 7, "id": "5ff63cce-715a-4561-951f-9321c82e8d81", "metadata": {}, "outputs": [{"name": "stdout", "output_type": "stream", "text": ["{}\n", "{'countries': []}\n", "{'countries': [{}]}\n", "{'countries': [{'name': ''}]}\n", "{'countries': [{'name': 'France'}]}\n", "{'countries': [{'name': 'France', 'population': 67}]}\n", "{'countries': [{'name': 'France', 'population': 67413}]}\n", "{'countries': [{'name': 'France', 'population': 67413000}]}\n", "{'countries': [{'name': 'France', 'population': 67413000}, {}]}\n", "{'countries': [{'name': 'France', 'population': 67413000}, {'name': ''}]}\n", "{'countries': [{'name': 'France', 'population': 67413000}, {'name': 'Spain'}]}\n", "{'countries': [{'name': 'France', 'population': 67413000}, {'name': 'Spain', 'population': 47}]}\n", "{'countries': [{'name': 'France', 'population': 67413000}, {'name': 'Spain', 'population': 47351}]}\n", "{'countries': [{'name': 'France', 'population': 67413000}, {'name': 'Spain', 'population': 47351567}]}\n", "{'countries': [{'name': 'France', 'population': 67413000}, {'name': 'Spain', 'population': 47351567}, {}]}\n", "{'countries': [{'name': 'France', 'population': 67413000}, {'name': 'Spain', 'population': 47351567}, {'name': ''}]}\n", "{'countries': [{'name': 'France', 'population': 67413000}, {'name': 'Spain', 'population': 47351567}, {'name': 'Japan'}]}\n", "{'countries': [{'name': 'France', 'population': 67413000}, {'name': 'Spain', 'population': 47351567}, {'name': 'Japan', 'population': 125}]}\n", "{'countries': [{'name': 'France', 'population': 67413000}, {'name': 'Spain', 'population': 47351567}, {'name': 'Japan', 'population': 125584}]}\n", "{'countries': [{'name': 'France', 'population': 67413000}, {'name': 'Spain', 'population': 47351567}, {'name': 'Japan', 'population': 125584000}]}\n"]}], "source": ["from langchain_core.output_parsers import JsonOutputParser\n", "\n", "chain = (\n", "    model | JsonOutputParser()\n", ")  # Due to a bug in older versions of Langchain, JsonOutputParser did not stream results from some models\n", "async for text in chain.astream(\n", "    \"output a list of the countries france, spain and japan and their populations in JSON format. \"\n", "    'Use a dict with an outer key of \"countries\" which contains a list of countries. '\n", "    \"Each country should have the key `name` and `population`\"\n", "):\n", "    print(text, flush=True)"]}, {"cell_type": "markdown", "id": "151d4323-a6cf-49be-8779-e8797c5e3b00", "metadata": {}, "source": ["现在，让我们**打破**流式传输。我们将使用之前的示例，并在最后附加一个提取函数，用于从最终确定的JSON中提取国家名称。", "\n", ":::警告", "在链式操作中，任何对**最终输入**而非**输入流**进行处理的步骤，都可能通过`stream`或`astream`破坏流式功能。", ":::", "\n", ":::提示", "稍后，我们将讨论`astream_events` API，该接口可流式传输来自中间步骤的结果。即使链中包含仅对**最终确定输入**进行操作的步骤，此API仍会持续输出中间步骤的流式结果。", "好的,我会按照要求将英文翻译成中文,并保持markdown格式一致。以下是一个示例翻译:\n\n# 欢迎使用翻译助手\n\n这是一个**markdown格式**的翻译示例。\n\n## 主要功能\n\n1. 保持原始文档结构\n2. 准确传达原文含义\n3. 符合中文表达习惯\n\n### 注意事项\n\n* 不添加额外说明\n* 保持格式一致\n* 仅输出翻译内容\n\n> 翻译需要兼顾准确性和可读性\n\n[这是一个链接示例](https://example.com)\n\n```python\n# 代码块也会原样保留\ndef example():\n    print(\"Hello World\")\n```"]}, {"cell_type": "code", "execution_count": 8, "id": "d9c90117-9faa-4a01-b484-0db071808d1f", "metadata": {}, "outputs": [{"name": "stdout", "output_type": "stream", "text": ["['France', 'Spain', 'Japan']|"]}], "source": ["from langchain_core.output_parsers import (\n", "    JsonOutputParser,\n", ")\n", "\n", "\n", "# A function that operates on finalized inputs\n", "# rather than on an input_stream\n", "def _extract_country_names(inputs):\n", "    \"\"\"A function that does not operates on input streams and breaks streaming.\"\"\"\n", "    if not isinstance(inputs, dict):\n", "        return \"\"\n", "\n", "    if \"countries\" not in inputs:\n", "        return \"\"\n", "\n", "    countries = inputs[\"countries\"]\n", "\n", "    if not isinstance(countries, list):\n", "        return \"\"\n", "\n", "    country_names = [\n", "        country.get(\"name\") for country in countries if isinstance(country, dict)\n", "    ]\n", "    return country_names\n", "\n", "\n", "chain = model | JsonOutputParser() | _extract_country_names\n", "\n", "async for text in chain.astream(\n", "    \"output a list of the countries france, spain and japan and their populations in JSON format. \"\n", "    'Use a dict with an outer key of \"countries\" which contains a list of countries. '\n", "    \"Each country should have the key `name` and `population`\"\n", "):\n", "    print(text, end=\"|\", flush=True)"]}, {"cell_type": "markdown", "id": "cab6dca2-2027-414d-a196-2db6e3ebb8a5", "metadata": {}, "source": ["#### 生成器函数", "\n", "让我们使用一个能够在**输入流**上操作的生成器函数来修复流式传输问题。", "\n", ":::提示", "生成器函数（使用 `yield` 的函数）允许编写处理**输入流**的代码", "好的，请提供需要翻译的英文文本，我会将其转换为标准的中文Markdown格式，并保持原有的排版结构。"]}, {"cell_type": "code", "execution_count": 9, "id": "15984b2b-315a-4119-945b-2a3dabea3082", "metadata": {}, "outputs": [{"name": "stdout", "output_type": "stream", "text": ["France|Spain|Japan|"]}], "source": ["from langchain_core.output_parsers import JsonOutputParser\n", "\n", "\n", "async def _extract_country_names_streaming(input_stream):\n", "    \"\"\"A function that operates on input streams.\"\"\"\n", "    country_names_so_far = set()\n", "\n", "    async for input in input_stream:\n", "        if not isinstance(input, dict):\n", "            continue\n", "\n", "        if \"countries\" not in input:\n", "            continue\n", "\n", "        countries = input[\"countries\"]\n", "\n", "        if not isinstance(countries, list):\n", "            continue\n", "\n", "        for country in countries:\n", "            name = country.get(\"name\")\n", "            if not name:\n", "                continue\n", "            if name not in country_names_so_far:\n", "                yield name\n", "                country_names_so_far.add(name)\n", "\n", "\n", "chain = model | JsonOutputParser() | _extract_country_names_streaming\n", "\n", "async for text in chain.astream(\n", "    \"output a list of the countries france, spain and japan and their populations in JSON format. \"\n", "    'Use a dict with an outer key of \"countries\" which contains a list of countries. '\n", "    \"Each country should have the key `name` and `population`\",\n", "):\n", "    print(text, end=\"|\", flush=True)"]}, {"cell_type": "markdown", "id": "d59823f5-9b9a-43c5-a213-34644e2f1d3d", "metadata": {}, "source": [":::注意", "由于上述代码依赖于JSON自动补全功能，您可能会看到国家名称的部分片段（例如`Sp`和`Spain`），而这并非提取结果所期望的！", "\n", "我们关注的是流式处理的概念，而非链条处理的结果。", "好的，请提供需要翻译的英文文本，我会将其翻译成中文并保持原有的Markdown格式。"]}, {"cell_type": "markdown", "id": "6adf65b7-aa47-4321-98c7-a0abe43b833a", "metadata": {}, "source": ["### 非流式组件", "\n", "某些内置组件（如检索器）不提供任何`流式传输`功能。如果我们尝试对它们进行`流式传输`会发生什么呢？🤨"]}, {"cell_type": "code", "execution_count": 10, "id": "b9b1c00d-8b44-40d0-9e2b-8a70d238f82b", "metadata": {}, "outputs": [{"data": {"text/plain": ["[[Document(page_content='harrison worked at kensho'),\n", "  Document(page_content='harrison likes spicy food')]]"]}, "execution_count": 10, "metadata": {}, "output_type": "execute_result"}], "source": ["from langchain_community.vectorstores import FAISS\n", "from langchain_core.output_parsers import StrOutputParser\n", "from langchain_core.prompts import ChatPromptTemplate\n", "from langchain_core.runnables import RunnablePassthrough\n", "from langchain_openai import OpenAIEmbeddings\n", "\n", "template = \"\"\"Answer the question based only on the following context:\n", "{context}\n", "\n", "Question: {question}\n", "\"\"\"\n", "prompt = ChatPromptTemplate.from_template(template)\n", "\n", "vectorstore = FAISS.from_texts(\n", "    [\"harrison worked at kensho\", \"harrison likes spicy food\"],\n", "    embedding=OpenAIEmbeddings(),\n", ")\n", "retriever = vectorstore.as_retriever()\n", "\n", "chunks = [chunk for chunk in retriever.stream(\"where did harrison work?\")]\n", "chunks"]}, {"cell_type": "markdown", "id": "6fd3e71b-439e-418f-8a8a-5232fba3d9fd", "metadata": {}, "source": ["该数据流刚刚输出了该组件的最终结果。", "\n", "这样是可以的 🥹！并非所有组件都必须实现流式传输——在某些情况下，流式传输要么没有必要，要么难以实现，要么根本不合逻辑。", "\n", ":::提示", "使用非流式组件构建的LCEL链，在许多情况下仍能实现流式输出，其部分输出的流式传输将在链中最后一个非流式步骤之后开始。", "好的, 以下是翻译成中文的markdown格式内容:\n\n# 欢迎使用翻译助手\n\n## 功能特点\n\n1. **多语言支持**: 支持多种语言的互译\n2. **格式保留**: 保持原文的markdown格式不变\n3. **快速响应**: 提供即时翻译服务\n\n## 使用方法\n\n```python\ndef translate(text):\n    # 这里是翻译函数的示例代码\n    return translated_text\n```\n\n> 注意: 请确保输入的文本是标准的markdown格式\n\n| 功能 | 描述 |\n|------|------|\n| 翻译 | 将文本从一种语言转换为另一种语言 |\n| 格式化 | 保持原有的文本格式 |\n\n[点击这里](#) 了解更多信息"]}, {"cell_type": "code", "execution_count": 11, "id": "957447e6-1e60-41ef-8c10-2654bd9e738d", "metadata": {}, "outputs": [], "source": ["retrieval_chain = (\n", "    {\n", "        \"context\": retriever.with_config(run_name=\"Docs\"),\n", "        \"question\": RunnablePassthrough(),\n", "    }\n", "    | prompt\n", "    | model\n", "    | StrOutputParser()\n", ")"]}, {"cell_type": "code", "execution_count": 12, "id": "94e50b5d-bf51-4eee-9da0-ee40dd9ce42b", "metadata": {}, "outputs": [{"name": "stdout", "output_type": "stream", "text": ["Base|d on| the| given| context|,| Harrison| worke|d at| K|ens|ho|.|\n", "\n", "Here| are| |3| |made| up| sentences| about| this| place|:|\n", "\n", "1|.| K|ens|ho| was| a| cutting|-|edge| technology| company| known| for| its| innovative| solutions| in| artificial| intelligence| an|d data| analytics|.|\n", "\n", "2|.| The| modern| office| space| at| K|ens|ho| feature|d open| floor| plans|,| collaborative| work|sp|aces|,| an|d a| vib|rant| atmosphere| that| fos|tere|d creativity| an|d team|work|.|\n", "\n", "3|.| With| its| prime| location| in| the| heart| of| the| city|,| K|ens|ho| attracte|d top| talent| from| aroun|d the| worl|d,| creating| a| diverse| an|d dynamic| work| environment|.|"]}], "source": ["for chunk in retrieval_chain.stream(\n", "    \"Where did harrison work? \" \"Write 3 made up sentences about this place.\"\n", "):\n", "    print(chunk, end=\"|\", flush=True)"]}, {"cell_type": "markdown", "id": "8657aa4e-3469-4b5b-a09c-60b53a23b1e7", "metadata": {}, "source": ["既然我们已经了解了 `stream` 和 `astream` 的运作方式，现在让我们一同探索流式事件的世界吧。🏞️"]}, {"cell_type": "markdown", "id": "baceb5c0-d4a4-4b98-8733-80ae4407b62d", "metadata": {}, "source": ["## 使用流事件", "\n", "事件流是一个**测试版**API。该API可能会根据反馈进行一些调整。", "\n", ":::note", "\n", "本指南展示的是 `V2` 版 API，要求 langchain-core 版本 ≥ 0.2。如需兼容旧版 LangChain 的 `V1` 版 API，请参阅[此处](https://python.langchain.com/v0.1/docs/expression_language/streaming/#using-stream-events)。", "好的,我会按照要求将英文翻译成中文,并保持markdown格式一致。以下是一个示例翻译:\n\n# 欢迎使用翻译助手\n\n这是一个**markdown格式**的翻译示例:\n\n## 主要功能\n1. 保持原始文档结构\n2. 保留所有格式标记\n3. 提供准确自然的翻译\n\n### 注意事项\n- 不添加额外说明文字\n- 严格遵循原文排版\n- 仅输出翻译内容\n\n> 这是引用的翻译示例  \n> 多行内容也会正确处理\n\n`代码块`和[链接](https://example.com)等元素都会保留原格式。\n\n表格示例:\n\n| 项目 | 描述 |\n|------|------|\n| 质量 | 高标准 |\n| 速度 | 快速响应 |\n\n请提供需要翻译的英文内容。"]}, {"cell_type": "code", "execution_count": null, "id": "61348df9-ec58-401e-be89-68a70042f88e", "metadata": {}, "outputs": [], "source": ["import langchain_core\n", "\n", "langchain_core.__version__"]}, {"cell_type": "markdown", "id": "52e9e983-bbde-4906-9eca-4ccc06eabd91", "metadata": {}, "source": ["为确保 `astream_events` API 正常运行：", "\n", "* 在代码中尽可能使用 `async`（例如异步工具等）", "* 在定义自定义函数/可运行对象时传播回调", "* 在不使用LCEL的情况下使用可运行对象时，请确保在LLM上调用`.astream()`而非`.ainvoke`，以强制LLM流式传输令牌。", "* 如有任何功能未按预期运行，请告知我们！ :)", "\n", "### 事件参考", "\n", "以下是参考表格，展示了各种可运行对象可能触发的事件。", "\n", "\n", ":::note", "当流式传输正确实现时，可运行对象的输入只有在输入流被完全消费后才会知晓。这意味着`inputs`通常只会在`end`事件中包含，而不会出现在`start`事件中。", ":::", "\n", "| 事件                | 名称             | 数据块                          | 输入                                          | 输出                                           |", "|----------------------|------------------|---------------------------------|-----------------------------------------------|-------------------------------------------------|", "| on_chat_model_start  | [模型名称]     |                                 | \\{\"messages\": [[系统消息, 用户消息]]\\} |                                                 |", "| on_chat_model_stream | [模型名称]     | AIMessageChunk(content=\"你好\") |                                               |                                                 |", "| on_chat_model_end    | [模型名称]     |                                 | \\{\"messages\": [[系统消息, 人工消息]]\\} | AI消息块(content=\"你好世界\")           |", "| on_llm_start         | [模型名称]     |                                 | \\{'input': '你好'\\}                            |                                                 |", "| on_llm_stream        | [模型名称]     | '你好'                         |                                               |                                                 |", "| on_llm_end           | [模型名称]     |                                 | '你好，人类！'                                |                                                 |", "| on_chain_start       | format_docs      |                                 |                                               |                                                 |", "| on_chain_stream      | format_docs      | \"你好，世界！再见，世界！\"  |                                               |                                                 |", "| on_chain_end         | format_docs      |                                 | [Document(...)]                               | \"你好，世界！再见，世界！\"                  |", "| on_tool_start        | some_tool        |                                 | \\{\"x\": 1, \"y\": \"2\"\\}                            |                                                 |", "| on_tool_end          | some_tool        |                                 |                                               | \\{\"x\": 1, \"y\": \"2\"\\}                              |", "| on_retriever_start   | [检索器名称] |                                 | \\{\"query\": \"你好\"\\}                            |                                                 |", "| on_retriever_end     | [检索器名称]     |                                 | \\{\"query\": \"你好\"\\}                             | [文档(...), ..]                                |", "| on_prompt_start      | [模板名称]       |                                 | \\{\"question\": \"你好\"\\}                          |                                                 |", "| on_prompt_end        | [模板名称]       |                                 | \\{\"question\": \"你好\"\\}                          | 聊天提示值(消息列表: [系统消息, ...])          |"]}, {"cell_type": "markdown", "id": "1f6ec135-3348-4041-8f55-bf3e59b3b2d0", "metadata": {}, "source": ["### 聊天模型", "\n", "让我们首先来看看聊天模型产生的事件。"]}, {"cell_type": "code", "execution_count": 13, "id": "bab5f910-fee0-4a94-9f05-b469006333b8", "metadata": {}, "outputs": [], "source": ["events = []\n", "async for event in model.astream_events(\"hello\"):\n", "    events.append(event)"]}, {"cell_type": "markdown", "id": "32972939-2995-4b2e-84db-045adb044fad", "metadata": {}, "source": [":::note\n注意", "\n", "对于 `langchain-core<0.3.37` 版本，需显式设置 `version` 参数（例如：`model.astream_events(\"hello\", version=\"v2\")`）。", "\n", ":::"]}, {"cell_type": "markdown", "id": "ad2b8f47-da78-4569-a49a-53a8efaa26bc", "metadata": {}, "source": ["让我们来看几个开始事件和几个结束事件。"]}, {"cell_type": "code", "execution_count": 14, "id": "c4a2f5dc-2c75-4be4-a8ca-b5b84a3cdbef", "metadata": {}, "outputs": [{"data": {"text/plain": ["[{'event': 'on_chat_model_start',\n", "  'data': {'input': 'hello'},\n", "  'name': 'ChatAnthropic',\n", "  'tags': [],\n", "  'run_id': 'b18d016d-8b9b-49e7-a555-44db498fcf66',\n", "  'metadata': {'ls_provider': 'anthropic',\n", "   'ls_model_name': 'claude-3-sonnet-20240229',\n", "   'ls_model_type': 'chat',\n", "   'ls_temperature': 0.0,\n", "   'ls_max_tokens': 1024},\n", "  'parent_ids': []},\n", " {'event': 'on_chat_model_stream',\n", "  'run_id': 'b18d016d-8b9b-49e7-a555-44db498fcf66',\n", "  'name': 'ChatAnthropic',\n", "  'tags': [],\n", "  'metadata': {'ls_provider': 'anthropic',\n", "   'ls_model_name': 'claude-3-sonnet-20240229',\n", "   'ls_model_type': 'chat',\n", "   'ls_temperature': 0.0,\n", "   'ls_max_tokens': 1024},\n", "  'data': {'chunk': AIMessageChunk(content='', additional_kwargs={}, response_metadata={}, id='run-b18d016d-8b9b-49e7-a555-44db498fcf66', usage_metadata={'input_tokens': 8, 'output_tokens': 4, 'total_tokens': 12, 'input_token_details': {'cache_creation': 0, 'cache_read': 0}})},\n", "  'parent_ids': []},\n", " {'event': 'on_chat_model_stream',\n", "  'run_id': 'b18d016d-8b9b-49e7-a555-44db498fcf66',\n", "  'name': 'ChatAnthropic',\n", "  'tags': [],\n", "  'metadata': {'ls_provider': 'anthropic',\n", "   'ls_model_name': 'claude-3-sonnet-20240229',\n", "   'ls_model_type': 'chat',\n", "   'ls_temperature': 0.0,\n", "   'ls_max_tokens': 1024},\n", "  'data': {'chunk': AIMessageChunk(content='Hello! How can', additional_kwargs={}, response_metadata={}, id='run-b18d016d-8b9b-49e7-a555-44db498fcf66')},\n", "  'parent_ids': []}]"]}, "execution_count": 14, "metadata": {}, "output_type": "execute_result"}], "source": ["events[:3]"]}, {"cell_type": "code", "execution_count": 15, "id": "76cfe826-ee63-4310-ad48-55a95eb3b9d6", "metadata": {}, "outputs": [{"data": {"text/plain": ["[{'event': 'on_chat_model_stream',\n", "  'run_id': 'b18d016d-8b9b-49e7-a555-44db498fcf66',\n", "  'name': 'ChatAnthropic',\n", "  'tags': [],\n", "  'metadata': {'ls_provider': 'anthropic',\n", "   'ls_model_name': 'claude-3-sonnet-20240229',\n", "   'ls_model_type': 'chat',\n", "   'ls_temperature': 0.0,\n", "   'ls_max_tokens': 1024},\n", "  'data': {'chunk': AIMessageChunk(content='', additional_kwargs={}, response_metadata={'stop_reason': 'end_turn', 'stop_sequence': None}, id='run-b18d016d-8b9b-49e7-a555-44db498fcf66', usage_metadata={'input_tokens': 0, 'output_tokens': 12, 'total_tokens': 12, 'input_token_details': {}})},\n", "  'parent_ids': []},\n", " {'event': 'on_chat_model_end',\n", "  'data': {'output': AIMessageChunk(content='Hello! How can I assist you today?', additional_kwargs={}, response_metadata={'stop_reason': 'end_turn', 'stop_sequence': None}, id='run-b18d016d-8b9b-49e7-a555-44db498fcf66', usage_metadata={'input_tokens': 8, 'output_tokens': 16, 'total_tokens': 24, 'input_token_details': {'cache_creation': 0, 'cache_read': 0}})},\n", "  'run_id': 'b18d016d-8b9b-49e7-a555-44db498fcf66',\n", "  'name': 'ChatAnthropic',\n", "  'tags': [],\n", "  'metadata': {'ls_provider': 'anthropic',\n", "   'ls_model_name': 'claude-3-sonnet-20240229',\n", "   'ls_model_type': 'chat',\n", "   'ls_temperature': 0.0,\n", "   'ls_max_tokens': 1024},\n", "  'parent_ids': []}]"]}, "execution_count": 15, "metadata": {}, "output_type": "execute_result"}], "source": ["events[-2:]"]}, {"cell_type": "markdown", "id": "98c8f173-e9c7-4c27-81a5-b7c85c12714d", "metadata": {}, "source": ["### 链", "\n", "让我们重新审视解析流式JSON的示例链，以探索流式事件API。"]}, {"cell_type": "code", "execution_count": 16, "id": "4328c56c-a303-427b-b1f2-f354e9af555c", "metadata": {}, "outputs": [], "source": ["chain = (\n", "    model | JsonOutputParser()\n", ")  # Due to a bug in older versions of Langchain, JsonOutputParser did not stream results from some models\n", "\n", "events = [\n", "    event\n", "    async for event in chain.astream_events(\n", "        \"output a list of the countries france, spain and japan and their populations in JSON format. \"\n", "        'Use a dict with an outer key of \"countries\" which contains a list of countries. '\n", "        \"Each country should have the key `name` and `population`\",\n", "    )\n", "]"]}, {"cell_type": "markdown", "id": "4cc00b99-a961-4221-a3c7-9d807114bbfb", "metadata": {}, "source": ["如果查看前几个事件，你会发现有 **3** 个不同的开始事件，而不是 **2** 个。", "\n", "三个起始事件对应：", "\n", "1. 链式结构（模型 + 解析器）", "2. 模型", "3. 解析器"]}, {"cell_type": "code", "execution_count": 18, "id": "8e66ea3d-a450-436a-aaac-d9478abc6c28", "metadata": {}, "outputs": [{"data": {"text/plain": ["[{'event': 'on_chain_start',\n", "  'data': {'input': 'output a list of the countries france, spain and japan and their populations in JSON format. Use a dict with an outer key of \"countries\" which contains a list of countries. Each country should have the key `name` and `population`'},\n", "  'name': 'RunnableSequence',\n", "  'tags': [],\n", "  'run_id': '4765006b-16e2-4b1d-a523-edd9fd64cb92',\n", "  'metadata': {}},\n", " {'event': 'on_chat_model_start',\n", "  'data': {'input': {'messages': [[HumanMessage(content='output a list of the countries france, spain and japan and their populations in JSON format. Use a dict with an outer key of \"countries\" which contains a list of countries. Each country should have the key `name` and `population`')]]}},\n", "  'name': 'ChatAnthropic',\n", "  'tags': ['seq:step:1'],\n", "  'run_id': '0320c234-7b52-4a14-ae4e-5f100949e589',\n", "  'metadata': {}},\n", " {'event': 'on_chat_model_stream',\n", "  'data': {'chunk': AIMessageChunk(content='{', id='run-0320c234-7b52-4a14-ae4e-5f100949e589')},\n", "  'run_id': '0320c234-7b52-4a14-ae4e-5f100949e589',\n", "  'name': 'ChatAnthropic',\n", "  'tags': ['seq:step:1'],\n", "  'metadata': {}}]"]}, "execution_count": 18, "metadata": {}, "output_type": "execute_result"}], "source": ["events[:3]"]}, {"cell_type": "markdown", "id": "c8512238-d035-4acd-9248-a8570da064c9", "metadata": {}, "source": ["你认为如果查看最后3个事件会看到什么？中间的呢？"]}, {"cell_type": "markdown", "id": "c742cfa4-9b03-4a5b-96d9-5fe56e95e3b4", "metadata": {}, "source": ["让我们使用这个API来输出模型和解析器的流事件。我们将忽略开始事件、结束事件以及来自链的事件。"]}, {"cell_type": "code", "execution_count": 19, "id": "630c71d6-8d94-4ce0-a78a-f20e90f628df", "metadata": {}, "outputs": [{"name": "stdout", "output_type": "stream", "text": ["Chat model chunk: ''\n", "Chat model chunk: '{'\n", "Parser chunk: {}\n", "Chat model chunk: '\\n  \"countries'\n", "Chat model chunk: '\": [\\n    '\n", "Parser chunk: {'countries': []}\n", "Chat model chunk: '{\\n      \"'\n", "Parser chunk: {'countries': [{}]}\n", "Chat model chunk: 'name\": \"France'\n", "Parser chunk: {'countries': [{'name': 'France'}]}\n", "Chat model chunk: '\",\\n      \"'\n", "Chat model chunk: 'population\": 67'\n", "Parser chunk: {'countries': [{'name': 'France', 'population': 67}]}\n", "Chat model chunk: '413'\n", "Parser chunk: {'countries': [{'name': 'France', 'population': 67413}]}\n", "Chat model chunk: '000\\n    },'\n", "Parser chunk: {'countries': [{'name': 'France', 'population': 67413000}]}\n", "Chat model chunk: '\\n    {'\n", "Parser chunk: {'countries': [{'name': 'France', 'population': 67413000}, {}]}\n", "Chat model chunk: '\\n      \"name\":'\n", "...\n"]}], "source": ["num_events = 0\n", "\n", "async for event in chain.astream_events(\n", "    \"output a list of the countries france, spain and japan and their populations in JSON format. \"\n", "    'Use a dict with an outer key of \"countries\" which contains a list of countries. '\n", "    \"Each country should have the key `name` and `population`\",\n", "):\n", "    kind = event[\"event\"]\n", "    if kind == \"on_chat_model_stream\":\n", "        print(\n", "            f\"Chat model chunk: {repr(event['data']['chunk'].content)}\",\n", "            flush=True,\n", "        )\n", "    if kind == \"on_parser_stream\":\n", "        print(f\"Parser chunk: {event['data']['chunk']}\", flush=True)\n", "    num_events += 1\n", "    if num_events > 30:\n", "        # Truncate the output\n", "        print(\"...\")\n", "        break"]}, {"cell_type": "markdown", "id": "798ea891-997c-454c-bf60-43124f40ee1b", "metadata": {}, "source": ["由于模型和解析器均支持流式处理，我们可以实时看到来自这两个组件的流式事件！有点酷炫，不是吗？🦜"]}, {"cell_type": "markdown", "id": "5084148b-bcdc-4373-9caa-6568f03e7b23", "metadata": {}, "source": ["### 事件筛选", "\n", "由于该API会产生大量事件，因此能够对事件进行过滤非常有用。", "\n", "您可以通过组件`名称`、组件`标签`或组件`类型`进行筛选。", "\n", "#### 按名称"]}, {"cell_type": "code", "execution_count": 20, "id": "42145735-25e8-4e67-b081-b0c15ea45dd1", "metadata": {}, "outputs": [{"name": "stdout", "output_type": "stream", "text": ["{'event': 'on_parser_start', 'data': {'input': 'output a list of the countries france, spain and japan and their populations in JSON format. Use a dict with an outer key of \"countries\" which contains a list of countries. Each country should have the key `name` and `population`'}, 'name': 'my_parser', 'tags': ['seq:step:2'], 'run_id': '37ee9e85-481c-415e-863b-c9e132d24948', 'metadata': {}, 'parent_ids': ['5a0bc625-09fd-4bdf-9932-54909a9a8c29']}\n", "{'event': 'on_parser_stream', 'run_id': '37ee9e85-481c-415e-863b-c9e132d24948', 'name': 'my_parser', 'tags': ['seq:step:2'], 'metadata': {}, 'data': {'chunk': {}}, 'parent_ids': ['5a0bc625-09fd-4bdf-9932-54909a9a8c29']}\n", "{'event': 'on_parser_stream', 'run_id': '37ee9e85-481c-415e-863b-c9e132d24948', 'name': 'my_parser', 'tags': ['seq:step:2'], 'metadata': {}, 'data': {'chunk': {'countries': []}}, 'parent_ids': ['5a0bc625-09fd-4bdf-9932-54909a9a8c29']}\n", "{'event': 'on_parser_stream', 'run_id': '37ee9e85-481c-415e-863b-c9e132d24948', 'name': 'my_parser', 'tags': ['seq:step:2'], 'metadata': {}, 'data': {'chunk': {'countries': [{}]}}, 'parent_ids': ['5a0bc625-09fd-4bdf-9932-54909a9a8c29']}\n", "{'event': 'on_parser_stream', 'run_id': '37ee9e85-481c-415e-863b-c9e132d24948', 'name': 'my_parser', 'tags': ['seq:step:2'], 'metadata': {}, 'data': {'chunk': {'countries': [{'name': 'France'}]}}, 'parent_ids': ['5a0bc625-09fd-4bdf-9932-54909a9a8c29']}\n", "{'event': 'on_parser_stream', 'run_id': '37ee9e85-481c-415e-863b-c9e132d24948', 'name': 'my_parser', 'tags': ['seq:step:2'], 'metadata': {}, 'data': {'chunk': {'countries': [{'name': 'France', 'population': 67}]}}, 'parent_ids': ['5a0bc625-09fd-4bdf-9932-54909a9a8c29']}\n", "{'event': 'on_parser_stream', 'run_id': '37ee9e85-481c-415e-863b-c9e132d24948', 'name': 'my_parser', 'tags': ['seq:step:2'], 'metadata': {}, 'data': {'chunk': {'countries': [{'name': 'France', 'population': 67413}]}}, 'parent_ids': ['5a0bc625-09fd-4bdf-9932-54909a9a8c29']}\n", "{'event': 'on_parser_stream', 'run_id': '37ee9e85-481c-415e-863b-c9e132d24948', 'name': 'my_parser', 'tags': ['seq:step:2'], 'metadata': {}, 'data': {'chunk': {'countries': [{'name': 'France', 'population': 67413000}]}}, 'parent_ids': ['5a0bc625-09fd-4bdf-9932-54909a9a8c29']}\n", "{'event': 'on_parser_stream', 'run_id': '37ee9e85-481c-415e-863b-c9e132d24948', 'name': 'my_parser', 'tags': ['seq:step:2'], 'metadata': {}, 'data': {'chunk': {'countries': [{'name': 'France', 'population': 67413000}, {}]}}, 'parent_ids': ['5a0bc625-09fd-4bdf-9932-54909a9a8c29']}\n", "{'event': 'on_parser_stream', 'run_id': '37ee9e85-481c-415e-863b-c9e132d24948', 'name': 'my_parser', 'tags': ['seq:step:2'], 'metadata': {}, 'data': {'chunk': {'countries': [{'name': 'France', 'population': 67413000}, {'name': 'Spain'}]}}, 'parent_ids': ['5a0bc625-09fd-4bdf-9932-54909a9a8c29']}\n", "{'event': 'on_parser_stream', 'run_id': '37ee9e85-481c-415e-863b-c9e132d24948', 'name': 'my_parser', 'tags': ['seq:step:2'], 'metadata': {}, 'data': {'chunk': {'countries': [{'name': 'France', 'population': 67413000}, {'name': 'Spain', 'population': 47}]}}, 'parent_ids': ['5a0bc625-09fd-4bdf-9932-54909a9a8c29']}\n", "...\n"]}], "source": ["chain = model.with_config({\"run_name\": \"model\"}) | JsonOutputParser().with_config(\n", "    {\"run_name\": \"my_parser\"}\n", ")\n", "\n", "max_events = 0\n", "async for event in chain.astream_events(\n", "    \"output a list of the countries france, spain and japan and their populations in JSON format. \"\n", "    'Use a dict with an outer key of \"countries\" which contains a list of countries. '\n", "    \"Each country should have the key `name` and `population`\",\n", "    include_names=[\"my_parser\"],\n", "):\n", "    print(event)\n", "    max_events += 1\n", "    if max_events > 10:\n", "        # Truncate output\n", "        print(\"...\")\n", "        break"]}, {"cell_type": "markdown", "id": "c59d5626-7dba-4eb3-ad81-76c1092c5146", "metadata": {}, "source": ["#### 按类型"]}, {"cell_type": "code", "execution_count": 21, "id": "2a7d8fe0-47ca-4ab4-9c10-b34e3f6106ee", "metadata": {}, "outputs": [{"name": "stdout", "output_type": "stream", "text": ["{'event': 'on_chat_model_start', 'data': {'input': 'output a list of the countries france, spain and japan and their populations in JSON format. Use a dict with an outer key of \"countries\" which contains a list of countries. Each country should have the key `name` and `population`'}, 'name': 'model', 'tags': ['seq:step:1'], 'run_id': '156c3e40-82fb-49ff-8e41-9e998061be8c', 'metadata': {'ls_provider': 'anthropic', 'ls_model_name': 'claude-3-sonnet-20240229', 'ls_model_type': 'chat', 'ls_temperature': 0.0, 'ls_max_tokens': 1024}, 'parent_ids': ['7b927055-bc1b-4b50-a34c-10d3cfcb3899']}\n", "{'event': 'on_chat_model_stream', 'data': {'chunk': AIMessageChunk(content='', additional_kwargs={}, response_metadata={}, id='run-156c3e40-82fb-49ff-8e41-9e998061be8c', usage_metadata={'input_tokens': 56, 'output_tokens': 1, 'total_tokens': 57, 'input_token_details': {'cache_creation': 0, 'cache_read': 0}})}, 'run_id': '156c3e40-82fb-49ff-8e41-9e998061be8c', 'name': 'model', 'tags': ['seq:step:1'], 'metadata': {'ls_provider': 'anthropic', 'ls_model_name': 'claude-3-sonnet-20240229', 'ls_model_type': 'chat', 'ls_temperature': 0.0, 'ls_max_tokens': 1024}, 'parent_ids': ['7b927055-bc1b-4b50-a34c-10d3cfcb3899']}\n", "{'event': 'on_chat_model_stream', 'data': {'chunk': AIMessageChunk(content='{', additional_kwargs={}, response_metadata={}, id='run-156c3e40-82fb-49ff-8e41-9e998061be8c')}, 'run_id': '156c3e40-82fb-49ff-8e41-9e998061be8c', 'name': 'model', 'tags': ['seq:step:1'], 'metadata': {'ls_provider': 'anthropic', 'ls_model_name': 'claude-3-sonnet-20240229', 'ls_model_type': 'chat', 'ls_temperature': 0.0, 'ls_max_tokens': 1024}, 'parent_ids': ['7b927055-bc1b-4b50-a34c-10d3cfcb3899']}\n", "{'event': 'on_chat_model_stream', 'data': {'chunk': AIMessageChunk(content='\\n  \"countries', additional_kwargs={}, response_metadata={}, id='run-156c3e40-82fb-49ff-8e41-9e998061be8c')}, 'run_id': '156c3e40-82fb-49ff-8e41-9e998061be8c', 'name': 'model', 'tags': ['seq:step:1'], 'metadata': {'ls_provider': 'anthropic', 'ls_model_name': 'claude-3-sonnet-20240229', 'ls_model_type': 'chat', 'ls_temperature': 0.0, 'ls_max_tokens': 1024}, 'parent_ids': ['7b927055-bc1b-4b50-a34c-10d3cfcb3899']}\n", "{'event': 'on_chat_model_stream', 'data': {'chunk': AIMessageChunk(content='\": [\\n    ', additional_kwargs={}, response_metadata={}, id='run-156c3e40-82fb-49ff-8e41-9e998061be8c')}, 'run_id': '156c3e40-82fb-49ff-8e41-9e998061be8c', 'name': 'model', 'tags': ['seq:step:1'], 'metadata': {'ls_provider': 'anthropic', 'ls_model_name': 'claude-3-sonnet-20240229', 'ls_model_type': 'chat', 'ls_temperature': 0.0, 'ls_max_tokens': 1024}, 'parent_ids': ['7b927055-bc1b-4b50-a34c-10d3cfcb3899']}\n", "{'event': 'on_chat_model_stream', 'data': {'chunk': AIMessageChunk(content='{\\n      \"', additional_kwargs={}, response_metadata={}, id='run-156c3e40-82fb-49ff-8e41-9e998061be8c')}, 'run_id': '156c3e40-82fb-49ff-8e41-9e998061be8c', 'name': 'model', 'tags': ['seq:step:1'], 'metadata': {'ls_provider': 'anthropic', 'ls_model_name': 'claude-3-sonnet-20240229', 'ls_model_type': 'chat', 'ls_temperature': 0.0, 'ls_max_tokens': 1024}, 'parent_ids': ['7b927055-bc1b-4b50-a34c-10d3cfcb3899']}\n", "{'event': 'on_chat_model_stream', 'data': {'chunk': AIMessageChunk(content='name\": \"France', additional_kwargs={}, response_metadata={}, id='run-156c3e40-82fb-49ff-8e41-9e998061be8c')}, 'run_id': '156c3e40-82fb-49ff-8e41-9e998061be8c', 'name': 'model', 'tags': ['seq:step:1'], 'metadata': {'ls_provider': 'anthropic', 'ls_model_name': 'claude-3-sonnet-20240229', 'ls_model_type': 'chat', 'ls_temperature': 0.0, 'ls_max_tokens': 1024}, 'parent_ids': ['7b927055-bc1b-4b50-a34c-10d3cfcb3899']}\n", "{'event': 'on_chat_model_stream', 'data': {'chunk': AIMessageChunk(content='\",\\n      \"', additional_kwargs={}, response_metadata={}, id='run-156c3e40-82fb-49ff-8e41-9e998061be8c')}, 'run_id': '156c3e40-82fb-49ff-8e41-9e998061be8c', 'name': 'model', 'tags': ['seq:step:1'], 'metadata': {'ls_provider': 'anthropic', 'ls_model_name': 'claude-3-sonnet-20240229', 'ls_model_type': 'chat', 'ls_temperature': 0.0, 'ls_max_tokens': 1024}, 'parent_ids': ['7b927055-bc1b-4b50-a34c-10d3cfcb3899']}\n", "{'event': 'on_chat_model_stream', 'data': {'chunk': AIMessageChunk(content='population\": 67', additional_kwargs={}, response_metadata={}, id='run-156c3e40-82fb-49ff-8e41-9e998061be8c')}, 'run_id': '156c3e40-82fb-49ff-8e41-9e998061be8c', 'name': 'model', 'tags': ['seq:step:1'], 'metadata': {'ls_provider': 'anthropic', 'ls_model_name': 'claude-3-sonnet-20240229', 'ls_model_type': 'chat', 'ls_temperature': 0.0, 'ls_max_tokens': 1024}, 'parent_ids': ['7b927055-bc1b-4b50-a34c-10d3cfcb3899']}\n", "{'event': 'on_chat_model_stream', 'data': {'chunk': AIMessageChunk(content='413', additional_kwargs={}, response_metadata={}, id='run-156c3e40-82fb-49ff-8e41-9e998061be8c')}, 'run_id': '156c3e40-82fb-49ff-8e41-9e998061be8c', 'name': 'model', 'tags': ['seq:step:1'], 'metadata': {'ls_provider': 'anthropic', 'ls_model_name': 'claude-3-sonnet-20240229', 'ls_model_type': 'chat', 'ls_temperature': 0.0, 'ls_max_tokens': 1024}, 'parent_ids': ['7b927055-bc1b-4b50-a34c-10d3cfcb3899']}\n", "{'event': 'on_chat_model_stream', 'data': {'chunk': AIMessageChunk(content='000\\n    },', additional_kwargs={}, response_metadata={}, id='run-156c3e40-82fb-49ff-8e41-9e998061be8c')}, 'run_id': '156c3e40-82fb-49ff-8e41-9e998061be8c', 'name': 'model', 'tags': ['seq:step:1'], 'metadata': {'ls_provider': 'anthropic', 'ls_model_name': 'claude-3-sonnet-20240229', 'ls_model_type': 'chat', 'ls_temperature': 0.0, 'ls_max_tokens': 1024}, 'parent_ids': ['7b927055-bc1b-4b50-a34c-10d3cfcb3899']}\n", "...\n"]}], "source": ["chain = model.with_config({\"run_name\": \"model\"}) | JsonOutputParser().with_config(\n", "    {\"run_name\": \"my_parser\"}\n", ")\n", "\n", "max_events = 0\n", "async for event in chain.astream_events(\n", "    'output a list of the countries france, spain and japan and their populations in JSON format. Use a dict with an outer key of \"countries\" which contains a list of countries. Each country should have the key `name` and `population`',\n", "    include_types=[\"chat_model\"],\n", "):\n", "    print(event)\n", "    max_events += 1\n", "    if max_events > 10:\n", "        # Truncate output\n", "        print(\"...\")\n", "        break"]}, {"cell_type": "markdown", "id": "f1ec8dd4-9b5b-4000-b63f-5845bfc5a065", "metadata": {}, "source": ["#### 按标签分类", "\n", ":::注意", "\n", "标签会被给定可运行组件的子组件继承。", "\n", "如果你正在使用标签进行筛选，请确保这是你想要的操作。", "好的,我会按照要求进行翻译,确保markdown格式一致。以下是一个示例翻译:\n\n# 欢迎使用翻译助手\n\n这是一个**markdown格式**的翻译示例。\n\n## 功能特点\n\n1. 保持原始格式\n2. 准确传达语义\n3. 自然流畅的表达\n\n> 翻译不仅是语言的转换,更是文化的桥梁\n\n```python\n# 代码块也会保留原格式\ndef hello():\n    print(\"你好,世界!\")\n```\n\n[这是一个链接示例](https://example.com)\n\n* 列表项1\n* 列表项2\n* 列表项3\n\n**注意**: 我只输出翻译后的具体内容,不包含任何额外的标记或说明。"]}, {"cell_type": "code", "execution_count": 22, "id": "c237c218-5fd6-4146-ac68-020a038cf582", "metadata": {}, "outputs": [{"name": "stdout", "output_type": "stream", "text": ["{'event': 'on_chain_start', 'data': {'input': 'output a list of the countries france, spain and japan and their populations in JSON format. Use a dict with an outer key of \"countries\" which contains a list of countries. Each country should have the key `name` and `population`'}, 'name': 'RunnableSequence', 'tags': ['my_chain'], 'run_id': '58d1302e-36ce-4df7-a3cb-47cb73d57e44', 'metadata': {}, 'parent_ids': []}\n", "{'event': 'on_chat_model_start', 'data': {'input': {'messages': [[HumanMessage(content='output a list of the countries france, spain and japan and their populations in JSON format. Use a dict with an outer key of \"countries\" which contains a list of countries. Each country should have the key `name` and `population`', additional_kwargs={}, response_metadata={})]]}}, 'name': 'ChatAnthropic', 'tags': ['seq:step:1', 'my_chain'], 'run_id': '8222e8a1-d978-4f30-87fc-b2dba838774b', 'metadata': {'ls_provider': 'anthropic', 'ls_model_name': 'claude-3-sonnet-20240229', 'ls_model_type': 'chat', 'ls_temperature': 0.0, 'ls_max_tokens': 1024}, 'parent_ids': ['58d1302e-36ce-4df7-a3cb-47cb73d57e44']}\n", "{'event': 'on_chat_model_stream', 'data': {'chunk': AIMessageChunk(content='', additional_kwargs={}, response_metadata={}, id='run-8222e8a1-d978-4f30-87fc-b2dba838774b', usage_metadata={'input_tokens': 56, 'output_tokens': 1, 'total_tokens': 57, 'input_token_details': {'cache_creation': 0, 'cache_read': 0}})}, 'run_id': '8222e8a1-d978-4f30-87fc-b2dba838774b', 'name': 'ChatAnthropic', 'tags': ['seq:step:1', 'my_chain'], 'metadata': {'ls_provider': 'anthropic', 'ls_model_name': 'claude-3-sonnet-20240229', 'ls_model_type': 'chat', 'ls_temperature': 0.0, 'ls_max_tokens': 1024}, 'parent_ids': ['58d1302e-36ce-4df7-a3cb-47cb73d57e44']}\n", "{'event': 'on_parser_start', 'data': {}, 'name': 'JsonOutputParser', 'tags': ['seq:step:2', 'my_chain'], 'run_id': '75604c84-e1e6-494a-8b2a-950f45d932e8', 'metadata': {}, 'parent_ids': ['58d1302e-36ce-4df7-a3cb-47cb73d57e44']}\n", "{'event': 'on_chat_model_stream', 'data': {'chunk': AIMessageChunk(content='{', additional_kwargs={}, response_metadata={}, id='run-8222e8a1-d978-4f30-87fc-b2dba838774b')}, 'run_id': '8222e8a1-d978-4f30-87fc-b2dba838774b', 'name': 'ChatAnthropic', 'tags': ['seq:step:1', 'my_chain'], 'metadata': {'ls_provider': 'anthropic', 'ls_model_name': 'claude-3-sonnet-20240229', 'ls_model_type': 'chat', 'ls_temperature': 0.0, 'ls_max_tokens': 1024}, 'parent_ids': ['58d1302e-36ce-4df7-a3cb-47cb73d57e44']}\n", "{'event': 'on_parser_stream', 'run_id': '75604c84-e1e6-494a-8b2a-950f45d932e8', 'name': 'JsonOutputParser', 'tags': ['seq:step:2', 'my_chain'], 'metadata': {}, 'data': {'chunk': {}}, 'parent_ids': ['58d1302e-36ce-4df7-a3cb-47cb73d57e44']}\n", "{'event': 'on_chain_stream', 'run_id': '58d1302e-36ce-4df7-a3cb-47cb73d57e44', 'name': 'RunnableSequence', 'tags': ['my_chain'], 'metadata': {}, 'data': {'chunk': {}}, 'parent_ids': []}\n", "{'event': 'on_chat_model_stream', 'data': {'chunk': AIMessageChunk(content='\\n  \"countries', additional_kwargs={}, response_metadata={}, id='run-8222e8a1-d978-4f30-87fc-b2dba838774b')}, 'run_id': '8222e8a1-d978-4f30-87fc-b2dba838774b', 'name': 'ChatAnthropic', 'tags': ['seq:step:1', 'my_chain'], 'metadata': {'ls_provider': 'anthropic', 'ls_model_name': 'claude-3-sonnet-20240229', 'ls_model_type': 'chat', 'ls_temperature': 0.0, 'ls_max_tokens': 1024}, 'parent_ids': ['58d1302e-36ce-4df7-a3cb-47cb73d57e44']}\n", "{'event': 'on_chat_model_stream', 'data': {'chunk': AIMessageChunk(content='\": [\\n    ', additional_kwargs={}, response_metadata={}, id='run-8222e8a1-d978-4f30-87fc-b2dba838774b')}, 'run_id': '8222e8a1-d978-4f30-87fc-b2dba838774b', 'name': 'ChatAnthropic', 'tags': ['seq:step:1', 'my_chain'], 'metadata': {'ls_provider': 'anthropic', 'ls_model_name': 'claude-3-sonnet-20240229', 'ls_model_type': 'chat', 'ls_temperature': 0.0, 'ls_max_tokens': 1024}, 'parent_ids': ['58d1302e-36ce-4df7-a3cb-47cb73d57e44']}\n", "{'event': 'on_parser_stream', 'run_id': '75604c84-e1e6-494a-8b2a-950f45d932e8', 'name': 'JsonOutputParser', 'tags': ['seq:step:2', 'my_chain'], 'metadata': {}, 'data': {'chunk': {'countries': []}}, 'parent_ids': ['58d1302e-36ce-4df7-a3cb-47cb73d57e44']}\n", "{'event': 'on_chain_stream', 'run_id': '58d1302e-36ce-4df7-a3cb-47cb73d57e44', 'name': 'RunnableSequence', 'tags': ['my_chain'], 'metadata': {}, 'data': {'chunk': {'countries': []}}, 'parent_ids': []}\n", "...\n"]}], "source": ["chain = (model | JsonOutputParser()).with_config({\"tags\": [\"my_chain\"]})\n", "\n", "max_events = 0\n", "async for event in chain.astream_events(\n", "    'output a list of the countries france, spain and japan and their populations in JSON format. Use a dict with an outer key of \"countries\" which contains a list of countries. Each country should have the key `name` and `population`',\n", "    include_tags=[\"my_chain\"],\n", "):\n", "    print(event)\n", "    max_events += 1\n", "    if max_events > 10:\n", "        # Truncate output\n", "        print(\"...\")\n", "        break"]}, {"cell_type": "markdown", "id": "e05e54c4-61a2-4f6c-aa68-d2b09b5e1d4f", "metadata": {}, "source": ["### 非流式组件", "\n", "还记得某些组件因为无法处理**输入流**而导致流式传输效果不佳的情况吗？", "\n", "在使用`astream`时，虽然这类组件可能会中断最终输出的流式传输，但`astream_events`仍会从支持流式传输的中间步骤生成流式事件！"]}, {"cell_type": "code", "execution_count": 23, "id": "0e6451d3-3b11-4a71-ae19-998f4c10180f", "metadata": {}, "outputs": [], "source": ["# Function that does not support streaming.\n", "# It operates on the finalizes inputs rather than\n", "# operating on the input stream.\n", "def _extract_country_names(inputs):\n", "    \"\"\"A function that does not operates on input streams and breaks streaming.\"\"\"\n", "    if not isinstance(inputs, dict):\n", "        return \"\"\n", "\n", "    if \"countries\" not in inputs:\n", "        return \"\"\n", "\n", "    countries = inputs[\"countries\"]\n", "\n", "    if not isinstance(countries, list):\n", "        return \"\"\n", "\n", "    country_names = [\n", "        country.get(\"name\") for country in countries if isinstance(country, dict)\n", "    ]\n", "    return country_names\n", "\n", "\n", "chain = (\n", "    model | JsonOutputParser() | _extract_country_names\n", ")  # This parser only works with OpenAI right now"]}, {"cell_type": "markdown", "id": "a972e1a6-80cd-4d59-90a0-73563f1503d4", "metadata": {}, "source": ["正如预期的那样，`astream` API 无法正常工作，因为 `_extract_country_names` 不支持流式处理。"]}, {"cell_type": "code", "execution_count": 24, "id": "f9a8fe35-faab-4970-b8c0-5c780845d98a", "metadata": {}, "outputs": [{"name": "stdout", "output_type": "stream", "text": ["['France', 'Spain', 'Japan']\n"]}], "source": ["async for chunk in chain.astream(\n", "    \"output a list of the countries france, spain and japan and their populations in JSON format. \"\n", "    'Use a dict with an outer key of \"countries\" which contains a list of countries. '\n", "    \"Each country should have the key `name` and `population`\",\n", "):\n", "    print(chunk, flush=True)"]}, {"cell_type": "markdown", "id": "b279ea33-54f1-400a-acb1-b8445ccbf1fa", "metadata": {}, "source": ["现在，让我们通过 `astream_events` 确认是否仍能从模型和解析器中获取流式输出。"]}, {"cell_type": "code", "execution_count": 25, "id": "2c83701e-b801-429f-b2ac-47ed44d2d11a", "metadata": {}, "outputs": [{"name": "stdout", "output_type": "stream", "text": ["Chat model chunk: ''\n", "Chat model chunk: '{'\n", "Parser chunk: {}\n", "Chat model chunk: '\\n  \"countries'\n", "Chat model chunk: '\": [\\n    '\n", "Parser chunk: {'countries': []}\n", "Chat model chunk: '{\\n      \"'\n", "Parser chunk: {'countries': [{}]}\n", "Chat model chunk: 'name\": \"France'\n", "Parser chunk: {'countries': [{'name': 'France'}]}\n", "Chat model chunk: '\",\\n      \"'\n", "Chat model chunk: 'population\": 67'\n", "Parser chunk: {'countries': [{'name': 'France', 'population': 67}]}\n", "Chat model chunk: '413'\n", "Parser chunk: {'countries': [{'name': 'France', 'population': 67413}]}\n", "Chat model chunk: '000\\n    },'\n", "Parser chunk: {'countries': [{'name': 'France', 'population': 67413000}]}\n", "Chat model chunk: '\\n    {'\n", "Parser chunk: {'countries': [{'name': 'France', 'population': 67413000}, {}]}\n", "Chat model chunk: '\\n      \"name\":'\n", "Chat model chunk: ' \"Spain\",'\n", "Parser chunk: {'countries': [{'name': 'France', 'population': 67413000}, {'name': 'Spain'}]}\n", "Chat model chunk: '\\n      \"population\":'\n", "Chat model chunk: ' 47'\n", "Parser chunk: {'countries': [{'name': 'France', 'population': 67413000}, {'name': 'Spain', 'population': 47}]}\n", "Chat model chunk: '351'\n", "Parser chunk: {'countries': [{'name': 'France', 'population': 67413000}, {'name': 'Spain', 'population': 47351}]}\n", "...\n"]}], "source": ["num_events = 0\n", "\n", "async for event in chain.astream_events(\n", "    \"output a list of the countries france, spain and japan and their populations in JSON format. \"\n", "    'Use a dict with an outer key of \"countries\" which contains a list of countries. '\n", "    \"Each country should have the key `name` and `population`\",\n", "):\n", "    kind = event[\"event\"]\n", "    if kind == \"on_chat_model_stream\":\n", "        print(\n", "            f\"Chat model chunk: {repr(event['data']['chunk'].content)}\",\n", "            flush=True,\n", "        )\n", "    if kind == \"on_parser_stream\":\n", "        print(f\"Parser chunk: {event['data']['chunk']}\", flush=True)\n", "    num_events += 1\n", "    if num_events > 30:\n", "        # Truncate the output\n", "        print(\"...\")\n", "        break"]}, {"cell_type": "markdown", "id": "6e91bdd3-f4a3-4b3c-b21a-26365c6c1566", "metadata": {}, "source": ["### 回调传播", "\n", ":::注意", "如果你在工具内部调用可运行对象，需要将回调函数传递给该可运行对象；否则不会生成任何流式事件。", ":::", "\n", ":::note", "在使用 `RunnableLambdas` 或 `@chain` 装饰器时，回调函数会在后台自动传播。", "好的,我会按照要求将英文翻译成中文,并保持markdown格式一致。以下是一个示例翻译:\n\n# 欢迎使用翻译助手\n\n这是一个标准的markdown格式文档示例。\n\n## 主要功能\n\n1. 提供高质量的翻译服务\n2. 保持原始文档格式\n3. 支持多种文件格式\n\n### 使用说明\n\n- 输入需要翻译的文本\n- 选择目标语言\n- 获取翻译结果\n\n> 注意:请确保输入文本的准确性以获得最佳翻译效果\n\n**重要提示**:本服务完全免费!\n\n[点击这里](https://example.com)了解更多信息"]}, {"cell_type": "code", "execution_count": 26, "id": "1854206d-b3a5-4f91-9e00-bccbaebac61f", "metadata": {}, "outputs": [{"name": "stdout", "output_type": "stream", "text": ["{'event': 'on_tool_start', 'data': {'input': 'hello'}, 'name': 'bad_tool', 'tags': [], 'run_id': 'ea900472-a8f7-425d-b627-facdef936ee8', 'metadata': {}}\n", "{'event': 'on_chain_start', 'data': {'input': 'hello'}, 'name': 'reverse_word', 'tags': [], 'run_id': '77b01284-0515-48f4-8d7c-eb27c1882f86', 'metadata': {}}\n", "{'event': 'on_chain_end', 'data': {'output': 'olleh', 'input': 'hello'}, 'run_id': '77b01284-0515-48f4-8d7c-eb27c1882f86', 'name': 'reverse_word', 'tags': [], 'metadata': {}}\n", "{'event': 'on_tool_end', 'data': {'output': 'olleh'}, 'run_id': 'ea900472-a8f7-425d-b627-facdef936ee8', 'name': 'bad_tool', 'tags': [], 'metadata': {}}\n"]}], "source": ["from langchain_core.runnables import RunnableLambda\n", "from langchain_core.tools import tool\n", "\n", "\n", "def reverse_word(word: str):\n", "    return word[::-1]\n", "\n", "\n", "reverse_word = RunnableLambda(reverse_word)\n", "\n", "\n", "@tool\n", "def bad_tool(word: str):\n", "    \"\"\"Custom tool that doesn't propagate callbacks.\"\"\"\n", "    return reverse_word.invoke(word)\n", "\n", "\n", "async for event in bad_tool.astream_events(\"hello\"):\n", "    print(event)"]}, {"cell_type": "markdown", "id": "23e68a99-7886-465b-8575-116022857469", "metadata": {}, "source": ["以下是正确传递回调的重新实现。您会注意到，现在我们也从 `reverse_word` 可运行对象获取事件了。"]}, {"cell_type": "code", "execution_count": 27, "id": "a20a6cb3-bb43-465c-8cfc-0a7349d70968", "metadata": {}, "outputs": [{"name": "stdout", "output_type": "stream", "text": ["{'event': 'on_tool_start', 'data': {'input': 'hello'}, 'name': 'correct_tool', 'tags': [], 'run_id': 'd5ea83b9-9278-49cc-9f1d-aa302d671040', 'metadata': {}}\n", "{'event': 'on_chain_start', 'data': {'input': 'hello'}, 'name': 'reverse_word', 'tags': [], 'run_id': '44dafbf4-2f87-412b-ae0e-9f71713810df', 'metadata': {}}\n", "{'event': 'on_chain_end', 'data': {'output': 'olleh', 'input': 'hello'}, 'run_id': '44dafbf4-2f87-412b-ae0e-9f71713810df', 'name': 'reverse_word', 'tags': [], 'metadata': {}}\n", "{'event': 'on_tool_end', 'data': {'output': 'olleh'}, 'run_id': 'd5ea83b9-9278-49cc-9f1d-aa302d671040', 'name': 'correct_tool', 'tags': [], 'metadata': {}}\n"]}], "source": ["@tool\n", "def correct_tool(word: str, callbacks):\n", "    \"\"\"A tool that correctly propagates callbacks.\"\"\"\n", "    return reverse_word.invoke(word, {\"callbacks\": callbacks})\n", "\n", "\n", "async for event in correct_tool.astream_events(\"hello\"):\n", "    print(event)"]}, {"cell_type": "markdown", "id": "640daa94-e4fe-4997-ab6e-45120f18b9ee", "metadata": {}, "source": ["如果你在可运行Lambda或`@chains`中调用可运行对象，那么回调将自动代表你传递。"]}, {"cell_type": "code", "execution_count": 28, "id": "0ac0a3c1-f3a4-4157-b053-4fec8d2e698c", "metadata": {}, "outputs": [{"name": "stdout", "output_type": "stream", "text": ["{'event': 'on_chain_start', 'data': {'input': '1234'}, 'name': 'reverse_and_double', 'tags': [], 'run_id': '03b0e6a1-3e60-42fc-8373-1e7829198d80', 'metadata': {}}\n", "{'event': 'on_chain_start', 'data': {'input': '1234'}, 'name': 'reverse_word', 'tags': [], 'run_id': '5cf26fc8-840b-4642-98ed-623dda28707a', 'metadata': {}}\n", "{'event': 'on_chain_end', 'data': {'output': '4321', 'input': '1234'}, 'run_id': '5cf26fc8-840b-4642-98ed-623dda28707a', 'name': 'reverse_word', 'tags': [], 'metadata': {}}\n", "{'event': 'on_chain_stream', 'data': {'chunk': '43214321'}, 'run_id': '03b0e6a1-3e60-42fc-8373-1e7829198d80', 'name': 'reverse_and_double', 'tags': [], 'metadata': {}}\n", "{'event': 'on_chain_end', 'data': {'output': '43214321'}, 'run_id': '03b0e6a1-3e60-42fc-8373-1e7829198d80', 'name': 'reverse_and_double', 'tags': [], 'metadata': {}}\n"]}], "source": ["from langchain_core.runnables import RunnableLambda\n", "\n", "\n", "async def reverse_and_double(word: str):\n", "    return await reverse_word.ainvoke(word) * 2\n", "\n", "\n", "reverse_and_double = RunnableLambda(reverse_and_double)\n", "\n", "await reverse_and_double.ainvoke(\"1234\")\n", "\n", "async for event in reverse_and_double.astream_events(\"1234\"):\n", "    print(event)"]}, {"cell_type": "markdown", "id": "35a34268-9b3d-4857-b4ed-65d95f4a1293", "metadata": {}, "source": ["使用 `@chain` 装饰器时："]}, {"cell_type": "code", "execution_count": 29, "id": "c896bb94-9d10-41ff-8fe2-d6b05b1ed74b", "metadata": {}, "outputs": [{"name": "stdout", "output_type": "stream", "text": ["{'event': 'on_chain_start', 'data': {'input': '1234'}, 'name': 'reverse_and_double', 'tags': [], 'run_id': '1bfcaedc-f4aa-4d8e-beee-9bba6ef17008', 'metadata': {}}\n", "{'event': 'on_chain_start', 'data': {'input': '1234'}, 'name': 'reverse_word', 'tags': [], 'run_id': '64fc99f0-5d7d-442b-b4f5-4537129f67d1', 'metadata': {}}\n", "{'event': 'on_chain_end', 'data': {'output': '4321', 'input': '1234'}, 'run_id': '64fc99f0-5d7d-442b-b4f5-4537129f67d1', 'name': 'reverse_word', 'tags': [], 'metadata': {}}\n", "{'event': 'on_chain_stream', 'data': {'chunk': '43214321'}, 'run_id': '1bfcaedc-f4aa-4d8e-beee-9bba6ef17008', 'name': 'reverse_and_double', 'tags': [], 'metadata': {}}\n", "{'event': 'on_chain_end', 'data': {'output': '43214321'}, 'run_id': '1bfcaedc-f4aa-4d8e-beee-9bba6ef17008', 'name': 'reverse_and_double', 'tags': [], 'metadata': {}}\n"]}], "source": ["from langchain_core.runnables import chain\n", "\n", "\n", "@chain\n", "async def reverse_and_double(word: str):\n", "    return await reverse_word.ainvoke(word) * 2\n", "\n", "\n", "await reverse_and_double.ainvoke(\"1234\")\n", "\n", "async for event in reverse_and_double.astream_events(\"1234\"):\n", "    print(event)"]}, {"cell_type": "markdown", "id": "2a3efcd9", "metadata": {}, "source": ["## 后续步骤", "\n", "现在你已经学会了一些使用LangChain流式传输最终输出和内部步骤的方法。", "\n", "要了解更多信息，请查看本节中的其他操作指南，或参阅[Langchain表达式语言的概念指南](/docs/concepts/lcel/)。"]}], "metadata": {"kernelspec": {"display_name": "Python 3 (ipykernel)", "language": "python", "name": "python3"}, "language_info": {"codemirror_mode": {"name": "ipython", "version": 3}, "file_extension": ".py", "mimetype": "text/x-python", "name": "python", "nbconvert_exporter": "python", "pygments_lexer": "ipython3", "version": "3.10.4"}}, "nbformat": 4, "nbformat_minor": 5}