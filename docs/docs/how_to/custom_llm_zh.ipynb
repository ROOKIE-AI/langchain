{"cells": [{"cell_type": "markdown", "id": "9e9b7651", "metadata": {}, "source": ["# 如何创建一个自定义LLM类", "\n", "本笔记本介绍如何创建自定义LLM封装器，适用于以下场景：\n- 希望使用自己的LLM模型\n- 需要使用LangChain已支持封装器之外的替代方案", "\n", "将您的LLM封装为标准`LLM`接口后，只需极少的代码修改即可在现有LangChain程序中使用该模型。", "\n", "作为额外优势，您的LLM将自动成为LangChain的`Runnable`，并默认获得一些优化功能，如异步支持、`astream_events` API等。", "\n", ":::注意", "您当前正在查阅[文本补全模型](/docs/concepts/text_llms)的使用文档。许多最新且最受欢迎的模型属于[聊天补全模型](/docs/concepts/chat_models)。", "\n", "除非您正在专门使用更高级的提示技术，否则您可能正在寻找[此页面](/docs/how_to/custom_chat_model/)。", ":::", "\n", "## 实现", "\n", "自定义LLM只需实现以下两项必需内容：", "\n", "\n", "| 方法          | 描述                                                                       |", "|---------------|---------------------------------------------------------------------------|", "| `_call`       | 接收一个字符串和一些可选的停止词，返回一个字符串。由 `invoke` 调用。 |", "| `_llm_type`   | 一个返回字符串的属性，仅用于日志记录目的。", "\n", "\n", "\n", "可选实现方案：", "\n", "\n", "| 方法      | 描述                                                                                                     |", "|----------------------|-----------------------------------------------------------------------------------------------------------|", "| `_identifying_params` | 用于辅助识别模型并打印LLM信息；应返回一个字典。这是一个 **@property** 属性。 |", "| `_acall`              | 提供`_call`的异步原生实现，由`ainvoke`使用。                                    |", "| `_stream`             | 用于逐令牌流式输出结果的方法。                                                               |", "| `_astream`            | 提供 `_stream` 的异步原生实现；在较新的 LangChain 版本中，默认指向 `_stream`。 |", "\n", "\n", "\n", "让我们实现一个简单的自定义LLM，它仅返回输入内容的前n个字符。"]}, {"cell_type": "code", "execution_count": 1, "id": "2e9bb32f-6fd1-46ac-b32f-d175663710c0", "metadata": {"tags": []}, "outputs": [], "source": ["from typing import Any, Dict, Iterator, List, Mapping, Optional\n", "\n", "from langchain_core.callbacks.manager import CallbackManagerForLLMRun\n", "from langchain_core.language_models.llms import LLM\n", "from langchain_core.outputs import GenerationChunk\n", "\n", "\n", "class CustomLLM(LLM):\n", "    \"\"\"A custom chat model that echoes the first `n` characters of the input.\n", "\n", "    When contributing an implementation to LangChain, carefully document\n", "    the model including the initialization parameters, include\n", "    an example of how to initialize the model and include any relevant\n", "    links to the underlying models documentation or API.\n", "\n", "    Example:\n", "\n", "        .. code-block:: python\n", "\n", "            model = CustomChatModel(n=2)\n", "            result = model.invoke([HumanMessage(content=\"hello\")])\n", "            result = model.batch([[HumanMessage(content=\"hello\")],\n", "                                 [HumanMessage(content=\"world\")]])\n", "    \"\"\"\n", "\n", "    n: int\n", "    \"\"\"The number of characters from the last message of the prompt to be echoed.\"\"\"\n", "\n", "    def _call(\n", "        self,\n", "        prompt: str,\n", "        stop: Optional[List[str]] = None,\n", "        run_manager: Optional[CallbackManagerForLLMRun] = None,\n", "        **kwargs: Any,\n", "    ) -> str:\n", "        \"\"\"Run the LLM on the given input.\n", "\n", "        Override this method to implement the LLM logic.\n", "\n", "        Args:\n", "            prompt: The prompt to generate from.\n", "            stop: Stop words to use when generating. Model output is cut off at the\n", "                first occurrence of any of the stop substrings.\n", "                If stop tokens are not supported consider raising NotImplementedError.\n", "            run_manager: Callback manager for the run.\n", "            **kwargs: Arbitrary additional keyword arguments. These are usually passed\n", "                to the model provider API call.\n", "\n", "        Returns:\n", "            The model output as a string. Actual completions SHOULD NOT include the prompt.\n", "        \"\"\"\n", "        if stop is not None:\n", "            raise ValueError(\"stop kwargs are not permitted.\")\n", "        return prompt[: self.n]\n", "\n", "    def _stream(\n", "        self,\n", "        prompt: str,\n", "        stop: Optional[List[str]] = None,\n", "        run_manager: Optional[CallbackManagerForLLMRun] = None,\n", "        **kwargs: Any,\n", "    ) -> Iterator[GenerationChunk]:\n", "        \"\"\"Stream the LLM on the given prompt.\n", "\n", "        This method should be overridden by subclasses that support streaming.\n", "\n", "        If not implemented, the default behavior of calls to stream will be to\n", "        fallback to the non-streaming version of the model and return\n", "        the output as a single chunk.\n", "\n", "        Args:\n", "            prompt: The prompt to generate from.\n", "            stop: Stop words to use when generating. Model output is cut off at the\n", "                first occurrence of any of these substrings.\n", "            run_manager: Callback manager for the run.\n", "            **kwargs: Arbitrary additional keyword arguments. These are usually passed\n", "                to the model provider API call.\n", "\n", "        Returns:\n", "            An iterator of GenerationChunks.\n", "        \"\"\"\n", "        for char in prompt[: self.n]:\n", "            chunk = GenerationChunk(text=char)\n", "            if run_manager:\n", "                run_manager.on_llm_new_token(chunk.text, chunk=chunk)\n", "\n", "            yield chunk\n", "\n", "    @property\n", "    def _identifying_params(self) -> Dict[str, Any]:\n", "        \"\"\"Return a dictionary of identifying parameters.\"\"\"\n", "        return {\n", "            # The model name allows users to specify custom token counting\n", "            # rules in LLM monitoring applications (e.g., in LangSmith users\n", "            # can provide per token pricing for their model and monitor\n", "            # costs for the given LLM.)\n", "            \"model_name\": \"CustomChatModel\",\n", "        }\n", "\n", "    @property\n", "    def _llm_type(self) -> str:\n", "        \"\"\"Get the type of language model used by this chat model. Used for logging purposes only.\"\"\"\n", "        return \"custom\""]}, {"cell_type": "markdown", "id": "f614fb7b-e476-4d81-821b-57a2ebebe21c", "metadata": {"tags": []}, "source": ["### 让我们来测试一下 🧪"]}, {"cell_type": "markdown", "id": "e3feae15-4afc-49f4-8542-93867d4ea769", "metadata": {"tags": []}, "source": ["该LLM将实现LangChain的标准`Runnable`接口，许多LangChain抽象功能都支持此接口！"]}, {"cell_type": "code", "execution_count": 2, "id": "dfff4a95-99b2-4dba-b80d-9c3855046ef1", "metadata": {"tags": []}, "outputs": [{"name": "stdout", "output_type": "stream", "text": ["\u001b[1mCustomLLM\u001b[0m\n", "Params: {'model_name': 'CustomChatModel'}\n"]}], "source": ["llm = CustomLLM(n=5)\n", "print(llm)"]}, {"cell_type": "code", "execution_count": 3, "id": "8cd49199", "metadata": {"tags": []}, "outputs": [{"data": {"text/plain": ["'This '"]}, "execution_count": 3, "metadata": {}, "output_type": "execute_result"}], "source": ["llm.invoke(\"This is a foobar thing\")"]}, {"cell_type": "code", "execution_count": 4, "id": "511b3cb1-9c6f-49b6-9002-a2ec490632b0", "metadata": {"tags": []}, "outputs": [{"data": {"text/plain": ["'world'"]}, "execution_count": 4, "metadata": {}, "output_type": "execute_result"}], "source": ["await llm.ainvoke(\"world\")"]}, {"cell_type": "code", "execution_count": 5, "id": "d9d5bec2-d60a-4ebd-a97d-ac32c98ab02f", "metadata": {"tags": []}, "outputs": [{"data": {"text/plain": ["['woof ', 'meow ']"]}, "execution_count": 5, "metadata": {}, "output_type": "execute_result"}], "source": ["llm.batch([\"woof woof woof\", \"meow meow meow\"])"]}, {"cell_type": "code", "execution_count": 6, "id": "fe246b29-7a93-4bef-8861-389445598c25", "metadata": {"tags": []}, "outputs": [{"data": {"text/plain": ["['woof ', 'meow ']"]}, "execution_count": 6, "metadata": {}, "output_type": "execute_result"}], "source": ["await llm.abatch([\"woof woof woof\", \"meow meow meow\"])"]}, {"cell_type": "code", "execution_count": 7, "id": "3a67c38f-b83b-4eb9-a231-441c55ee8c82", "metadata": {"tags": []}, "outputs": [{"name": "stdout", "output_type": "stream", "text": ["h|e|l|l|o|"]}], "source": ["async for token in llm.astream(\"hello\"):\n", "    print(token, end=\"|\", flush=True)"]}, {"cell_type": "markdown", "id": "b62c282b-3a35-4529-aac4-2c2f0916790e", "metadata": {}, "source": ["让我们确认它能很好地与其他 `LangChain` API 集成。"]}, {"cell_type": "code", "execution_count": 15, "id": "d5578e74-7fa8-4673-afee-7a59d442aaff", "metadata": {"tags": []}, "outputs": [], "source": ["from langchain_core.prompts import ChatPromptTemplate"]}, {"cell_type": "code", "execution_count": 16, "id": "672ff664-8673-4832-9f4f-335253880141", "metadata": {"tags": []}, "outputs": [], "source": ["prompt = ChatPromptTemplate.from_messages(\n", "    [(\"system\", \"you are a bot\"), (\"human\", \"{input}\")]\n", ")"]}, {"cell_type": "code", "execution_count": 17, "id": "c400538a-9146-4c93-9fac-293d8f9ca6bf", "metadata": {"tags": []}, "outputs": [], "source": ["llm = CustomLLM(n=7)\n", "chain = prompt | llm"]}, {"cell_type": "code", "execution_count": 18, "id": "080964af-3e2d-4573-85cb-0d7cc58a6f42", "metadata": {"tags": []}, "outputs": [{"name": "stdout", "output_type": "stream", "text": ["{'event': 'on_chain_start', 'run_id': '05f24b4f-7ea3-4fb6-8417-3aa21633462f', 'name': 'RunnableSequence', 'tags': [], 'metadata': {}, 'data': {'input': {'input': 'hello there!'}}}\n", "{'event': 'on_prompt_start', 'name': 'ChatPromptTemplate', 'run_id': '7e996251-a926-4344-809e-c425a9846d21', 'tags': ['seq:step:1'], 'metadata': {}, 'data': {'input': {'input': 'hello there!'}}}\n", "{'event': 'on_prompt_end', 'name': 'ChatPromptTemplate', 'run_id': '7e996251-a926-4344-809e-c425a9846d21', 'tags': ['seq:step:1'], 'metadata': {}, 'data': {'input': {'input': 'hello there!'}, 'output': ChatPromptValue(messages=[SystemMessage(content='you are a bot'), HumanMessage(content='hello there!')])}}\n", "{'event': 'on_llm_start', 'name': 'CustomLLM', 'run_id': 'a8766beb-10f4-41de-8750-3ea7cf0ca7e2', 'tags': ['seq:step:2'], 'metadata': {}, 'data': {'input': {'prompts': ['System: you are a bot\\nHuman: hello there!']}}}\n", "{'event': 'on_llm_stream', 'name': 'CustomLLM', 'run_id': 'a8766beb-10f4-41de-8750-3ea7cf0ca7e2', 'tags': ['seq:step:2'], 'metadata': {}, 'data': {'chunk': 'S'}}\n", "{'event': 'on_chain_stream', 'run_id': '05f24b4f-7ea3-4fb6-8417-3aa21633462f', 'tags': [], 'metadata': {}, 'name': 'RunnableSequence', 'data': {'chunk': 'S'}}\n", "{'event': 'on_llm_stream', 'name': 'CustomLLM', 'run_id': 'a8766beb-10f4-41de-8750-3ea7cf0ca7e2', 'tags': ['seq:step:2'], 'metadata': {}, 'data': {'chunk': 'y'}}\n", "{'event': 'on_chain_stream', 'run_id': '05f24b4f-7ea3-4fb6-8417-3aa21633462f', 'tags': [], 'metadata': {}, 'name': 'RunnableSequence', 'data': {'chunk': 'y'}}\n"]}], "source": ["idx = 0\n", "async for event in chain.astream_events({\"input\": \"hello there!\"}, version=\"v1\"):\n", "    print(event)\n", "    idx += 1\n", "    if idx > 7:\n", "        # Truncate\n", "        break"]}, {"cell_type": "markdown", "id": "a85e848a-5316-4318-b770-3f8fd34f4231", "metadata": {}, "source": ["## 贡献指南", "\n", "我们感谢所有聊天模型集成方面的贡献。", "\n", "以下是一份清单，用于确保您的贡献能够被纳入LangChain：", "\n", "文档：", "\n", "* 该模型为所有初始化参数提供了文档字符串，因为这些内容将展示在[API参考文档](https://python.langchain.com/api_reference/langchain/index.html)中。", "* 如果模型由某项服务提供支持，则该类的文档字符串（doc-string）会包含指向模型API的链接。", "\n", "测试：", "\n", "* [ ] 为被重写的方法添加单元测试或集成测试。若你已重写相关代码，请验证`invoke`、`ainvoke`、`batch`、`stream`等功能是否正常运作。", "\n", "流式传输（如果您正在实现它）：", "\n", "* [ ] 确保调用 `on_llm_new_token` 回调函数", "* [ ] `on_llm_new_token` 在生成分块之前被调用", "\n", "停止标记行为：", "\n", "* [ ] 应遵守停止标记", "* [ ] 停止标记应作为响应的一部分被包含", "\n", "机密API密钥：", "\n", "* [ ] 如果您的模型需要连接API，初始化时可能会接收API密钥作为参数。对于这类敏感信息，请使用Pydantic的`SecretStr`类型进行封装，以避免用户打印模型时意外泄露密钥内容。"]}], "metadata": {"kernelspec": {"display_name": "Python 3 (ipykernel)", "language": "python", "name": "python3"}, "language_info": {"codemirror_mode": {"name": "ipython", "version": 3}, "file_extension": ".py", "mimetype": "text/x-python", "name": "python", "nbconvert_exporter": "python", "pygments_lexer": "ipython3", "version": "3.10.1"}}, "nbformat": 4, "nbformat_minor": 5}