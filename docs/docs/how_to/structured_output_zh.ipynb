{"cells": [{"cell_type": "raw", "id": "27598444", "metadata": {"vscode": {"languageId": "raw"}}, "source": ["---\n", "sidebar_position: 3\n", "keywords: [structured output, json, information extraction, with_structured_output]\n", "---"]}, {"cell_type": "markdown", "id": "6e3f0f72", "metadata": {}, "source": ["# 如何从模型返回结构化数据", "\n", ":::info 前提条件", "\n", "本指南假定您熟悉以下概念：", "- [聊天模型](/docs/concepts/chat_models)", "- [函数/工具调用](/docs/concepts/tool_calling)", ":::", "\n", "让模型返回符合特定[模式](/docs/concepts/structured_outputs/)的输出通常很有用。一个常见的应用场景是从文本中提取数据，以便插入数据库或与其他下游系统一起使用。本指南介绍了几种从模型获取结构化输出的策略。", "\n", "## `.with_structured_output()` 方法", "\n", "<span data-heading-keywords=\"with_structured_output\"></span>\n", "\n", ":::info 支持的模型", "\n", "您可以在此处找到[支持此方法的模型列表](/docs/integrations/chat/)。", "\n", ":::", "\n", "这是获取结构化输出的最简单且最可靠的方法。`with_structured_output()` 专为[提供原生结构化输出API的模型](/docs/integrations/chat/)（如工具/函数调用或JSON模式）实现，并在底层利用这些功能。", "\n", "该方法接收一个模式（schema）作为输入，该模式指定了期望输出属性的名称、类型和描述。该方法返回一个类似模型的 Runnable 对象，但与输出字符串或[消息](/docs/concepts/messages/)不同，它会输出与给定模式对应的对象。该模式可以指定为 TypedDict 类、[JSON Schema](https://json-schema.org/) 或 Pydantic 类。如果使用 TypedDict 或 JSON Schema，则 Runnable 将返回一个字典；如果使用 Pydantic 类，则返回一个 Pydantic 对象。", "\n", "举个例子，我们让模型生成一个笑话，并将铺垫部分与笑点分开：", "\n", "import ChatModelTabs from \"@theme/ChatModelTabs\";", "\n", "<ChatModelTabs\n", "customVarName=\"llm\"", "/>"]}, {"cell_type": "code", "execution_count": 1, "id": "6d55008f", "metadata": {}, "outputs": [], "source": ["# | output: false\n", "# | echo: false\n", "\n", "from langchain_openai import ChatOpenAI\n", "\n", "llm = ChatOpenAI(model=\"gpt-4o\", temperature=0)"]}, {"cell_type": "markdown", "id": "a808a401-be1f-49f9-ad13-58dd68f7db5f", "metadata": {}, "source": ["### Pydantic 类", "\n", "如果我们希望模型返回一个 Pydantic 对象，只需传入所需的 Pydantic 类即可。使用 Pydantic 的主要优势在于，模型生成的输出将经过验证。如果缺少任何必填字段或任何字段类型错误，Pydantic 都会引发错误。"]}, {"cell_type": "code", "execution_count": 2, "id": "070bf702", "metadata": {}, "outputs": [{"data": {"text/plain": ["Joke(setup='Why was the cat sitting on the computer?', punchline='Because it wanted to keep an eye on the mouse!', rating=7)"]}, "execution_count": 2, "metadata": {}, "output_type": "execute_result"}], "source": ["from typing import Optional\n", "\n", "from pydantic import BaseModel, Field\n", "\n", "\n", "# Pydantic\n", "class Joke(BaseModel):\n", "    \"\"\"Joke to tell user.\"\"\"\n", "\n", "    setup: str = Field(description=\"The setup of the joke\")\n", "    punchline: str = Field(description=\"The punchline to the joke\")\n", "    rating: Optional[int] = Field(\n", "        default=None, description=\"How funny the joke is, from 1 to 10\"\n", "    )\n", "\n", "\n", "structured_llm = llm.with_structured_output(Joke)\n", "\n", "structured_llm.invoke(\"Tell me a joke about cats\")"]}, {"cell_type": "markdown", "id": "00890a47-3cdf-4805-b8f1-6d110f0633d3", "metadata": {}, "source": [":::提示", "除了Pydantic类的结构之外，Pydantic类的名称、文档字符串（docstring）、参数的名称及其提供的描述也非常重要。大多数情况下，`with_structured_output`会使用模型的功能/工具调用API，你可以有效地认为所有这些信息都被添加到了模型提示（prompt）中。", "好的,我会严格按照要求进行翻译,确保markdown格式一致。以下是一个示例翻译:\n\n# Getting Started with Markdown\n\nMarkdown is a lightweight markup language that you can use to add formatting elements to plaintext text documents. \n\n## Basic Syntax\n\nHere are some of the most commonly used markdown syntax:\n\n- **Headers**: Use `#` for h1, `##` for h2 etc.\n- *Emphasis*: Use `*` or `_` for *italic*, `**` or `__` for **bold**\n- Lists:\n  - Unordered lists use `-`, `*` or `+`\n  - Ordered lists use numbers\n- `Code`: Wrap inline code with backticks (`` ` ``)\n- [Links](https://example.com): `[text](URL)`\n- Images: `![alt text](image.jpg)`\n\n> Blockquotes are created using `>` characters\n\n```python\n# Code blocks\ndef hello():\n    print(\"Hello Markdown!\")\n```"]}, {"cell_type": "markdown", "id": "deddb6d3", "metadata": {}, "source": ["### TypedDict 或 JSON 模式", "\n", "如果你不想使用 Pydantic、明确不希望进行参数验证，或者希望能够流式传输模型输出，你可以使用 TypedDict 类来定义你的模式。我们可以选择使用 LangChain 支持的特殊 `Annotated` 语法，该语法允许你指定字段的默认值和描述。请注意，如果模型未生成该字段，默认值*不会*自动填充，它仅用于传递给模型的模式定义中。", "\n", ":::info 要求", "\n", "- 核心库：`langchain-core>=0.2.26`", "- 类型扩展：强烈建议从 `typing_extensions` 导入 `Annotated` 和 `TypedDict`，而非从 `typing` 导入，以确保在不同 Python 版本中的行为一致。", "\n", ":::"]}, {"cell_type": "code", "execution_count": 3, "id": "70d82891-42e8-424a-919e-07d83bcfec61", "metadata": {}, "outputs": [{"data": {"text/plain": ["{'setup': 'Why was the cat sitting on the computer?',\n", " 'punchline': 'Because it wanted to keep an eye on the mouse!',\n", " 'rating': 7}"]}, "execution_count": 3, "metadata": {}, "output_type": "execute_result"}], "source": ["from typing import Optional\n", "\n", "from typing_extensions import Annotated, TypedDict\n", "\n", "\n", "# TypedDict\n", "class Joke(TypedDict):\n", "    \"\"\"Joke to tell user.\"\"\"\n", "\n", "    setup: Annotated[str, ..., \"The setup of the joke\"]\n", "\n", "    # Alternatively, we could have specified setup as:\n", "\n", "    # setup: str                    # no default, no description\n", "    # setup: Annotated[str, ...]    # no default, no description\n", "    # setup: Annotated[str, \"foo\"]  # default, no description\n", "\n", "    punchline: Annotated[str, ..., \"The punchline of the joke\"]\n", "    rating: Annotated[Optional[int], None, \"How funny the joke is, from 1 to 10\"]\n", "\n", "\n", "structured_llm = llm.with_structured_output(Joke)\n", "\n", "structured_llm.invoke(\"Tell me a joke about cats\")"]}, {"cell_type": "markdown", "id": "e4d7b4dc-f617-4ea8-aa58-847c228791b4", "metadata": {}, "source": ["同样地，我们可以传入一个 [JSON Schema](https://json-schema.org/) 字典。这种方式无需导入任何模块或类，能非常清晰地展示每个参数的文档说明，但代价是代码会略显冗长。"]}, {"cell_type": "code", "execution_count": 4, "id": "6700994a", "metadata": {}, "outputs": [{"data": {"text/plain": ["{'setup': 'Why was the cat sitting on the computer?',\n", " 'punchline': 'Because it wanted to keep an eye on the mouse!',\n", " 'rating': 7}"]}, "execution_count": 4, "metadata": {}, "output_type": "execute_result"}], "source": ["json_schema = {\n", "    \"title\": \"joke\",\n", "    \"description\": \"Joke to tell user.\",\n", "    \"type\": \"object\",\n", "    \"properties\": {\n", "        \"setup\": {\n", "            \"type\": \"string\",\n", "            \"description\": \"The setup of the joke\",\n", "        },\n", "        \"punchline\": {\n", "            \"type\": \"string\",\n", "            \"description\": \"The punchline to the joke\",\n", "        },\n", "        \"rating\": {\n", "            \"type\": \"integer\",\n", "            \"description\": \"How funny the joke is, from 1 to 10\",\n", "            \"default\": None,\n", "        },\n", "    },\n", "    \"required\": [\"setup\", \"punchline\"],\n", "}\n", "structured_llm = llm.with_structured_output(json_schema)\n", "\n", "structured_llm.invoke(\"Tell me a joke about cats\")"]}, {"cell_type": "markdown", "id": "3da57988", "metadata": {}, "source": ["### 在多个模式之间进行选择", "\n", "让模型从多个模式中进行选择的最简单方法是创建一个具有联合类型属性的父模式。", "\n", "#### 使用 Pydantic"]}, {"cell_type": "code", "execution_count": 7, "id": "9194bcf2", "metadata": {}, "outputs": [{"data": {"text/plain": ["FinalResponse(final_output=Joke(setup='Why was the cat sitting on the computer?', punchline='Because it wanted to keep an eye on the mouse!', rating=7))"]}, "execution_count": 7, "metadata": {}, "output_type": "execute_result"}], "source": ["from typing import Union\n", "\n", "\n", "class Joke(BaseModel):\n", "    \"\"\"Joke to tell user.\"\"\"\n", "\n", "    setup: str = Field(description=\"The setup of the joke\")\n", "    punchline: str = Field(description=\"The punchline to the joke\")\n", "    rating: Optional[int] = Field(\n", "        default=None, description=\"How funny the joke is, from 1 to 10\"\n", "    )\n", "\n", "\n", "class ConversationalResponse(BaseModel):\n", "    \"\"\"Respond in a conversational manner. Be kind and helpful.\"\"\"\n", "\n", "    response: str = Field(description=\"A conversational response to the user's query\")\n", "\n", "\n", "class FinalResponse(BaseModel):\n", "    final_output: Union[Joke, ConversationalResponse]\n", "\n", "\n", "structured_llm = llm.with_structured_output(FinalResponse)\n", "\n", "structured_llm.invoke(\"Tell me a joke about cats\")"]}, {"cell_type": "code", "execution_count": 8, "id": "84d86132", "metadata": {}, "outputs": [{"data": {"text/plain": ["FinalResponse(final_output=ConversationalResponse(response=\"I'm just a computer program, so I don't have feelings, but I'm here and ready to help you with whatever you need!\"))"]}, "execution_count": 8, "metadata": {}, "output_type": "execute_result"}], "source": ["structured_llm.invoke(\"How are you today?\")"]}, {"cell_type": "markdown", "id": "8b087112c23bafcd", "metadata": {}, "source": ["#### 使用 TypedDict"]}, {"cell_type": "code", "execution_count": 9, "id": "eb0d5855-feba-48fb-84ea-9acb0edb238b", "metadata": {}, "outputs": [{"data": {"text/plain": ["{'final_output': {'setup': 'Why was the cat sitting on the computer?',\n", "  'punchline': 'Because it wanted to keep an eye on the mouse!',\n", "  'rating': 7}}"]}, "execution_count": 9, "metadata": {}, "output_type": "execute_result"}], "source": ["from typing import Optional, Union\n", "\n", "from typing_extensions import Annotated, TypedDict\n", "\n", "\n", "class Joke(TypedDict):\n", "    \"\"\"Joke to tell user.\"\"\"\n", "\n", "    setup: Annotated[str, ..., \"The setup of the joke\"]\n", "    punchline: Annotated[str, ..., \"The punchline of the joke\"]\n", "    rating: Annotated[Optional[int], None, \"How funny the joke is, from 1 to 10\"]\n", "\n", "\n", "class ConversationalResponse(TypedDict):\n", "    \"\"\"Respond in a conversational manner. Be kind and helpful.\"\"\"\n", "\n", "    response: Annotated[str, ..., \"A conversational response to the user's query\"]\n", "\n", "\n", "class FinalResponse(TypedDict):\n", "    final_output: Union[Joke, ConversationalResponse]\n", "\n", "\n", "structured_llm = llm.with_structured_output(FinalResponse)\n", "\n", "structured_llm.invoke(\"Tell me a joke about cats\")"]}, {"cell_type": "code", "execution_count": 10, "id": "ec753809-c2c1-41c0-a3c5-69855d65475b", "metadata": {}, "outputs": [{"data": {"text/plain": ["{'final_output': {'response': \"I'm just a computer program, so I don't have feelings, but I'm here and ready to help you with whatever you need!\"}}"]}, "execution_count": 10, "metadata": {}, "output_type": "execute_result"}], "source": ["structured_llm.invoke(\"How are you today?\")"]}, {"cell_type": "markdown", "id": "dd22149ac9d41d57", "metadata": {}, "source": ["响应应当与Pydantic示例中展示的完全一致。"]}, {"cell_type": "markdown", "id": "e28c14d3", "metadata": {}, "source": ["或者，你也可以直接使用工具调用功能，让模型在选项之间进行选择（前提是[你选用的模型支持此功能](/docs/integrations/chat/)）。这种方法需要稍多的解析和设置工作，但在某些情况下能带来更好的性能表现，因为你无需使用嵌套模式。更多细节请参阅[这份操作指南](/docs/how_to/tool_calling)。"]}, {"cell_type": "markdown", "id": "9a40f703-7fd2-4fe0-ab2a-fa2d711ba009", "metadata": {}, "source": ["### 流式传输", "\n", "当输出类型为字典（即模式被指定为TypedDict类或JSON Schema字典）时，我们可以从结构化模型中流式传输输出。", "\n", ":::信息", "\n", "请注意，所生成的是已经聚合的块，而非增量数据。", "\n", ":::"]}, {"cell_type": "code", "execution_count": 11, "id": "aff89877-28a3-472f-a1aa-eff893fe7736", "metadata": {}, "outputs": [{"name": "stdout", "output_type": "stream", "text": ["{}\n", "{'setup': ''}\n", "{'setup': 'Why'}\n", "{'setup': 'Why was'}\n", "{'setup': 'Why was the'}\n", "{'setup': 'Why was the cat'}\n", "{'setup': 'Why was the cat sitting'}\n", "{'setup': 'Why was the cat sitting on'}\n", "{'setup': 'Why was the cat sitting on the'}\n", "{'setup': 'Why was the cat sitting on the computer'}\n", "{'setup': 'Why was the cat sitting on the computer?'}\n", "{'setup': 'Why was the cat sitting on the computer?', 'punchline': ''}\n", "{'setup': 'Why was the cat sitting on the computer?', 'punchline': 'Because'}\n", "{'setup': 'Why was the cat sitting on the computer?', 'punchline': 'Because it'}\n", "{'setup': 'Why was the cat sitting on the computer?', 'punchline': 'Because it wanted'}\n", "{'setup': 'Why was the cat sitting on the computer?', 'punchline': 'Because it wanted to'}\n", "{'setup': 'Why was the cat sitting on the computer?', 'punchline': 'Because it wanted to keep'}\n", "{'setup': 'Why was the cat sitting on the computer?', 'punchline': 'Because it wanted to keep an'}\n", "{'setup': 'Why was the cat sitting on the computer?', 'punchline': 'Because it wanted to keep an eye'}\n", "{'setup': 'Why was the cat sitting on the computer?', 'punchline': 'Because it wanted to keep an eye on'}\n", "{'setup': 'Why was the cat sitting on the computer?', 'punchline': 'Because it wanted to keep an eye on the'}\n", "{'setup': 'Why was the cat sitting on the computer?', 'punchline': 'Because it wanted to keep an eye on the mouse'}\n", "{'setup': 'Why was the cat sitting on the computer?', 'punchline': 'Because it wanted to keep an eye on the mouse!'}\n", "{'setup': 'Why was the cat sitting on the computer?', 'punchline': 'Because it wanted to keep an eye on the mouse!', 'rating': 7}\n"]}], "source": ["from typing_extensions import Annotated, TypedDict\n", "\n", "\n", "# TypedDict\n", "class Joke(TypedDict):\n", "    \"\"\"Joke to tell user.\"\"\"\n", "\n", "    setup: Annotated[str, ..., \"The setup of the joke\"]\n", "    punchline: Annotated[str, ..., \"The punchline of the joke\"]\n", "    rating: Annotated[Optional[int], None, \"How funny the joke is, from 1 to 10\"]\n", "\n", "\n", "structured_llm = llm.with_structured_output(Joke)\n", "\n", "for chunk in structured_llm.stream(\"Tell me a joke about cats\"):\n", "    print(chunk)"]}, {"cell_type": "markdown", "id": "0a526cdf-e736-451b-96be-22e8986d3863", "metadata": {}, "source": ["### 少量样本提示", "\n", "对于更复杂的模式，在提示中添加少量示例非常有用。这可以通过几种方式实现。", "\n", "最简单且最通用的方法是在提示中的系统消息里添加示例："]}, {"cell_type": "code", "execution_count": 12, "id": "283ba784-2072-47ee-9b2c-1119e3c69e8e", "metadata": {}, "outputs": [{"data": {"text/plain": ["{'setup': 'Woodpecker',\n", " 'punchline': \"Woodpecker you a joke, but I'm afraid it might be too 'hole-some'!\",\n", " 'rating': 7}"]}, "execution_count": 12, "metadata": {}, "output_type": "execute_result"}], "source": ["from langchain_core.prompts import ChatPromptTemplate\n", "\n", "system = \"\"\"You are a hilarious comedian. Your specialty is knock-knock jokes. \\\n", "Return a joke which has the setup (the response to \"Who's there?\") and the final punchline (the response to \"<setup> who?\").\n", "\n", "Here are some examples of jokes:\n", "\n", "example_user: Tell me a joke about planes\n", "example_assistant: {{\"setup\": \"Why don't planes ever get tired?\", \"punchline\": \"Because they have rest wings!\", \"rating\": 2}}\n", "\n", "example_user: Tell me another joke about planes\n", "example_assistant: {{\"setup\": \"Cargo\", \"punchline\": \"Cargo 'vroom vroom', but planes go 'zoom zoom'!\", \"rating\": 10}}\n", "\n", "example_user: Now about caterpillars\n", "example_assistant: {{\"setup\": \"Caterpillar\", \"punchline\": \"Caterpillar really slow, but watch me turn into a butterfly and steal the show!\", \"rating\": 5}}\"\"\"\n", "\n", "prompt = ChatPromptTemplate.from_messages([(\"system\", system), (\"human\", \"{input}\")])\n", "\n", "few_shot_structured_llm = prompt | structured_llm\n", "few_shot_structured_llm.invoke(\"what's something funny about woodpeckers\")"]}, {"cell_type": "markdown", "id": "3c12b389-153d-44d1-af34-37e5b926d3db", "metadata": {}, "source": ["当构建输出的底层方法是工具调用时，我们可以将示例作为显式工具调用传入。您可以通过API参考文档确认当前使用的模型是否支持工具调用功能。"]}, {"cell_type": "code", "execution_count": 13, "id": "d7381cb0-b2c3-4302-a319-ed72d0b9e43f", "metadata": {}, "outputs": [{"data": {"text/plain": ["{'setup': 'Crocodile',\n", " 'punchline': 'Crocodile be seeing you later, alligator!',\n", " 'rating': 6}"]}, "execution_count": 13, "metadata": {}, "output_type": "execute_result"}], "source": ["from langchain_core.messages import AIMessage, HumanMessage, ToolMessage\n", "\n", "examples = [\n", "    HumanMessage(\"Tell me a joke about planes\", name=\"example_user\"),\n", "    AIMessage(\n", "        \"\",\n", "        name=\"example_assistant\",\n", "        tool_calls=[\n", "            {\n", "                \"name\": \"joke\",\n", "                \"args\": {\n", "                    \"setup\": \"Why don't planes ever get tired?\",\n", "                    \"punchline\": \"Because they have rest wings!\",\n", "                    \"rating\": 2,\n", "                },\n", "                \"id\": \"1\",\n", "            }\n", "        ],\n", "    ),\n", "    # Most tool-calling models expect a ToolMessage(s) to follow an AIMessage with tool calls.\n", "    ToolMessage(\"\", tool_call_id=\"1\"),\n", "    # Some models also expect an AIMessage to follow any ToolMessages,\n", "    # so you may need to add an AIMessage here.\n", "    HumanMessage(\"Tell me another joke about planes\", name=\"example_user\"),\n", "    AIMessage(\n", "        \"\",\n", "        name=\"example_assistant\",\n", "        tool_calls=[\n", "            {\n", "                \"name\": \"joke\",\n", "                \"args\": {\n", "                    \"setup\": \"Cargo\",\n", "                    \"punchline\": \"Cargo 'vroom vroom', but planes go 'zoom zoom'!\",\n", "                    \"rating\": 10,\n", "                },\n", "                \"id\": \"2\",\n", "            }\n", "        ],\n", "    ),\n", "    ToolMessage(\"\", tool_call_id=\"2\"),\n", "    HumanMessage(\"Now about caterpillars\", name=\"example_user\"),\n", "    AIMessage(\n", "        \"\",\n", "        tool_calls=[\n", "            {\n", "                \"name\": \"joke\",\n", "                \"args\": {\n", "                    \"setup\": \"Caterpillar\",\n", "                    \"punchline\": \"Caterpillar really slow, but watch me turn into a butterfly and steal the show!\",\n", "                    \"rating\": 5,\n", "                },\n", "                \"id\": \"3\",\n", "            }\n", "        ],\n", "    ),\n", "    ToolMessage(\"\", tool_call_id=\"3\"),\n", "]\n", "system = \"\"\"You are a hilarious comedian. Your specialty is knock-knock jokes. \\\n", "Return a joke which has the setup (the response to \"Who's there?\") \\\n", "and the final punchline (the response to \"<setup> who?\").\"\"\"\n", "\n", "prompt = ChatPromptTemplate.from_messages(\n", "    [(\"system\", system), (\"placeholder\", \"{examples}\"), (\"human\", \"{input}\")]\n", ")\n", "few_shot_structured_llm = prompt | structured_llm\n", "few_shot_structured_llm.invoke({\"input\": \"crocodiles\", \"examples\": examples})"]}, {"cell_type": "markdown", "id": "498d893b-ceaa-47ff-a9d8-4faa60702715", "metadata": {}, "source": ["有关使用工具调用时的小样本提示的更多信息，请参阅[此处](/docs/how_to/tools_few_shot/)。"]}, {"cell_type": "markdown", "id": "39d7a555", "metadata": {}, "source": ["### （高级）指定结构化输出的方法", "\n", "对于支持多种输出结构方式的模型（即同时支持工具调用和JSON模式），您可以通过`method=`参数指定要使用的方法。", "\n", ":::info JSON 模式", "\n", "如果使用 JSON 模式，你仍需在模型提示中指定所需的模式。传递给 `with_structured_output` 的模式仅用于解析模型输出，不会像工具调用那样直接传递给模型。", "\n", "要查看您使用的模型是否支持 JSON 模式，请查阅 [API 参考](https://python.langchain.com/api_reference/langchain/index.html) 中该模型的条目。", "\n", "好的,我会按照您的要求进行翻译,确保输出标准的markdown格式内容,不显示任何额外标记。以下是一个示例翻译:\n\n# 欢迎使用翻译助手\n\n这是一个标准的markdown格式翻译示例:\n\n## 二级标题示例\n\n- 列表项1\n- 列表项2\n- 列表项3\n\n**加粗文本** 和 *斜体文本*\n\n> 引用区块示例\n\n`行内代码` 和 \n\n```\n代码块\n```\n\n[链接文本](https://example.com)\n\n![图片描述](image.jpg)\n\n表格示例:\n\n| 列1 | 列2 | 列3 |\n|-----|-----|-----|\n| 数据1 | 数据2 | 数据3 |\n| 数据4 | 数据5 | 数据6 |\n\n请提供您需要翻译的具体英文内容,我会保持相同的markdown格式进行翻译。"]}, {"cell_type": "code", "execution_count": 14, "id": "df0370e3", "metadata": {}, "outputs": [{"data": {"text/plain": ["{'setup': 'Why was the cat sitting on the computer?',\n", " 'punchline': 'Because it wanted to keep an eye on the mouse!'}"]}, "execution_count": 14, "metadata": {}, "output_type": "execute_result"}], "source": ["structured_llm = llm.with_structured_output(None, method=\"json_mode\")\n", "\n", "structured_llm.invoke(\n", "    \"Tell me a joke about cats, respond in JSON with `setup` and `punchline` keys\"\n", ")"]}, {"cell_type": "markdown", "id": "91e95aa2", "metadata": {}, "source": ["### （高级）原始输出", "\n", "大型语言模型（LLMs）在生成结构化输出时并不完美，尤其是当数据结构变得复杂时。您可以通过传递参数 `include_raw=True` 来避免抛出异常，并自行处理原始输出。这将改变输出格式，使其包含原始消息输出、解析后的值（如果成功）以及可能出现的错误："]}, {"cell_type": "code", "execution_count": 17, "id": "10ed2842", "metadata": {}, "outputs": [{"data": {"text/plain": ["{'raw': AIMessage(content='', additional_kwargs={'tool_calls': [{'id': 'call_f25ZRmh8u5vHlOWfTUw8sJFZ', 'function': {'arguments': '{\"setup\":\"Why was the cat sitting on the computer?\",\"punchline\":\"Because it wanted to keep an eye on the mouse!\",\"rating\":7}', 'name': 'Joke'}, 'type': 'function'}]}, response_metadata={'token_usage': {'completion_tokens': 33, 'prompt_tokens': 93, 'total_tokens': 126}, 'model_name': 'gpt-4o-2024-05-13', 'system_fingerprint': 'fp_4e2b2da518', 'finish_reason': 'stop', 'logprobs': None}, id='run-d880d7e2-df08-4e9e-ad92-dfc29f2fd52f-0', tool_calls=[{'name': 'Joke', 'args': {'setup': 'Why was the cat sitting on the computer?', 'punchline': 'Because it wanted to keep an eye on the mouse!', 'rating': 7}, 'id': 'call_f25ZRmh8u5vHlOWfTUw8sJFZ', 'type': 'tool_call'}], usage_metadata={'input_tokens': 93, 'output_tokens': 33, 'total_tokens': 126}),\n", " 'parsed': {'setup': 'Why was the cat sitting on the computer?',\n", "  'punchline': 'Because it wanted to keep an eye on the mouse!',\n", "  'rating': 7},\n", " 'parsing_error': None}"]}, "execution_count": 17, "metadata": {}, "output_type": "execute_result"}], "source": ["structured_llm = llm.with_structured_output(Joke, include_raw=True)\n", "\n", "structured_llm.invoke(\"Tell me a joke about cats\")"]}, {"cell_type": "markdown", "id": "5e92a98a", "metadata": {}, "source": ["## 直接提示与解析模型输出", "\n", "并非所有模型都支持 `.with_structured_output()` 方法，因为并非所有模型都具备工具调用或 JSON 模式支持功能。对于此类模型，您需要直接提示模型使用特定格式，并通过输出解析器从原始模型输出中提取结构化响应。", "\n", "### 使用 `PydanticOutputParser`", "\n", "以下示例使用内置的 [`PydanticOutputParser`](https://python.langchain.com/api_reference/core/output_parsers/langchain_core.output_parsers.pydantic.PydanticOutputParser.html) 来解析聊天模型的输出，该模型被提示匹配给定的 Pydantic 模式。请注意，我们通过解析器的方法直接将 `format_instructions` 添加到提示中："]}, {"cell_type": "code", "execution_count": 31, "id": "6e514455", "metadata": {}, "outputs": [], "source": ["from typing import List\n", "\n", "from langchain_core.output_parsers import PydanticOutputParser\n", "from langchain_core.prompts import ChatPromptTemplate\n", "from pydantic import BaseModel, Field\n", "\n", "\n", "class Person(BaseModel):\n", "    \"\"\"Information about a person.\"\"\"\n", "\n", "    name: str = Field(..., description=\"The name of the person\")\n", "    height_in_meters: float = Field(\n", "        ..., description=\"The height of the person expressed in meters.\"\n", "    )\n", "\n", "\n", "class People(BaseModel):\n", "    \"\"\"Identifying information about all people in a text.\"\"\"\n", "\n", "    people: List[Person]\n", "\n", "\n", "# Set up a parser\n", "parser = PydanticOutputParser(pydantic_object=People)\n", "\n", "# Prompt\n", "prompt = ChatPromptTemplate.from_messages(\n", "    [\n", "        (\n", "            \"system\",\n", "            \"Answer the user query. Wrap the output in `json` tags\\n{format_instructions}\",\n", "        ),\n", "        (\"human\", \"{query}\"),\n", "    ]\n", ").partial(format_instructions=parser.get_format_instructions())"]}, {"cell_type": "markdown", "id": "082fa166", "metadata": {}, "source": ["让我们看看发送给模型的信息有哪些："]}, {"cell_type": "code", "execution_count": 37, "id": "3d73d33d", "metadata": {}, "outputs": [{"name": "stdout", "output_type": "stream", "text": ["System: Answer the user query. Wrap the output in `json` tags\n", "The output should be formatted as a JSON instance that conforms to the JSON schema below.\n", "\n", "As an example, for the schema {\"properties\": {\"foo\": {\"title\": \"Foo\", \"description\": \"a list of strings\", \"type\": \"array\", \"items\": {\"type\": \"string\"}}}, \"required\": [\"foo\"]}\n", "the object {\"foo\": [\"bar\", \"baz\"]} is a well-formatted instance of the schema. The object {\"properties\": {\"foo\": [\"bar\", \"baz\"]}} is not well-formatted.\n", "\n", "Here is the output schema:\n", "```\n", "{\"description\": \"Identifying information about all people in a text.\", \"properties\": {\"people\": {\"title\": \"People\", \"type\": \"array\", \"items\": {\"$ref\": \"#/definitions/Person\"}}}, \"required\": [\"people\"], \"definitions\": {\"Person\": {\"title\": \"Person\", \"description\": \"Information about a person.\", \"type\": \"object\", \"properties\": {\"name\": {\"title\": \"Name\", \"description\": \"The name of the person\", \"type\": \"string\"}, \"height_in_meters\": {\"title\": \"Height In Meters\", \"description\": \"The height of the person expressed in meters.\", \"type\": \"number\"}}, \"required\": [\"name\", \"height_in_meters\"]}}}\n", "```\n", "Human: Anna is 23 years old and she is 6 feet tall\n"]}], "source": ["query = \"Anna is 23 years old and she is 6 feet tall\"\n", "\n", "print(prompt.invoke({\"query\": query}).to_string())"]}, {"cell_type": "markdown", "id": "081956b9", "metadata": {}, "source": ["现在让我们来调用它："]}, {"cell_type": "code", "execution_count": 9, "id": "8d6b3d17", "metadata": {}, "outputs": [{"data": {"text/plain": ["People(people=[Person(name='Anna', height_in_meters=1.8288)])"]}, "execution_count": 9, "metadata": {}, "output_type": "execute_result"}], "source": ["chain = prompt | llm | parser\n", "\n", "chain.invoke({\"query\": query})"]}, {"cell_type": "markdown", "id": "6732dd87", "metadata": {}, "source": ["要深入了解如何结合提示技术使用输出解析器来生成结构化输出，请参阅[本指南](/docs/how_to/output_parser_structured)。", "\n", "### 自定义解析", "\n", "你也可以使用 [LangChain 表达式语言 (LCEL)](/docs/concepts/lcel) 创建自定义提示和解析器，通过普通函数来解析模型的输出："]}, {"cell_type": "code", "execution_count": 10, "id": "e8d37e15", "metadata": {}, "outputs": [], "source": ["import json\n", "import re\n", "from typing import List\n", "\n", "from langchain_core.messages import AIMessage\n", "from langchain_core.prompts import ChatPromptTemplate\n", "from pydantic import BaseModel, Field\n", "\n", "\n", "class Person(BaseModel):\n", "    \"\"\"Information about a person.\"\"\"\n", "\n", "    name: str = Field(..., description=\"The name of the person\")\n", "    height_in_meters: float = Field(\n", "        ..., description=\"The height of the person expressed in meters.\"\n", "    )\n", "\n", "\n", "class People(BaseModel):\n", "    \"\"\"Identifying information about all people in a text.\"\"\"\n", "\n", "    people: List[Person]\n", "\n", "\n", "# Prompt\n", "prompt = ChatPromptTemplate.from_messages(\n", "    [\n", "        (\n", "            \"system\",\n", "            \"Answer the user query. Output your answer as JSON that  \"\n", "            \"matches the given schema: ```json\\n{schema}\\n```. \"\n", "            \"Make sure to wrap the answer in ```json and ``` tags\",\n", "        ),\n", "        (\"human\", \"{query}\"),\n", "    ]\n", ").partial(schema=People.schema())\n", "\n", "\n", "# Custom parser\n", "def extract_json(message: AIMessage) -> List[dict]:\n", "    \"\"\"Extracts JSON content from a string where JSON is embedded between ```json and ``` tags.\n", "\n", "    Parameters:\n", "        text (str): The text containing the JSON content.\n", "\n", "    Returns:\n", "        list: A list of extracted JSON strings.\n", "    \"\"\"\n", "    text = message.content\n", "    # Define the regular expression pattern to match JSON blocks\n", "    pattern = r\"```json(.*?)```\"\n", "\n", "    # Find all non-overlapping matches of the pattern in the string\n", "    matches = re.findall(pattern, text, re.DOTALL)\n", "\n", "    # Return the list of matched JSON strings, stripping any leading or trailing whitespace\n", "    try:\n", "        return [json.loads(match.strip()) for match in matches]\n", "    except Exception:\n", "        raise ValueError(f\"Failed to parse: {message}\")"]}, {"cell_type": "markdown", "id": "9f1bc8f7", "metadata": {}, "source": ["以下是发送给模型的提示："]}, {"cell_type": "code", "execution_count": 11, "id": "c8a30d0e", "metadata": {}, "outputs": [{"name": "stdout", "output_type": "stream", "text": ["System: Answer the user query. Output your answer as JSON that  matches the given schema: ```json\n", "{'title': 'People', 'description': 'Identifying information about all people in a text.', 'type': 'object', 'properties': {'people': {'title': 'People', 'type': 'array', 'items': {'$ref': '#/definitions/Person'}}}, 'required': ['people'], 'definitions': {'Person': {'title': 'Person', 'description': 'Information about a person.', 'type': 'object', 'properties': {'name': {'title': 'Name', 'description': 'The name of the person', 'type': 'string'}, 'height_in_meters': {'title': 'Height In Meters', 'description': 'The height of the person expressed in meters.', 'type': 'number'}}, 'required': ['name', 'height_in_meters']}}}\n", "```. Make sure to wrap the answer in ```json and ``` tags\n", "Human: Anna is 23 years old and she is 6 feet tall\n"]}], "source": ["query = \"Anna is 23 years old and she is 6 feet tall\"\n", "\n", "print(prompt.format_prompt(query=query).to_string())"]}, {"cell_type": "markdown", "id": "ec018893", "metadata": {}, "source": ["以下是我们调用它时的效果："]}, {"cell_type": "code", "execution_count": 12, "id": "e1e7baf6", "metadata": {}, "outputs": [{"data": {"text/plain": ["[{'people': [{'name': 'Anna', 'height_in_meters': 1.8288}]}]"]}, "execution_count": 12, "metadata": {}, "output_type": "execute_result"}], "source": ["chain = prompt | llm | extract_json\n", "\n", "chain.invoke({\"query\": query})"]}], "metadata": {"kernelspec": {"display_name": "Python 3 (ipykernel)", "language": "python", "name": "python3"}, "language_info": {"codemirror_mode": {"name": "ipython", "version": 3}, "file_extension": ".py", "mimetype": "text/x-python", "name": "python", "nbconvert_exporter": "python", "pygments_lexer": "ipython3", "version": "3.11.4"}}, "nbformat": 4, "nbformat_minor": 5}