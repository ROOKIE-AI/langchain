{"cells": [{"cell_type": "raw", "id": "018f3868-e60d-4db6-a1c6-c6633c66b1f4", "metadata": {}, "source": ["---\n", "keywords: [LCEL, fallbacks]\n", "---"]}, {"cell_type": "markdown", "id": "19c9cbd6", "metadata": {}, "source": ["# 如何为可运行对象添加回退机制", "\n", "在使用语言模型时，您可能会经常遇到底层API的问题，无论是速率限制还是停机。因此，当您将LLM应用程序投入生产时，防范这些问题变得越来越重要。这就是我们引入回退概念的原因。", "\n", "**后备方案**（fallback）是指在紧急情况下可以采用的替代计划。", "\n", "关键的是，回退机制不仅可以应用在LLM层面，还能在整个可运行层面实施。这一点非常重要，因为不同模型通常需要不同的提示词。因此，当调用OpenAI失败时，你并不想直接将相同的提示词发送给Anthropic——你可能需要使用不同的提示模板，并发送一个调整后的版本。"]}, {"cell_type": "markdown", "id": "a6bb9ba9", "metadata": {}, "source": ["## 大语言模型API错误的回退方案", "\n", "这可能是后备方案最常见的用例之一。向大型语言模型(LLM)API发出的请求可能因多种原因失败——API可能宕机、可能触及速率限制，或是其他各种问题。因此，使用后备方案有助于防范此类情况。", "\n", "重要提示：默认情况下，许多LLM封装器会捕获错误并自动重试。在使用后备方案时，您很可能需要关闭这些功能。否则，第一个封装器会持续重试而不会报错。"]}, {"cell_type": "code", "execution_count": null, "id": "3a449a2e", "metadata": {}, "outputs": [], "source": ["%pip install --upgrade --quiet  langchain langchain-openai"]}, {"cell_type": "code", "execution_count": 1, "id": "d3e893bf", "metadata": {}, "outputs": [], "source": ["from langchain_anthropic import ChatAnthropic\n", "from langchain_openai import ChatOpenAI"]}, {"cell_type": "markdown", "id": "4847c82d", "metadata": {}, "source": ["首先，模拟一下如果我们遇到OpenAI的RateLimitError会发生什么"]}, {"cell_type": "code", "execution_count": 2, "id": "dfdd8bf5", "metadata": {}, "outputs": [], "source": ["from unittest.mock import patch\n", "\n", "import httpx\n", "from openai import RateLimitError\n", "\n", "request = httpx.Request(\"GET\", \"/\")\n", "response = httpx.Response(200, request=request)\n", "error = RateLimitError(\"rate limit\", response=response, body=\"\")"]}, {"cell_type": "code", "execution_count": 3, "id": "e6fdffc1", "metadata": {}, "outputs": [], "source": ["# Note that we set max_retries = 0 to avoid retrying on RateLimits, etc\n", "openai_llm = ChatOpenAI(model=\"gpt-4o-mini\", max_retries=0)\n", "anthropic_llm = ChatAnthropic(model=\"claude-3-haiku-20240307\")\n", "llm = openai_llm.with_fallbacks([anthropic_llm])"]}, {"cell_type": "code", "execution_count": 4, "id": "584461ab", "metadata": {}, "outputs": [{"name": "stdout", "output_type": "stream", "text": ["Hit error\n"]}], "source": ["# Let's use just the OpenAI LLm first, to show that we run into an error\n", "with patch(\"openai.resources.chat.completions.Completions.create\", side_effect=error):\n", "    try:\n", "        print(openai_llm.invoke(\"Why did the chicken cross the road?\"))\n", "    except RateLimitError:\n", "        print(\"Hit error\")"]}, {"cell_type": "code", "execution_count": 28, "id": "4fc1e673", "metadata": {}, "outputs": [{"name": "stdout", "output_type": "stream", "text": ["content=' I don\\'t actually know why the chicken crossed the road, but here are some possible humorous answers:\\n\\n- To get to the other side!\\n\\n- It was too chicken to just stand there. \\n\\n- It wanted a change of scenery.\\n\\n- It wanted to show the possum it could be done.\\n\\n- It was on its way to a poultry farmers\\' convention.\\n\\nThe joke plays on the double meaning of \"the other side\" - literally crossing the road to the other side, or the \"other side\" meaning the afterlife. So it\\'s an anti-joke, with a silly or unexpected pun as the answer.' additional_kwargs={} example=False\n"]}], "source": ["# Now let's try with fallbacks to Anthropic\n", "with patch(\"openai.resources.chat.completions.Completions.create\", side_effect=error):\n", "    try:\n", "        print(llm.invoke(\"Why did the chicken cross the road?\"))\n", "    except RateLimitError:\n", "        print(\"Hit error\")"]}, {"cell_type": "markdown", "id": "f00bea25", "metadata": {}, "source": ["我们可以像使用普通大语言模型一样使用我们的“带后备方案的大语言模型”。"]}, {"cell_type": "code", "execution_count": 29, "id": "4f8eaaa0", "metadata": {}, "outputs": [{"name": "stdout", "output_type": "stream", "text": ["content=\" I don't actually know why the kangaroo crossed the road, but I can take a guess! Here are some possible reasons:\\n\\n- To get to the other side (the classic joke answer!)\\n\\n- It was trying to find some food or water \\n\\n- It was trying to find a mate during mating season\\n\\n- It was fleeing from a predator or perceived threat\\n\\n- It was disoriented and crossed accidentally \\n\\n- It was following a herd of other kangaroos who were crossing\\n\\n- It wanted a change of scenery or environment \\n\\n- It was trying to reach a new habitat or territory\\n\\nThe real reason is unknown without more context, but hopefully one of those potential explanations does the joke justice! Let me know if you have any other animal jokes I can try to decipher.\" additional_kwargs={} example=False\n"]}], "source": ["from langchain_core.prompts import ChatPromptTemplate\n", "\n", "prompt = ChatPromptTemplate.from_messages(\n", "    [\n", "        (\n", "            \"system\",\n", "            \"You're a nice assistant who always includes a compliment in your response\",\n", "        ),\n", "        (\"human\", \"Why did the {animal} cross the road\"),\n", "    ]\n", ")\n", "chain = prompt | llm\n", "with patch(\"openai.resources.chat.completions.Completions.create\", side_effect=error):\n", "    try:\n", "        print(chain.invoke({\"animal\": \"kangaroo\"}))\n", "    except RateLimitError:\n", "        print(\"Hit error\")"]}, {"cell_type": "markdown", "id": "8d62241b", "metadata": {}, "source": ["## 序列的回退机制", "\n", "我们也可以为序列本身创建后备序列。这里我们使用两种不同的模型来实现这一点：ChatOpenAI 和普通的 OpenAI（后者不使用聊天模型）。由于 OpenAI 并非聊天模型，您可能需要使用不同的提示词。"]}, {"cell_type": "code", "execution_count": 30, "id": "6d0b8056", "metadata": {}, "outputs": [], "source": ["# First let's create a chain with a ChatModel\n", "# We add in a string output parser here so the outputs between the two are the same type\n", "from langchain_core.output_parsers import StrOutputParser\n", "\n", "chat_prompt = ChatPromptTemplate.from_messages(\n", "    [\n", "        (\n", "            \"system\",\n", "            \"You're a nice assistant who always includes a compliment in your response\",\n", "        ),\n", "        (\"human\", \"Why did the {animal} cross the road\"),\n", "    ]\n", ")\n", "# Here we're going to use a bad model name to easily create a chain that will error\n", "chat_model = ChatOpenAI(model=\"gpt-fake\")\n", "bad_chain = chat_prompt | chat_model | StrOutputParser()"]}, {"cell_type": "code", "execution_count": 31, "id": "8d1fc2a5", "metadata": {}, "outputs": [], "source": ["# Now lets create a chain with the normal OpenAI model\n", "from langchain_core.prompts import PromptTemplate\n", "from langchain_openai import OpenAI\n", "\n", "prompt_template = \"\"\"Instructions: You should always include a compliment in your response.\n", "\n", "Question: Why did the {animal} cross the road?\"\"\"\n", "prompt = PromptTemplate.from_template(prompt_template)\n", "llm = OpenAI()\n", "good_chain = prompt | llm"]}, {"cell_type": "code", "execution_count": 32, "id": "283bfa44", "metadata": {}, "outputs": [{"data": {"text/plain": ["'\\n\\nAnswer: The turtle crossed the road to get to the other side, and I have to say he had some impressive determination.'"]}, "execution_count": 32, "metadata": {}, "output_type": "execute_result"}], "source": ["# We can now create a final chain which combines the two\n", "chain = bad_chain.with_fallbacks([good_chain])\n", "chain.invoke({\"animal\": \"turtle\"})"]}, {"cell_type": "markdown", "id": "ec4685b4", "metadata": {}, "source": ["## 长输入回退处理", "\n", "大型语言模型（LLMs）的主要限制因素之一是其上下文窗口。通常情况下，你可以在将提示发送给LLM之前计算并跟踪其长度，但在难以/复杂的情况下，可以回退到具有更长上下文长度的模型。"]}, {"cell_type": "code", "execution_count": 34, "id": "564b84c9", "metadata": {}, "outputs": [], "source": ["short_llm = ChatOpenAI()\n", "long_llm = ChatOpenAI(model=\"gpt-3.5-turbo-16k\")\n", "llm = short_llm.with_fallbacks([long_llm])"]}, {"cell_type": "code", "execution_count": 38, "id": "5e27a775", "metadata": {}, "outputs": [], "source": ["inputs = \"What is the next number: \" + \", \".join([\"one\", \"two\"] * 3000)"]}, {"cell_type": "code", "execution_count": 40, "id": "0a502731", "metadata": {}, "outputs": [{"name": "stdout", "output_type": "stream", "text": ["This model's maximum context length is 4097 tokens. However, your messages resulted in 12012 tokens. Please reduce the length of the messages.\n"]}], "source": ["try:\n", "    print(short_llm.invoke(inputs))\n", "except Exception as e:\n", "    print(e)"]}, {"cell_type": "code", "execution_count": 41, "id": "d91ba5d7", "metadata": {}, "outputs": [{"name": "stdout", "output_type": "stream", "text": ["content='The next number in the sequence is two.' additional_kwargs={} example=False\n"]}], "source": ["try:\n", "    print(llm.invoke(inputs))\n", "except Exception as e:\n", "    print(e)"]}, {"cell_type": "markdown", "id": "2a6735df", "metadata": {}, "source": ["## 回退至更优模型", "\n", "我们经常要求模型以特定格式（如JSON）输出结果。像GPT-3.5这样的模型能够较好地完成这一任务，但有时也会遇到困难。这自然引出了备选方案——我们可以先尝试使用GPT-3.5（速度更快、成本更低），如果解析失败，再改用GPT-4。"]}, {"cell_type": "code", "execution_count": 42, "id": "867a3793", "metadata": {}, "outputs": [], "source": ["from langchain.output_parsers import DatetimeOutputParser"]}, {"cell_type": "code", "execution_count": 67, "id": "b8d9959d", "metadata": {}, "outputs": [], "source": ["prompt = ChatPromptTemplate.from_template(\n", "    \"what time was {event} (in %Y-%m-%dT%H:%M:%S.%fZ format - only return this value)\"\n", ")"]}, {"cell_type": "code", "execution_count": 75, "id": "98087a76", "metadata": {}, "outputs": [], "source": ["# In this case we are going to do the fallbacks on the LLM + output parser level\n", "# Because the error will get raised in the OutputParser\n", "openai_35 = ChatOpenAI() | DatetimeOutputParser()\n", "openai_4 = ChatOpenAI(model=\"gpt-4\") | DatetimeOutputParser()"]}, {"cell_type": "code", "execution_count": 77, "id": "17ec9e8f", "metadata": {}, "outputs": [], "source": ["only_35 = prompt | openai_35\n", "fallback_4 = prompt | openai_35.with_fallbacks([openai_4])"]}, {"cell_type": "code", "execution_count": 80, "id": "7e536f0b", "metadata": {}, "outputs": [{"name": "stdout", "output_type": "stream", "text": ["Error: Could not parse datetime string: The Super Bowl in 1994 took place on January 30th at 3:30 PM local time. Converting this to the specified format (%Y-%m-%dT%H:%M:%S.%fZ) results in: 1994-01-30T15:30:00.000Z\n"]}], "source": ["try:\n", "    print(only_35.invoke({\"event\": \"the superbowl in 1994\"}))\n", "except Exception as e:\n", "    print(f\"Error: {e}\")"]}, {"cell_type": "code", "execution_count": 81, "id": "01355c5e", "metadata": {}, "outputs": [{"name": "stdout", "output_type": "stream", "text": ["1994-01-30 15:30:00\n"]}], "source": ["try:\n", "    print(fallback_4.invoke({\"event\": \"the superbowl in 1994\"}))\n", "except Exception as e:\n", "    print(f\"Error: {e}\")"]}, {"cell_type": "code", "execution_count": null, "id": "c537f9d0", "metadata": {}, "outputs": [], "source": []}], "metadata": {"kernelspec": {"display_name": "Python 3 (ipykernel)", "language": "python", "name": "python3"}, "language_info": {"codemirror_mode": {"name": "ipython", "version": 3}, "file_extension": ".py", "mimetype": "text/x-python", "name": "python", "nbconvert_exporter": "python", "pygments_lexer": "ipython3", "version": "3.9.1"}}, "nbformat": 4, "nbformat_minor": 5}