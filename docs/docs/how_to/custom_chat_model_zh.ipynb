{"cells": [{"cell_type": "markdown", "id": "e3da9a3f-f583-4ba6-994e-0e8c1158f5eb", "metadata": {}, "source": ["# 如何创建自定义聊天模型类", "\n", ":::info 前提条件", "\n", "本指南假设您熟悉以下概念：", "- [聊天模型](/docs/concepts/chat_models)", "\n", ":::", "\n", "在本指南中，我们将学习如何使用 LangChain 抽象层创建自定义[聊天模型](/docs/concepts/chat_models/)。", "\n", "将您的LLM封装为标准[`BaseChatModel`](https://python.langchain.com/api_reference/core/language_models/langchain_core.language_models.chat_models.BaseChatModel.html)接口后，您只需极少的代码修改即可在现有LangChain程序中使用您的LLM！", "\n", "作为额外福利，您的LLM将自动成为LangChain的[可运行组件](/docs/concepts/runnables/)，并立即获得多项优化支持（例如通过线程池实现的批处理）、异步功能、`astream_events` API等。", "\n", "## 输入与输出", "\n", "首先，我们需要讨论**[消息](/docs/concepts/messages/)**，这是聊天模型的输入和输出。", "\n", "### 消息", "\n", "聊天模型接收消息作为输入，并返回消息作为输出。", "\n", "LangChain 内置了几种[消息类型](/docs/concepts/messages)：", "\n", "| 消息类型          | 描述                                                                                     |", "|-----------------------|-------------------------------------------------------------------------------------------------|", "| `SystemMessage`       | 用于引导AI行为，通常作为一系列输入消息中的第一条传递。   |", "| `HumanMessage`        | 表示与聊天模型交互的人发送的消息。                             |", "| `AIMessage`           | 表示来自聊天模型的消息。这可以是文本，也可以是调用工具的请求。|", "| `FunctionMessage` / `ToolMessage` | 用于将工具调用结果传递回模型的消息。               |", "| `AIMessageChunk` / `HumanMessageChunk` / ... | 各类消息的分块变体。 |", "\n", "\n", ":::note", "`ToolMessage` 和 `FunctionMessage` 严格遵循 OpenAI 的 `function` 与 `tool` 角色规范。", "\n", "这是一个快速发展的领域，随着更多模型加入函数调用功能，预计该架构将不断扩展完善。", "好的,我会按照要求将英文翻译成中文,并保持markdown格式一致。以下是一个示例翻译:\n\n# 欢迎使用翻译助手\n\n这是一个标准的markdown格式文档示例。\n\n## 主要功能\n\n1. 提供高质量的翻译服务\n2. 保持原始文档格式\n3. 支持多种语言互译\n\n### 使用说明\n\n- 输入要翻译的文本\n- 指定目标语言\n- 获取翻译结果\n\n> 注意:翻译质量取决于原文的清晰度和复杂度\n\n[点击这里](#)了解更多信息\n\n**重要提示**:请确保输入文本的准确性\n\n*斜体表示强调内容*\n\n表格示例:\n\n| 项目 | 描述 |\n|------|------|\n| 速度 | 快速响应 |\n| 质量 | 专业级翻译 |\n| 支持 | 多语言 |\n\n代码块示例:\n```\nprint(\"Hello World\")\n```"]}, {"cell_type": "code", "execution_count": 4, "id": "c5046e6a-8b09-4a99-b6e6-7a605aac5738", "metadata": {"tags": []}, "outputs": [], "source": ["from langchain_core.messages import (\n", "    AIMessage,\n", "    BaseMessage,\n", "    FunctionMessage,\n", "    HumanMessage,\n", "    SystemMessage,\n", "    ToolMessage,\n", ")"]}, {"cell_type": "markdown", "id": "53033447-8260-4f53-bd6f-b2f744e04e75", "metadata": {}, "source": ["### 流式变体", "\n", "所有聊天消息都有一个流式变体，其名称中包含 `Chunk`。"]}, {"cell_type": "code", "execution_count": 2, "id": "d4656e9d-bfa1-4703-8f79-762fe6421294", "metadata": {"tags": []}, "outputs": [], "source": ["from langchain_core.messages import (\n", "    AIMessageChunk,\n", "    FunctionMessageChunk,\n", "    HumanMessageChunk,\n", "    SystemMessageChunk,\n", "    ToolMessageChunk,\n", ")"]}, {"cell_type": "markdown", "id": "81ebf3f4-c760-4898-b921-fdb469453d4a", "metadata": {}, "source": ["这些数据块用于从聊天模型流式输出时使用，它们都定义了一个可叠加的属性！"]}, {"cell_type": "code", "execution_count": 3, "id": "9c15c299-6f8a-49cf-a072-09924fd44396", "metadata": {"tags": []}, "outputs": [{"data": {"text/plain": ["AIMessageChunk(content='Hello World!')"]}, "execution_count": 3, "metadata": {}, "output_type": "execute_result"}], "source": ["AIMessageChunk(content=\"Hello\") + AIMessageChunk(content=\" World!\")"]}, {"cell_type": "markdown", "id": "bbfebea1", "metadata": {}, "source": ["## 基础聊天模型", "\n", "让我们实现一个聊天模型，它会回显提示中最后一条消息的前`n`个字符！", "\n", "为此，我们将继承 `BaseChatModel` 并需要实现以下内容：", "\n", "| 方法/属性                        | 描述                                                             | 必填/选填          |", "|------------------------------------|-------------------------------------------------------------------|--------------------|", "| `_generate`                        | 用于根据提示生成聊天结果                       | 必需           |", "| `_llm_type` (属性)             | 用于唯一标识模型的类型。用于日志记录。| 必填           |", "| `_identifying_params` (属性)   | 用于追踪目的，表示模型的参数化配置。            | 可选           |", "| `_stream`                          | 用于实现流式传输。                                       | 可选           |", "| `_agenerate`                       | 用于实现原生异步方法。                           | 可选           |", "| `_astream`                         | 用于实现 `_stream` 的异步版本。                      | 可选           |", "\n", "\n", ":::提示", "`_astream` 的实现使用 `run_in_executor` 在单独的线程中启动同步的 `_stream`（如果 `_stream` 已实现），否则会回退使用 `_agenerate`。", "\n", "如果你想复用 `_stream` 的实现，可以使用这个技巧。但如果你能实现原生异步代码，那会是更好的解决方案，因为原生异步代码运行时开销更小。", ":::"]}, {"cell_type": "markdown", "id": "8e7047bd-c235-46f6-85e1-d6d7e0868eb1", "metadata": {}, "source": ["### 实施"]}, {"cell_type": "code", "execution_count": null, "id": "25ba32e5-5a6d-49f4-bb68-911827b84d61", "metadata": {"tags": []}, "outputs": [], "source": ["from typing import Any, Dict, Iterator, List, Optional\n", "\n", "from langchain_core.callbacks import (\n", "    CallbackManagerForLLMRun,\n", ")\n", "from langchain_core.language_models import BaseChatModel\n", "from langchain_core.messages import (\n", "    AIMessage,\n", "    AIMessageChunk,\n", "    BaseMessage,\n", ")\n", "from langchain_core.messages.ai import UsageMetadata\n", "from langchain_core.outputs import ChatGeneration, ChatGenerationChunk, ChatResult\n", "from pydantic import Field\n", "\n", "\n", "class ChatParrotLink(BaseChatModel):\n", "    \"\"\"A custom chat model that echoes the first `parrot_buffer_length` characters\n", "    of the input.\n", "\n", "    When contributing an implementation to LangChain, carefully document\n", "    the model including the initialization parameters, include\n", "    an example of how to initialize the model and include any relevant\n", "    links to the underlying models documentation or API.\n", "\n", "    Example:\n", "\n", "        .. code-block:: python\n", "\n", "            model = ChatParrotLink(parrot_buffer_length=2, model=\"bird-brain-001\")\n", "            result = model.invoke([HumanMessage(content=\"hello\")])\n", "            result = model.batch([[HumanMessage(content=\"hello\")],\n", "                                 [HumanMessage(content=\"world\")]])\n", "    \"\"\"\n", "\n", "    model_name: str = Field(alias=\"model\")\n", "    \"\"\"The name of the model\"\"\"\n", "    parrot_buffer_length: int\n", "    \"\"\"The number of characters from the last message of the prompt to be echoed.\"\"\"\n", "    temperature: Optional[float] = None\n", "    max_tokens: Optional[int] = None\n", "    timeout: Optional[int] = None\n", "    stop: Optional[List[str]] = None\n", "    max_retries: int = 2\n", "\n", "    def _generate(\n", "        self,\n", "        messages: List[BaseMessage],\n", "        stop: Optional[List[str]] = None,\n", "        run_manager: Optional[CallbackManagerForLLMRun] = None,\n", "        **kwargs: Any,\n", "    ) -> ChatResult:\n", "        \"\"\"Override the _generate method to implement the chat model logic.\n", "\n", "        This can be a call to an API, a call to a local model, or any other\n", "        implementation that generates a response to the input prompt.\n", "\n", "        Args:\n", "            messages: the prompt composed of a list of messages.\n", "            stop: a list of strings on which the model should stop generating.\n", "                  If generation stops due to a stop token, the stop token itself\n", "                  SHOULD BE INCLUDED as part of the output. This is not enforced\n", "                  across models right now, but it's a good practice to follow since\n", "                  it makes it much easier to parse the output of the model\n", "                  downstream and understand why generation stopped.\n", "            run_manager: A run manager with callbacks for the LLM.\n", "        \"\"\"\n", "        # Replace this with actual logic to generate a response from a list\n", "        # of messages.\n", "        last_message = messages[-1]\n", "        tokens = last_message.content[: self.parrot_buffer_length]\n", "        ct_input_tokens = sum(len(message.content) for message in messages)\n", "        ct_output_tokens = len(tokens)\n", "        message = AIMessage(\n", "            content=tokens,\n", "            additional_kwargs={},  # Used to add additional payload to the message\n", "            response_metadata={  # Use for response metadata\n", "                \"time_in_seconds\": 3,\n", "                \"model_name\": self.model_name,\n", "            },\n", "            usage_metadata={\n", "                \"input_tokens\": ct_input_tokens,\n", "                \"output_tokens\": ct_output_tokens,\n", "                \"total_tokens\": ct_input_tokens + ct_output_tokens,\n", "            },\n", "        )\n", "        ##\n", "\n", "        generation = ChatGeneration(message=message)\n", "        return ChatResult(generations=[generation])\n", "\n", "    def _stream(\n", "        self,\n", "        messages: List[BaseMessage],\n", "        stop: Optional[List[str]] = None,\n", "        run_manager: Optional[CallbackManagerForLLMRun] = None,\n", "        **kwargs: Any,\n", "    ) -> Iterator[ChatGenerationChunk]:\n", "        \"\"\"Stream the output of the model.\n", "\n", "        This method should be implemented if the model can generate output\n", "        in a streaming fashion. If the model does not support streaming,\n", "        do not implement it. In that case streaming requests will be automatically\n", "        handled by the _generate method.\n", "\n", "        Args:\n", "            messages: the prompt composed of a list of messages.\n", "            stop: a list of strings on which the model should stop generating.\n", "                  If generation stops due to a stop token, the stop token itself\n", "                  SHOULD BE INCLUDED as part of the output. This is not enforced\n", "                  across models right now, but it's a good practice to follow since\n", "                  it makes it much easier to parse the output of the model\n", "                  downstream and understand why generation stopped.\n", "            run_manager: A run manager with callbacks for the LLM.\n", "        \"\"\"\n", "        last_message = messages[-1]\n", "        tokens = str(last_message.content[: self.parrot_buffer_length])\n", "        ct_input_tokens = sum(len(message.content) for message in messages)\n", "\n", "        for token in tokens:\n", "            usage_metadata = UsageMetadata(\n", "                {\n", "                    \"input_tokens\": ct_input_tokens,\n", "                    \"output_tokens\": 1,\n", "                    \"total_tokens\": ct_input_tokens + 1,\n", "                }\n", "            )\n", "            ct_input_tokens = 0\n", "            chunk = ChatGenerationChunk(\n", "                message=AIMessageChunk(content=token, usage_metadata=usage_metadata)\n", "            )\n", "\n", "            if run_manager:\n", "                # This is optional in newer versions of LangChain\n", "                # The on_llm_new_token will be called automatically\n", "                run_manager.on_llm_new_token(token, chunk=chunk)\n", "\n", "            yield chunk\n", "\n", "        # Let's add some other information (e.g., response metadata)\n", "        chunk = ChatGenerationChunk(\n", "            message=AIMessageChunk(\n", "                content=\"\",\n", "                response_metadata={\"time_in_sec\": 3, \"model_name\": self.model_name},\n", "            )\n", "        )\n", "        if run_manager:\n", "            # This is optional in newer versions of LangChain\n", "            # The on_llm_new_token will be called automatically\n", "            run_manager.on_llm_new_token(token, chunk=chunk)\n", "        yield chunk\n", "\n", "    @property\n", "    def _llm_type(self) -> str:\n", "        \"\"\"Get the type of language model used by this chat model.\"\"\"\n", "        return \"echoing-chat-model-advanced\"\n", "\n", "    @property\n", "    def _identifying_params(self) -> Dict[str, Any]:\n", "        \"\"\"Return a dictionary of identifying parameters.\n", "\n", "        This information is used by the LangChain callback system, which\n", "        is used for tracing purposes make it possible to monitor LLMs.\n", "        \"\"\"\n", "        return {\n", "            # The model name allows users to specify custom token counting\n", "            # rules in LLM monitoring applications (e.g., in LangSmith users\n", "            # can provide per token pricing for their model and monitor\n", "            # costs for the given LLM.)\n", "            \"model_name\": self.model_name,\n", "        }"]}, {"cell_type": "markdown", "id": "1e9af284-f2d3-44e2-ac6a-09b73d89ada3", "metadata": {}, "source": ["### 让我们来测试一下", "\n", "聊天模型将实现LangChain的标准`Runnable`接口，该接口受到许多LangChain抽象功能的支持！"]}, {"cell_type": "code", "execution_count": 5, "id": "27689f30-dcd2-466b-ba9d-f60b7d434110", "metadata": {"tags": []}, "outputs": [{"data": {"text/plain": ["AIMessage(content='Meo', additional_kwargs={}, response_metadata={'time_in_seconds': 3}, id='run-cf11aeb6-8ab6-43d7-8c68-c1ef89b6d78e-0', usage_metadata={'input_tokens': 26, 'output_tokens': 3, 'total_tokens': 29})"]}, "execution_count": 5, "metadata": {}, "output_type": "execute_result"}], "source": ["model = ChatParrotLink(parrot_buffer_length=3, model=\"my_custom_model\")\n", "\n", "model.invoke(\n", "    [\n", "        HumanMessage(content=\"hello!\"),\n", "        AIMessage(content=\"Hi there human!\"),\n", "        HumanMessage(content=\"Meow!\"),\n", "    ]\n", ")"]}, {"cell_type": "code", "execution_count": 6, "id": "406436df-31bf-466b-9c3d-39db9d6b6407", "metadata": {"tags": []}, "outputs": [{"data": {"text/plain": ["AIMessage(content='hel', additional_kwargs={}, response_metadata={'time_in_seconds': 3}, id='run-618e5ed4-d611-4083-8cf1-c270726be8d9-0', usage_metadata={'input_tokens': 5, 'output_tokens': 3, 'total_tokens': 8})"]}, "execution_count": 6, "metadata": {}, "output_type": "execute_result"}], "source": ["model.invoke(\"hello\")"]}, {"cell_type": "code", "execution_count": 7, "id": "a72ffa46-6004-41ef-bbe4-56fa17a029e2", "metadata": {"tags": []}, "outputs": [{"data": {"text/plain": ["[AIMessage(content='hel', additional_kwargs={}, response_metadata={'time_in_seconds': 3}, id='run-eea4ed7d-d750-48dc-90c0-7acca1ff388f-0', usage_metadata={'input_tokens': 5, 'output_tokens': 3, 'total_tokens': 8}),\n", " AIMessage(content='goo', additional_kwargs={}, response_metadata={'time_in_seconds': 3}, id='run-07cfc5c1-3c62-485f-b1e0-3d46e1547287-0', usage_metadata={'input_tokens': 7, 'output_tokens': 3, 'total_tokens': 10})]"]}, "execution_count": 7, "metadata": {}, "output_type": "execute_result"}], "source": ["model.batch([\"hello\", \"goodbye\"])"]}, {"cell_type": "code", "execution_count": 8, "id": "3633be2c-2ea0-42f9-a72f-3b5240690b55", "metadata": {"tags": []}, "outputs": [{"name": "stdout", "output_type": "stream", "text": ["c|a|t||"]}], "source": ["for chunk in model.stream(\"cat\"):\n", "    print(chunk.content, end=\"|\")"]}, {"cell_type": "markdown", "id": "3f8a7c42-aec4-4116-adf3-93133d409827", "metadata": {}, "source": ["请查看模型中 `_astream` 的实现！如果未实现该方法，则不会有输出流式传输。"]}, {"cell_type": "code", "execution_count": 9, "id": "b7d73995-eeab-48c6-a7d8-32c98ba29fc2", "metadata": {"tags": []}, "outputs": [{"name": "stdout", "output_type": "stream", "text": ["c|a|t||"]}], "source": ["async for chunk in model.astream(\"cat\"):\n", "    print(chunk.content, end=\"|\")"]}, {"cell_type": "markdown", "id": "f80dc55b-d159-4527-9191-407a7c6d6042", "metadata": {}, "source": ["让我们尝试使用 astream 事件 API，这也有助于双重检查所有回调是否都已实现！"]}, {"cell_type": "code", "execution_count": 10, "id": "17840eba-8ff4-4e73-8e4f-85f16eb1c9d0", "metadata": {"tags": []}, "outputs": [{"name": "stdout", "output_type": "stream", "text": ["{'event': 'on_chat_model_start', 'run_id': '3f0b5501-5c78-45b3-92fc-8322a6a5024a', 'name': 'ChatParrotLink', 'tags': [], 'metadata': {}, 'data': {'input': 'cat'}, 'parent_ids': []}\n", "{'event': 'on_chat_model_stream', 'run_id': '3f0b5501-5c78-45b3-92fc-8322a6a5024a', 'tags': [], 'metadata': {}, 'name': 'ChatParrotLink', 'data': {'chunk': AIMessageChunk(content='c', additional_kwargs={}, response_metadata={}, id='run-3f0b5501-5c78-45b3-92fc-8322a6a5024a', usage_metadata={'input_tokens': 3, 'output_tokens': 1, 'total_tokens': 4})}, 'parent_ids': []}\n", "{'event': 'on_chat_model_stream', 'run_id': '3f0b5501-5c78-45b3-92fc-8322a6a5024a', 'tags': [], 'metadata': {}, 'name': 'ChatParrotLink', 'data': {'chunk': AIMessageChunk(content='a', additional_kwargs={}, response_metadata={}, id='run-3f0b5501-5c78-45b3-92fc-8322a6a5024a', usage_metadata={'input_tokens': 0, 'output_tokens': 1, 'total_tokens': 1})}, 'parent_ids': []}\n", "{'event': 'on_chat_model_stream', 'run_id': '3f0b5501-5c78-45b3-92fc-8322a6a5024a', 'tags': [], 'metadata': {}, 'name': 'ChatParrotLink', 'data': {'chunk': AIMessageChunk(content='t', additional_kwargs={}, response_metadata={}, id='run-3f0b5501-5c78-45b3-92fc-8322a6a5024a', usage_metadata={'input_tokens': 0, 'output_tokens': 1, 'total_tokens': 1})}, 'parent_ids': []}\n", "{'event': 'on_chat_model_stream', 'run_id': '3f0b5501-5c78-45b3-92fc-8322a6a5024a', 'tags': [], 'metadata': {}, 'name': 'ChatParrotLink', 'data': {'chunk': AIMessageChunk(content='', additional_kwargs={}, response_metadata={'time_in_sec': 3}, id='run-3f0b5501-5c78-45b3-92fc-8322a6a5024a')}, 'parent_ids': []}\n", "{'event': 'on_chat_model_end', 'name': 'ChatParrotLink', 'run_id': '3f0b5501-5c78-45b3-92fc-8322a6a5024a', 'tags': [], 'metadata': {}, 'data': {'output': AIMessageChunk(content='cat', additional_kwargs={}, response_metadata={'time_in_sec': 3}, id='run-3f0b5501-5c78-45b3-92fc-8322a6a5024a', usage_metadata={'input_tokens': 3, 'output_tokens': 3, 'total_tokens': 6})}, 'parent_ids': []}\n"]}], "source": ["async for event in model.astream_events(\"cat\", version=\"v1\"):\n", "    print(event)"]}, {"cell_type": "markdown", "id": "44ee559b-b1da-4851-8c97-420ab394aff9", "metadata": {}, "source": ["## 贡献指南", "\n", "我们感谢所有聊天模型集成方面的贡献。", "\n", "以下是一份清单，用于确保您的贡献能够被纳入LangChain：", "\n", "文档：", "\n", "* 该模型为所有初始化参数提供了文档字符串，这些内容将展示在[API参考文档](https://python.langchain.com/api_reference/langchain/index.html)中。", "* 如果模型由某项服务提供支持，则该类的文档字符串（doc-string）会包含指向模型API的链接。", "\n", "测试：", "\n", "* [ ] 为被重写的方法添加单元测试或集成测试。若已重写对应代码，请验证`invoke`、`ainvoke`、`batch`、`stream`等功能是否正常运作。", "\n", "\n", "流式传输（如果您正在实现它）：", "\n", "* [ ] 实现 _stream 方法以使流式传输正常工作", "\n", "停止标记行为：", "\n", "* [ ] 应遵守停止标记", "* [ ] 停止标记应作为响应的一部分被包含", "\n", "机密API密钥：", "\n", "* [ ] 如果你的模型需要连接API，它很可能会在初始化时接收API密钥作为参数。对于这类敏感信息，请使用Pydantic的`SecretStr`类型来存储，这样当用户打印模型时就不会意外泄露密钥内容。", "\n", "\n", "识别参数：", "\n", "* [ ] 在识别参数中包含 `model_name`", "\n", "\n", "优化措施：", "\n", "考虑提供原生异步支持以减少模型的开销！", " \n", "* [ ] 提供了`_agenerate`的原生异步实现（供`ainvoke`使用）", "* [ ] 提供了`_astream`的原生异步实现（被`astream`调用）", "\n", "## 后续步骤", "\n", "你现在已经学会了如何创建自己的自定义聊天模型。", "\n", "接下来，请查阅本节中的其他聊天模型操作指南，例如[如何让模型返回结构化输出](/docs/how_to/structured_output)或[如何追踪聊天模型的令牌使用情况](/docs/how_to/chat_token_usage_tracking)。"]}], "metadata": {"kernelspec": {"display_name": ".venv", "language": "python", "name": "python3"}, "language_info": {"codemirror_mode": {"name": "ipython", "version": 3}, "file_extension": ".py", "mimetype": "text/x-python", "name": "python", "nbconvert_exporter": "python", "pygments_lexer": "ipython3", "version": "3.11.4"}}, "nbformat": 4, "nbformat_minor": 5}