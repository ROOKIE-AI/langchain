{"cells": [{"cell_type": "raw", "metadata": {}, "source": ["---\n", "sidebar_position: 1\n", "---"]}, {"cell_type": "markdown", "metadata": {}, "source": ["# 如何为聊天机器人添加记忆功能", "\n", "聊天机器人的一个关键特性是它们能够利用先前对话轮次的内容作为上下文。这种状态管理可以采取多种形式，包括：", "\n", "- 简单地将之前的消息塞入聊天模型提示中。", "- 同上，但会修剪旧消息以减少模型需要处理的干扰信息量。", "- 更复杂的修改，例如为长时间运行的对话合成摘要。", "\n", "我们将在下文中详细介绍几种技术！", "\n", ":::note", "\n", "本操作指南先前使用 [RunnableWithMessageHistory](https://python.langchain.com/api_reference/core/runnables/langchain_core.runnables.history.RunnableWithMessageHistory.html) 构建了一个聊天机器人。您可以在 [v0.2 版本文档](https://python.langchain.com/v0.2/docs/how_to/chatbots_memory/) 中查看该版本的指南。", "\n", "截至 LangChain v0.3 版本发布，我们建议 LangChain 用户利用 [LangGraph 持久化功能](https://langchain-ai.github.io/langgraph/concepts/persistence/) 将 `memory` 集成到新的 LangChain 应用程序中。", "\n", "如果你的代码已经在使用 `RunnableWithMessageHistory` 或 `BaseChatMessageHistory`，那么你**无需**做任何更改。我们近期没有计划弃用此功能，因为它适用于简单的聊天应用，并且任何使用 `RunnableWithMessageHistory` 的代码将继续按预期工作。", "\n", "请参阅[如何迁移至 LangGraph Memory](/docs/versions/migrating_memory/)获取更多详情。", ":::", "\n", "## 安装设置", "\n", "你需要安装一些软件包，并将你的OpenAI API密钥设置为名为`OPENAI_API_KEY`的环境变量："]}, {"cell_type": "code", "execution_count": 1, "metadata": {}, "outputs": [{"name": "stdin", "output_type": "stream", "text": ["OpenAI API Key: ········\n"]}], "source": ["%pip install --upgrade --quiet langchain langchain-openai langgraph\n", "\n", "import getpass\n", "import os\n", "\n", "if not os.environ.get(\"OPENAI_API_KEY\"):\n", "    os.environ[\"OPENAI_API_KEY\"] = getpass.getpass(\"OpenAI API Key:\")"]}, {"cell_type": "markdown", "metadata": {}, "source": ["我们还需要设置一个聊天模型，用于以下示例。"]}, {"cell_type": "code", "execution_count": 2, "metadata": {}, "outputs": [], "source": ["from langchain_openai import ChatOpenAI\n", "\n", "model = ChatOpenAI(model=\"gpt-4o-mini\")"]}, {"cell_type": "markdown", "metadata": {}, "source": ["## 消息传递", "\n", "最简单的记忆形式就是直接将聊天历史消息传递到链中。以下是一个示例："]}, {"cell_type": "code", "execution_count": 3, "metadata": {}, "outputs": [{"name": "stdout", "output_type": "stream", "text": ["I said, \"I love programming\" in French: \"J'adore la programmation.\"\n"]}], "source": ["from langchain_core.messages import AIMessage, HumanMessage, SystemMessage\n", "from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\n", "\n", "prompt = ChatPromptTemplate.from_messages(\n", "    [\n", "        SystemMessage(\n", "            content=\"You are a helpful assistant. Answer all questions to the best of your ability.\"\n", "        ),\n", "        MessagesPlaceholder(variable_name=\"messages\"),\n", "    ]\n", ")\n", "\n", "chain = prompt | model\n", "\n", "ai_msg = chain.invoke(\n", "    {\n", "        \"messages\": [\n", "            HumanMessage(\n", "                content=\"Translate from English to French: I love programming.\"\n", "            ),\n", "            AIMessage(content=\"J'adore la programmation.\"),\n", "            HumanMessage(content=\"What did you just say?\"),\n", "        ],\n", "    }\n", ")\n", "print(ai_msg.content)"]}, {"cell_type": "markdown", "metadata": {}, "source": ["我们可以看到，通过将先前的对话内容传入链式结构，它能够将其作为上下文来回答问题。这正是聊天机器人记忆功能的核心原理——本指南后续部分将展示各种便捷的消息传递与格式化处理技巧。"]}, {"cell_type": "markdown", "metadata": {}, "source": ["## 自动历史记录管理", "\n", "之前的示例明确地将消息传递给链（和模型）。这是一种完全可以接受的方法，但它确实需要外部管理新消息。LangChain 还提供了一种利用 LangGraph 的[持久化](https://langchain-ai.github.io/langgraph/concepts/persistence/)功能来构建具有记忆的应用程序的方式。您可以通过在编译图时提供一个 `checkpointer` 来[启用持久化](https://langchain-ai.github.io/langgraph/how-tos/persistence/)功能。"]}, {"cell_type": "code", "execution_count": 4, "metadata": {}, "outputs": [], "source": ["from langgraph.checkpoint.memory import MemorySaver\n", "from langgraph.graph import START, MessagesState, StateGraph\n", "\n", "workflow = StateGraph(state_schema=MessagesState)\n", "\n", "\n", "# Define the function that calls the model\n", "def call_model(state: MessagesState):\n", "    system_prompt = (\n", "        \"You are a helpful assistant. \"\n", "        \"Answer all questions to the best of your ability.\"\n", "    )\n", "    messages = [SystemMessage(content=system_prompt)] + state[\"messages\"]\n", "    response = model.invoke(messages)\n", "    return {\"messages\": response}\n", "\n", "\n", "# Define the node and edge\n", "workflow.add_node(\"model\", call_model)\n", "workflow.add_edge(START, \"model\")\n", "\n", "# Add simple in-memory checkpointer\n", "# highlight-start\n", "memory = MemorySaver()\n", "app = workflow.compile(checkpointer=memory)\n", "# highlight-end"]}, {"cell_type": "markdown", "metadata": {}, "source": ["我们将在此处传递最新的输入到对话中，并让LangGraph通过检查点记录器来跟踪对话历史："]}, {"cell_type": "code", "execution_count": 5, "metadata": {}, "outputs": [{"data": {"text/plain": ["{'messages': [HumanMessage(content='Translate to French: I love programming.', additional_kwargs={}, response_metadata={}, id='be5e7099-3149-4293-af49-6b36c8ccd71b'),\n", "  AIMessage(content=\"J'aime programmer.\", additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 4, 'prompt_tokens': 35, 'total_tokens': 39, 'completion_tokens_details': {'reasoning_tokens': 0}}, 'model_name': 'gpt-4o-mini-2024-07-18', 'system_fingerprint': 'fp_e9627b5346', 'finish_reason': 'stop', 'logprobs': None}, id='run-8a753d7a-b97b-4d01-a661-626be6f41b38-0', usage_metadata={'input_tokens': 35, 'output_tokens': 4, 'total_tokens': 39})]}"]}, "execution_count": 5, "metadata": {}, "output_type": "execute_result"}], "source": ["app.invoke(\n", "    {\"messages\": [HumanMessage(content=\"Translate to French: I love programming.\")]},\n", "    config={\"configurable\": {\"thread_id\": \"1\"}},\n", ")"]}, {"cell_type": "code", "execution_count": 6, "metadata": {}, "outputs": [{"data": {"text/plain": ["{'messages': [HumanMessage(content='Translate to French: I love programming.', additional_kwargs={}, response_metadata={}, id='be5e7099-3149-4293-af49-6b36c8ccd71b'),\n", "  AIMessage(content=\"J'aime programmer.\", additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 4, 'prompt_tokens': 35, 'total_tokens': 39, 'completion_tokens_details': {'reasoning_tokens': 0}}, 'model_name': 'gpt-4o-mini-2024-07-18', 'system_fingerprint': 'fp_e9627b5346', 'finish_reason': 'stop', 'logprobs': None}, id='run-8a753d7a-b97b-4d01-a661-626be6f41b38-0', usage_metadata={'input_tokens': 35, 'output_tokens': 4, 'total_tokens': 39}),\n", "  HumanMessage(content='What did I just ask you?', additional_kwargs={}, response_metadata={}, id='c667529b-7c41-4cc0-9326-0af47328b816'),\n", "  AIMessage(content='You asked me to translate \"I love programming\" into French.', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 13, 'prompt_tokens': 54, 'total_tokens': 67, 'completion_tokens_details': {'reasoning_tokens': 0}}, 'model_name': 'gpt-4o-mini-2024-07-18', 'system_fingerprint': 'fp_1bb46167f9', 'finish_reason': 'stop', 'logprobs': None}, id='run-134a7ea0-d3a4-4923-bd58-25e5a43f6a1f-0', usage_metadata={'input_tokens': 54, 'output_tokens': 13, 'total_tokens': 67})]}"]}, "execution_count": 6, "metadata": {}, "output_type": "execute_result"}], "source": ["app.invoke(\n", "    {\"messages\": [HumanMessage(content=\"What did I just ask you?\")]},\n", "    config={\"configurable\": {\"thread_id\": \"1\"}},\n", ")"]}, {"cell_type": "markdown", "metadata": {}, "source": ["## 修改聊天记录", "\n", "修改存储的聊天记录可以帮助您的聊天机器人应对各种情况。以下是一些示例：", "\n", "### 消息裁剪", "\n", "大型语言模型（LLMs）和聊天模型存在有限的上下文窗口，即便未直接触及限制边界，也可能需要减少模型处理的干扰信息量。解决方案之一是在将历史消息传递给模型前进行精简。以下我们以先前声明的`app`为例，展示一段历史消息记录："]}, {"cell_type": "code", "execution_count": 7, "metadata": {}, "outputs": [{"data": {"text/plain": ["{'messages': [HumanMessage(content=\"Hey there! I'm Nemo.\", additional_kwargs={}, response_metadata={}, id='6b4cab70-ce18-49b0-bb06-267bde44e037'),\n", "  AIMessage(content='Hello!', additional_kwargs={}, response_metadata={}, id='ba3714f4-8876-440b-a651-efdcab2fcb4c'),\n", "  HumanMessage(content='How are you today?', additional_kwargs={}, response_metadata={}, id='08d032c0-1577-4862-a3f2-5c1b90687e21'),\n", "  AIMessage(content='Fine thanks!', additional_kwargs={}, response_metadata={}, id='21790e16-db05-4537-9a6b-ecad0fcec436'),\n", "  HumanMessage(content=\"What's my name?\", additional_kwargs={}, response_metadata={}, id='c933eca3-5fd8-4651-af16-20fe2d49c216'),\n", "  AIMessage(content='Your name is Nemo.', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 5, 'prompt_tokens': 63, 'total_tokens': 68, 'completion_tokens_details': {'reasoning_tokens': 0}}, 'model_name': 'gpt-4o-mini-2024-07-18', 'system_fingerprint': 'fp_1bb46167f9', 'finish_reason': 'stop', 'logprobs': None}, id='run-a0b21acc-9dbb-4fb6-a953-392020f37d88-0', usage_metadata={'input_tokens': 63, 'output_tokens': 5, 'total_tokens': 68})]}"]}, "execution_count": 7, "metadata": {}, "output_type": "execute_result"}], "source": ["demo_ephemeral_chat_history = [\n", "    HumanMessage(content=\"Hey there! I'm Nemo.\"),\n", "    AIMessage(content=\"Hello!\"),\n", "    HumanMessage(content=\"How are you today?\"),\n", "    AIMessage(content=\"Fine thanks!\"),\n", "]\n", "\n", "app.invoke(\n", "    {\n", "        \"messages\": demo_ephemeral_chat_history\n", "        + [HumanMessage(content=\"What's my name?\")]\n", "    },\n", "    config={\"configurable\": {\"thread_id\": \"2\"}},\n", ")"]}, {"cell_type": "markdown", "metadata": {}, "source": ["我们可以看到应用程序记住了预加载的名称。", "\n", "假设我们有一个非常小的上下文窗口，需要将传递给模型的消息数量精简至最近的2条。我们可以使用内置的[trim_messages](/docs/how_to/trim_messages/)工具，在消息到达提示词之前根据其令牌数量进行精简。本例中我们将每条消息计为1个\"令牌\"，仅保留最后两条消息："]}, {"cell_type": "code", "execution_count": 8, "metadata": {}, "outputs": [], "source": ["from langchain_core.messages import trim_messages\n", "from langgraph.checkpoint.memory import MemorySaver\n", "from langgraph.graph import START, MessagesState, StateGraph\n", "\n", "# Define trimmer\n", "# highlight-start\n", "# count each message as 1 \"token\" (token_counter=len) and keep only the last two messages\n", "trimmer = trim_messages(strategy=\"last\", max_tokens=2, token_counter=len)\n", "# highlight-end\n", "\n", "workflow = StateGraph(state_schema=MessagesState)\n", "\n", "\n", "# Define the function that calls the model\n", "def call_model(state: MessagesState):\n", "    # highlight-start\n", "    trimmed_messages = trimmer.invoke(state[\"messages\"])\n", "    system_prompt = (\n", "        \"You are a helpful assistant. \"\n", "        \"Answer all questions to the best of your ability.\"\n", "    )\n", "    messages = [SystemMessage(content=system_prompt)] + trimmed_messages\n", "    # highlight-end\n", "    response = model.invoke(messages)\n", "    return {\"messages\": response}\n", "\n", "\n", "# Define the node and edge\n", "workflow.add_node(\"model\", call_model)\n", "workflow.add_edge(START, \"model\")\n", "\n", "# Add simple in-memory checkpointer\n", "memory = MemorySaver()\n", "app = workflow.compile(checkpointer=memory)"]}, {"cell_type": "markdown", "metadata": {}, "source": ["让我们调用这个新应用并检查响应"]}, {"cell_type": "code", "execution_count": 9, "metadata": {}, "outputs": [{"data": {"text/plain": ["{'messages': [HumanMessage(content=\"Hey there! I'm Nemo.\", additional_kwargs={}, response_metadata={}, id='6b4cab70-ce18-49b0-bb06-267bde44e037'),\n", "  AIMessage(content='Hello!', additional_kwargs={}, response_metadata={}, id='ba3714f4-8876-440b-a651-efdcab2fcb4c'),\n", "  HumanMessage(content='How are you today?', additional_kwargs={}, response_metadata={}, id='08d032c0-1577-4862-a3f2-5c1b90687e21'),\n", "  AIMessage(content='Fine thanks!', additional_kwargs={}, response_metadata={}, id='21790e16-db05-4537-9a6b-ecad0fcec436'),\n", "  HumanMessage(content='What is my name?', additional_kwargs={}, response_metadata={}, id='a22ab7c5-8617-4821-b3e9-a9e7dca1ff78'),\n", "  AIMessage(content=\"I'm sorry, but I don't have access to personal information about you unless you share it with me. How can I assist you today?\", additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 27, 'prompt_tokens': 39, 'total_tokens': 66, 'completion_tokens_details': {'reasoning_tokens': 0}}, 'model_name': 'gpt-4o-mini-2024-07-18', 'system_fingerprint': 'fp_1bb46167f9', 'finish_reason': 'stop', 'logprobs': None}, id='run-f7b32d72-9f57-4705-be7e-43bf1c3d293b-0', usage_metadata={'input_tokens': 39, 'output_tokens': 27, 'total_tokens': 66})]}"]}, "execution_count": 9, "metadata": {}, "output_type": "execute_result"}], "source": ["app.invoke(\n", "    {\n", "        \"messages\": demo_ephemeral_chat_history\n", "        + [HumanMessage(content=\"What is my name?\")]\n", "    },\n", "    config={\"configurable\": {\"thread_id\": \"3\"}},\n", ")"]}, {"cell_type": "markdown", "metadata": {}, "source": ["我们可以看到 `trim_messages` 被调用，只有最近的两条消息会被传递给模型。在这种情况下，这意味着模型忘记了我们为它起的名字。"]}, {"cell_type": "markdown", "metadata": {}, "source": ["查看我们的[消息修剪操作指南](/docs/how_to/trim_messages/)了解更多。"]}, {"cell_type": "markdown", "metadata": {}, "source": ["### 摘要记忆", "\n", "我们同样可以以其他方式运用这一模式。例如，在调用应用程序之前，我们可以通过额外的LLM调用来生成对话摘要。让我们重新创建聊天记录："]}, {"cell_type": "code", "execution_count": 10, "metadata": {}, "outputs": [], "source": ["demo_ephemeral_chat_history = [\n", "    HumanMessage(content=\"Hey there! I'm Nemo.\"),\n", "    AIMessage(content=\"Hello!\"),\n", "    HumanMessage(content=\"How are you today?\"),\n", "    AIMessage(content=\"Fine thanks!\"),\n", "]"]}, {"cell_type": "markdown", "metadata": {}, "source": ["现在，让我们更新模型调用函数，将之前的交互提炼成摘要："]}, {"cell_type": "code", "execution_count": 11, "metadata": {}, "outputs": [], "source": ["from langchain_core.messages import HumanMessage, RemoveMessage\n", "from langgraph.checkpoint.memory import MemorySaver\n", "from langgraph.graph import START, MessagesState, StateGraph\n", "\n", "workflow = StateGraph(state_schema=MessagesState)\n", "\n", "\n", "# Define the function that calls the model\n", "def call_model(state: MessagesState):\n", "    system_prompt = (\n", "        \"You are a helpful assistant. \"\n", "        \"Answer all questions to the best of your ability. \"\n", "        \"The provided chat history includes a summary of the earlier conversation.\"\n", "    )\n", "    system_message = SystemMessage(content=system_prompt)\n", "    message_history = state[\"messages\"][:-1]  # exclude the most recent user input\n", "    # Summarize the messages if the chat history reaches a certain size\n", "    if len(message_history) >= 4:\n", "        last_human_message = state[\"messages\"][-1]\n", "        # Invoke the model to generate conversation summary\n", "        summary_prompt = (\n", "            \"Distill the above chat messages into a single summary message. \"\n", "            \"Include as many specific details as you can.\"\n", "        )\n", "        summary_message = model.invoke(\n", "            message_history + [HumanMessage(content=summary_prompt)]\n", "        )\n", "\n", "        # Delete messages that we no longer want to show up\n", "        delete_messages = [RemoveMessage(id=m.id) for m in state[\"messages\"]]\n", "        # Re-add user message\n", "        human_message = HumanMessage(content=last_human_message.content)\n", "        # Call the model with summary & response\n", "        response = model.invoke([system_message, summary_message, human_message])\n", "        message_updates = [summary_message, human_message, response] + delete_messages\n", "    else:\n", "        message_updates = model.invoke([system_message] + state[\"messages\"])\n", "\n", "    return {\"messages\": message_updates}\n", "\n", "\n", "# Define the node and edge\n", "workflow.add_node(\"model\", call_model)\n", "workflow.add_edge(START, \"model\")\n", "\n", "# Add simple in-memory checkpointer\n", "memory = MemorySaver()\n", "app = workflow.compile(checkpointer=memory)"]}, {"cell_type": "markdown", "metadata": {}, "source": ["让我们看看它是否还记得我们给它起的名字："]}, {"cell_type": "code", "execution_count": 12, "metadata": {}, "outputs": [{"data": {"text/plain": ["{'messages': [AIMessage(content=\"Nemo greeted me, and I responded positively, indicating that I'm doing well.\", additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 16, 'prompt_tokens': 60, 'total_tokens': 76, 'completion_tokens_details': {'reasoning_tokens': 0}}, 'model_name': 'gpt-4o-mini-2024-07-18', 'system_fingerprint': 'fp_1bb46167f9', 'finish_reason': 'stop', 'logprobs': None}, id='run-ee42f98d-907d-4bad-8f16-af2db789701d-0', usage_metadata={'input_tokens': 60, 'output_tokens': 16, 'total_tokens': 76}),\n", "  HumanMessage(content='What did I say my name was?', additional_kwargs={}, response_metadata={}, id='788555ea-5b1f-4c29-a2f2-a92f15d147be'),\n", "  AIMessage(content='You mentioned that your name is Nemo.', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 8, 'prompt_tokens': 67, 'total_tokens': 75, 'completion_tokens_details': {'reasoning_tokens': 0}}, 'model_name': 'gpt-4o-mini-2024-07-18', 'system_fingerprint': 'fp_1bb46167f9', 'finish_reason': 'stop', 'logprobs': None}, id='run-099a43bd-a284-4969-bb6f-0be486614cd8-0', usage_metadata={'input_tokens': 67, 'output_tokens': 8, 'total_tokens': 75})]}"]}, "execution_count": 12, "metadata": {}, "output_type": "execute_result"}], "source": ["app.invoke(\n", "    {\n", "        \"messages\": demo_ephemeral_chat_history\n", "        + [HumanMessage(\"What did I say my name was?\")]\n", "    },\n", "    config={\"configurable\": {\"thread_id\": \"4\"}},\n", ")"]}, {"cell_type": "markdown", "metadata": {}, "source": ["请注意，再次调用该应用会持续累积历史记录，直到达到指定的消息数量（在本例中为四条）。届时，我们将基于初始摘要及新增消息生成另一份摘要，并以此类推。"]}], "metadata": {"kernelspec": {"display_name": "Python 3 (ipykernel)", "language": "python", "name": "python3"}, "language_info": {"codemirror_mode": {"name": "ipython", "version": 3}, "file_extension": ".py", "mimetype": "text/x-python", "name": "python", "nbconvert_exporter": "python", "pygments_lexer": "ipython3", "version": "3.11.4"}}, "nbformat": 4, "nbformat_minor": 4}