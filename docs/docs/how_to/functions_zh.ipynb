{"cells": [{"cell_type": "raw", "id": "ce0e08fd", "metadata": {}, "source": ["---\n", "sidebar_position: 3\n", "keywords: [RunnableLambda, LCEL]\n", "---"]}, {"cell_type": "markdown", "id": "fbc4bf6e", "metadata": {}, "source": ["# 如何运行自定义函数", "\n", ":::info 前提条件", "\n", "本指南假定您已熟悉以下概念：", "- [LangChain 表达式语言 (LCEL)](/docs/concepts/lcel)", "- [链式运行](/docs/how_to/sequence/)", "\n", ":::", "\n", "你可以将任意函数用作 [Runnables](https://python.langchain.com/api_reference/core/runnables/langchain_core.runnables.base.Runnable.html#langchain_core.runnables.base.Runnable)。这在需要格式化或实现其他 LangChain 组件未提供的功能时非常有用，而作为 Runnables 使用的自定义函数被称为 [`RunnableLambdas`](https://python.langchain.com/api_reference/core/runnables/langchain_core.runnables.base.RunnableLambda.html)。", "\n", "请注意，这些函数的所有输入都必须是一个单一参数。如果你的函数接受多个参数，你应该编写一个包装器，该包装器接受一个字典输入并将其解包为多个参数。", "\n", "本指南将涵盖：", "\n", "- 如何通过`RunnableLambda`构造函数及便捷的`@chain`装饰器显式地从自定义函数创建可运行对象", "- 在链式调用中将自定义函数强制转换为可运行对象", "- 如何在自定义函数中接收和使用运行元数据", "- 如何通过让自定义函数返回生成器来实现流式传输", "\n", "## 使用构造函数", "\n", "下面，我们明确使用 `RunnableLambda` 构造函数来封装我们的自定义逻辑："]}, {"cell_type": "code", "execution_count": null, "id": "5c34d2af", "metadata": {}, "outputs": [], "source": ["%pip install -qU langchain langchain_openai\n", "\n", "import os\n", "from getpass import getpass\n", "\n", "if \"OPENAI_API_KEY\" not in os.environ:\n", "    os.environ[\"OPENAI_API_KEY\"] = getpass()"]}, {"cell_type": "code", "execution_count": 2, "id": "6bb221b3", "metadata": {}, "outputs": [{"data": {"text/plain": ["AIMessage(content='3 + 9 equals 12.', response_metadata={'token_usage': {'completion_tokens': 8, 'prompt_tokens': 14, 'total_tokens': 22}, 'model_name': 'gpt-3.5-turbo', 'system_fingerprint': 'fp_c2295e73ad', 'finish_reason': 'stop', 'logprobs': None}, id='run-73728de3-e483-49e3-ad54-51bd9570e71a-0')"]}, "execution_count": 2, "metadata": {}, "output_type": "execute_result"}], "source": ["from operator import itemgetter\n", "\n", "from langchain_core.prompts import ChatPromptTemplate\n", "from langchain_core.runnables import RunnableLambda\n", "from langchain_openai import ChatOpenAI\n", "\n", "\n", "def length_function(text):\n", "    return len(text)\n", "\n", "\n", "def _multiple_length_function(text1, text2):\n", "    return len(text1) * len(text2)\n", "\n", "\n", "def multiple_length_function(_dict):\n", "    return _multiple_length_function(_dict[\"text1\"], _dict[\"text2\"])\n", "\n", "\n", "model = ChatOpenAI()\n", "\n", "prompt = ChatPromptTemplate.from_template(\"what is {a} + {b}\")\n", "\n", "chain = (\n", "    {\n", "        \"a\": itemgetter(\"foo\") | RunnableLambda(length_function),\n", "        \"b\": {\"text1\": itemgetter(\"foo\"), \"text2\": itemgetter(\"bar\")}\n", "        | RunnableLambda(multiple_length_function),\n", "    }\n", "    | prompt\n", "    | model\n", ")\n", "\n", "chain.invoke({\"foo\": \"bar\", \"bar\": \"gah\"})"]}, {"cell_type": "markdown", "id": "b7926002", "metadata": {}, "source": ["## 便捷的 `@chain` 装饰器", "\n", "你也可以通过添加 `@chain` 装饰器将任意函数转换为链式结构。这在功能上等同于将函数包裹在如上所示的 `RunnableLambda` 构造函数中。示例如下："]}, {"cell_type": "code", "execution_count": 3, "id": "3142a516", "metadata": {}, "outputs": [{"data": {"text/plain": ["'The subject of the joke is the bear and his girlfriend.'"]}, "execution_count": 3, "metadata": {}, "output_type": "execute_result"}], "source": ["from langchain_core.output_parsers import StrOutputParser\n", "from langchain_core.runnables import chain\n", "\n", "prompt1 = ChatPromptTemplate.from_template(\"Tell me a joke about {topic}\")\n", "prompt2 = ChatPromptTemplate.from_template(\"What is the subject of this joke: {joke}\")\n", "\n", "\n", "@chain\n", "def custom_chain(text):\n", "    prompt_val1 = prompt1.invoke({\"topic\": text})\n", "    output1 = ChatOpenAI().invoke(prompt_val1)\n", "    parsed_output1 = StrOutputParser().invoke(output1)\n", "    chain2 = prompt2 | ChatOpenAI() | StrOutputParser()\n", "    return chain2.invoke({\"joke\": parsed_output1})\n", "\n", "\n", "custom_chain.invoke(\"bears\")"]}, {"cell_type": "markdown", "id": "4728ddd9-914d-42ce-ae9b-72c9ce8ec940", "metadata": {}, "source": ["在上面的代码中，`@chain`装饰器被用于将`custom_chain`转换为可运行对象，我们通过`.invoke()`方法来调用它。", "\n", "如果您正在使用 [LangSmith](https://docs.smith.langchain.com/) 的追踪功能，您应该能在其中看到一个名为 `custom_chain` 的追踪记录，其中嵌套了对 OpenAI 的调用。", "\n", "## 链式操作中的自动强制类型转换", "\n", "在使用管道操作符（`|`）的链式调用中自定义函数时，可以省略 `RunnableLambda` 或 `@chain` 构造器，直接依赖类型强制转换。以下是一个简单示例：该函数接收模型输出并返回其前五个字母。"]}, {"cell_type": "code", "execution_count": 4, "id": "5ab39a87", "metadata": {}, "outputs": [{"data": {"text/plain": ["'Once '"]}, "execution_count": 4, "metadata": {}, "output_type": "execute_result"}], "source": ["prompt = ChatPromptTemplate.from_template(\"tell me a story about {topic}\")\n", "\n", "model = ChatOpenAI()\n", "\n", "chain_with_coerced_function = prompt | model | (lambda x: x.content[:5])\n", "\n", "chain_with_coerced_function.invoke({\"topic\": \"bears\"})"]}, {"cell_type": "markdown", "id": "c9a481d1", "metadata": {}, "source": ["需要注意的是，我们无需将自定义函数 `(lambda x: x.content[:5])` 封装在 `RunnableLambda` 构造函数中，因为管道操作符左侧的 `model` 本身已是一个 Runnable 对象。该自定义函数会被**强制转换**为可运行对象。更多信息请参阅[此章节](/docs/how_to/sequence/#coercion)。", "\n", "## 传递运行元数据", "\n", "可运行的 lambda 函数可以选择性地接受一个 [RunnableConfig](https://python.langchain.com/api_reference/core/runnables/langchain_core.runnables.config.RunnableConfig.html#langchain_core.runnables.config.RunnableConfig) 参数，用于将回调函数、标签以及其他配置信息传递给嵌套的运行。"]}, {"cell_type": "code", "execution_count": 5, "id": "ff0daf0c-49dd-4d21-9772-e5fa133c5f36", "metadata": {}, "outputs": [{"name": "stdout", "output_type": "stream", "text": ["{'foo': 'bar'}\n", "Tokens Used: 62\n", "\tPrompt Tokens: 56\n", "\tCompletion Tokens: 6\n", "Successful Requests: 1\n", "Total Cost (USD): $9.6e-05\n"]}], "source": ["import json\n", "\n", "from langchain_core.runnables import RunnableConfig\n", "\n", "\n", "def parse_or_fix(text: str, config: RunnableConfig):\n", "    fixing_chain = (\n", "        ChatPromptTemplate.from_template(\n", "            \"Fix the following text:\\n\\n```text\\n{input}\\n```\\nError: {error}\"\n", "            \" Don't narrate, just respond with the fixed data.\"\n", "        )\n", "        | model\n", "        | StrOutputParser()\n", "    )\n", "    for _ in range(3):\n", "        try:\n", "            return json.loads(text)\n", "        except Exception as e:\n", "            text = fixing_chain.invoke({\"input\": text, \"error\": e}, config)\n", "    return \"Failed to parse\"\n", "\n", "\n", "from langchain_community.callbacks import get_openai_callback\n", "\n", "with get_openai_callback() as cb:\n", "    output = RunnableLambda(parse_or_fix).invoke(\n", "        \"{foo: bar}\", {\"tags\": [\"my-tag\"], \"callbacks\": [cb]}\n", "    )\n", "    print(output)\n", "    print(cb)"]}, {"cell_type": "code", "execution_count": 6, "id": "1a5e709e-9d75-48c7-bb9c-503251990505", "metadata": {}, "outputs": [{"name": "stdout", "output_type": "stream", "text": ["{'foo': 'bar'}\n", "Tokens Used: 62\n", "\tPrompt Tokens: 56\n", "\tCompletion Tokens: 6\n", "Successful Requests: 1\n", "Total Cost (USD): $9.6e-05\n"]}], "source": ["from langchain_community.callbacks import get_openai_callback\n", "\n", "with get_openai_callback() as cb:\n", "    output = RunnableLambda(parse_or_fix).invoke(\n", "        \"{foo: bar}\", {\"tags\": [\"my-tag\"], \"callbacks\": [cb]}\n", "    )\n", "    print(output)\n", "    print(cb)"]}, {"cell_type": "markdown", "id": "922b48bd", "metadata": {}, "source": ["## 流式传输", "\n", ":::note", "[RunnableLambda](https://python.langchain.com/api_reference/core/runnables/langchain_core.runnables.base.RunnableLambda.html) 最适合不需要支持流式处理的代码。如果需要支持流式处理（即能够操作输入块并生成输出块），请改用 [RunnableGenerator](https://python.langchain.com/api_reference/core/runnables/langchain_core.runnables.base.RunnableGenerator.html)，如下例所示。", ":::", "\n", "你可以将生成器函数（即使用 `yield` 关键字、行为类似迭代器的函数）以链式方式调用。", "\n", "这些生成器的签名应为 `Iterator[Input] -> Iterator[Output]`。对于异步生成器则为：`AsyncIterator[Input] -> AsyncIterator[Output]`。", "\n", "以下情况适用：", "- 实现自定义输出解析器", "- 在保留流式处理能力的同时，修改前一步骤的输出", "\n", "以下是一个针对逗号分隔列表的自定义输出解析器示例。首先，我们创建一个生成此类文本列表的链："]}, {"cell_type": "code", "execution_count": 7, "id": "29f55c38", "metadata": {}, "outputs": [{"name": "stdout", "output_type": "stream", "text": ["lion, tiger, wolf, gorilla, panda"]}], "source": ["from typing import Iterator, List\n", "\n", "prompt = ChatPromptTemplate.from_template(\n", "    \"Write a comma-separated list of 5 animals similar to: {animal}. Do not include numbers\"\n", ")\n", "\n", "str_chain = prompt | model | StrOutputParser()\n", "\n", "for chunk in str_chain.stream({\"animal\": \"bear\"}):\n", "    print(chunk, end=\"\", flush=True)"]}, {"cell_type": "markdown", "id": "46345323", "metadata": {}, "source": ["接下来，我们定义一个自定义函数，该函数将聚合当前流式传输的输出，并在模型生成列表中的下一个逗号时将其输出："]}, {"cell_type": "code", "execution_count": 8, "id": "f08b8a5b", "metadata": {}, "outputs": [{"name": "stdout", "output_type": "stream", "text": ["['lion']\n", "['tiger']\n", "['wolf']\n", "['gorilla']\n", "['raccoon']\n"]}], "source": ["# This is a custom parser that splits an iterator of llm tokens\n", "# into a list of strings separated by commas\n", "def split_into_list(input: Iterator[str]) -> Iterator[List[str]]:\n", "    # hold partial input until we get a comma\n", "    buffer = \"\"\n", "    for chunk in input:\n", "        # add current chunk to buffer\n", "        buffer += chunk\n", "        # while there are commas in the buffer\n", "        while \",\" in buffer:\n", "            # split buffer on comma\n", "            comma_index = buffer.index(\",\")\n", "            # yield everything before the comma\n", "            yield [buffer[:comma_index].strip()]\n", "            # save the rest for the next iteration\n", "            buffer = buffer[comma_index + 1 :]\n", "    # yield the last chunk\n", "    yield [buffer.strip()]\n", "\n", "\n", "list_chain = str_chain | split_into_list\n", "\n", "for chunk in list_chain.stream({\"animal\": \"bear\"}):\n", "    print(chunk, flush=True)"]}, {"cell_type": "markdown", "id": "0a5adb69", "metadata": {}, "source": ["调用它会返回完整的值数组："]}, {"cell_type": "code", "execution_count": 9, "id": "9ea4ddc6", "metadata": {}, "outputs": [{"data": {"text/plain": ["['lion', 'tiger', 'wolf', 'gorilla', 'raccoon']"]}, "execution_count": 9, "metadata": {}, "output_type": "execute_result"}], "source": ["list_chain.invoke({\"animal\": \"bear\"})"]}, {"cell_type": "markdown", "id": "96e320ed", "metadata": {}, "source": ["## 异步版本", "\n", "如果你在一个`async`环境中工作，以下是上述示例的`async`版本："]}, {"cell_type": "code", "execution_count": 10, "id": "569dbbef", "metadata": {}, "outputs": [{"name": "stdout", "output_type": "stream", "text": ["['lion']\n", "['tiger']\n", "['wolf']\n", "['gorilla']\n", "['panda']\n"]}], "source": ["from typing import AsyncIterator\n", "\n", "\n", "async def asplit_into_list(\n", "    input: AsyncIterator[str],\n", ") -> AsyncIterator[List[str]]:  # async def\n", "    buffer = \"\"\n", "    async for (\n", "        chunk\n", "    ) in input:  # `input` is a `async_generator` object, so use `async for`\n", "        buffer += chunk\n", "        while \",\" in buffer:\n", "            comma_index = buffer.index(\",\")\n", "            yield [buffer[:comma_index].strip()]\n", "            buffer = buffer[comma_index + 1 :]\n", "    yield [buffer.strip()]\n", "\n", "\n", "list_chain = str_chain | asplit_into_list\n", "\n", "async for chunk in list_chain.astream({\"animal\": \"bear\"}):\n", "    print(chunk, flush=True)"]}, {"cell_type": "code", "execution_count": 11, "id": "3a650482", "metadata": {}, "outputs": [{"data": {"text/plain": ["['lion', 'tiger', 'wolf', 'gorilla', 'panda']"]}, "execution_count": 11, "metadata": {}, "output_type": "execute_result"}], "source": ["await list_chain.ainvoke({\"animal\": \"bear\"})"]}, {"cell_type": "markdown", "id": "3306ac3b", "metadata": {}, "source": ["## 后续步骤", "\n", "现在你已经学会了在链中使用自定义逻辑的几种不同方法，以及如何实现流式处理。", "\n", "要了解更多信息，请参阅本节中关于可运行项的其他操作指南。"]}], "metadata": {"kernelspec": {"display_name": "Python 3 (ipykernel)", "language": "python", "name": "python3"}, "language_info": {"codemirror_mode": {"name": "ipython", "version": 3}, "file_extension": ".py", "mimetype": "text/x-python", "name": "python", "nbconvert_exporter": "python", "pygments_lexer": "ipython3", "version": "3.9.1"}}, "nbformat": 4, "nbformat_minor": 5}