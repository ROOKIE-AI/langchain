{"cells": [{"cell_type": "markdown", "id": "eaad9a82-0592-4315-9931-0621054bdd0e", "metadata": {}, "source": ["# 如何精简消息", "\n", ":::info 前提条件", "\n", "本指南假设您熟悉以下概念：", "\n", "- [消息](/docs/concepts/messages)", "- [聊天模型](/docs/concepts/chat_models)", "- [链式调用](/docs/how_to/sequence/)", "- [聊天记录](/docs/concepts/chat_history)", "\n", "本指南中的方法还需要 `langchain-core>=0.2.9`。", "\n", ":::", "\n", "所有模型都具备有限的上下文窗口，这意味着它们能接收的[标记](/docs/concepts/tokens/)输入量存在上限。若您处理超长消息，或使用会累积大量历史消息的链式结构/代理程序时，必须对传入模型的消息长度进行有效管理。", "\n", "[trim_messages](https://python.langchain.com/api_reference/core/messages/langchain_core.messages.utils.trim_messages.html) 可用于将聊天记录的大小缩减至指定的令牌数量或指定的消息数量。", "\n", "\n", "若要将修剪后的聊天历史直接传回聊天模型，修剪后的聊天历史应满足以下特性：", "\n", "1. 生成的聊天记录应当具备**有效性**。通常这意味着需要满足以下属性：", "- 聊天记录**始于**以下两种情况之一：(1) 一条 `HumanMessage` 或 (2) 一条 [SystemMessage](/docs/concepts/messages/#systemmessage) 后跟一条 `HumanMessage`。", "- 聊天历史以 `HumanMessage` 或 `ToolMessage` **结束**。", "- `ToolMessage` 只能在涉及工具调用的 `AIMessage` 之后出现。", "\n", "这可以通过设置 `start_on=\"human\"` 和 `ends_on=(\"human\", \"tool\")` 来实现。", "3. 它包含最近的聊天消息，并丢弃历史记录中的旧消息。", "这可以通过设置 `strategy=\"last\"` 来实现。", "4. 通常情况下，新的聊天记录应包含`SystemMessage`（系统消息）", "原始聊天记录中存在，因为`SystemMessage`包含", "对聊天模型的特殊指令。`SystemMessage`几乎总是", "历史记录中的第一条消息（如果存在）。这可以通过设置来实现", "`include_system=True`。"]}, {"cell_type": "markdown", "id": "e4bffc37-78c0-46c3-ad0c-b44de0ed3e90", "metadata": {}, "source": ["## 基于词元数量的剪裁", "\n", "在这里，我们将根据令牌数量对聊天记录进行裁剪。裁剪后的聊天记录将生成一个**有效的**聊天历史，其中包含`SystemMessage`（系统消息）。", "\n", "为了保留最新的消息，我们设置 `strategy=\"last\"`。同时设置 `include_system=True` 以包含 `SystemMessage`，并设置 `start_on=\"human\"` 以确保生成的聊天记录是有效的。", "\n", "这是一个基于令牌计数使用 `trim_messages` 时的良好默认配置。请记得根据你的使用场景调整 `token_counter` 和 `max_tokens`。", "\n", "请注意，对于我们的 `token_counter`，可以传入一个函数（下文将详细说明）或一个语言模型（因为语言模型具备消息令牌计数方法）。当您需要修剪消息以适应特定模型的上下文窗口时，传入该模型是合理的做法："]}, {"cell_type": "code", "execution_count": null, "id": "c9bed5ea-8aee-4d43-a717-77a431a02d2e", "metadata": {}, "outputs": [], "source": ["pip install -qU langchain-openai"]}, {"cell_type": "code", "execution_count": 2, "id": "40ea972c-d424-4bc4-9f2e-82f01c3d7598", "metadata": {}, "outputs": [{"data": {"text/plain": ["[SystemMessage(content=\"you're a good assistant, you always respond with a joke.\", additional_kwargs={}, response_metadata={}),\n", " HumanMessage(content='what do you call a speechless parrot', additional_kwargs={}, response_metadata={})]"]}, "execution_count": 2, "metadata": {}, "output_type": "execute_result"}], "source": ["from langchain_core.messages import (\n", "    AIMessage,\n", "    HumanMessage,\n", "    SystemMessage,\n", "    ToolMessage,\n", "    trim_messages,\n", ")\n", "from langchain_core.messages.utils import count_tokens_approximately\n", "\n", "messages = [\n", "    SystemMessage(\"you're a good assistant, you always respond with a joke.\"),\n", "    HumanMessage(\"i wonder why it's called langchain\"),\n", "    AIMessage(\n", "        'Well, I guess they thought \"WordRope\" and \"SentenceString\" just didn\\'t have the same ring to it!'\n", "    ),\n", "    HumanMessage(\"and who is harrison chasing anyways\"),\n", "    AIMessage(\n", "        \"Hmmm let me think.\\n\\nWhy, he's probably chasing after the last cup of coffee in the office!\"\n", "    ),\n", "    HumanMessage(\"what do you call a speechless parrot\"),\n", "]\n", "\n", "\n", "trim_messages(\n", "    messages,\n", "    # Keep the last <= n_count tokens of the messages.\n", "    strategy=\"last\",\n", "    # highlight-start\n", "    # Remember to adjust based on your model\n", "    # or else pass a custom token_counter\n", "    token_counter=count_tokens_approximately,\n", "    # highlight-end\n", "    # Most chat models expect that chat history starts with either:\n", "    # (1) a HumanMessage or\n", "    # (2) a SystemMessage followed by a HumanMessage\n", "    # highlight-start\n", "    # Remember to adjust based on the desired conversation\n", "    # length\n", "    max_tokens=45,\n", "    # highlight-end\n", "    # Most chat models expect that chat history starts with either:\n", "    # (1) a HumanMessage or\n", "    # (2) a SystemMessage followed by a HumanMessage\n", "    start_on=\"human\",\n", "    # Most chat models expect that chat history ends with either:\n", "    # (1) a HumanMessage or\n", "    # (2) a ToolMessage\n", "    end_on=(\"human\", \"tool\"),\n", "    # Usually, we want to keep the SystemMessage\n", "    # if it's present in the original history.\n", "    # The SystemMessage has special instructions for the model.\n", "    include_system=True,\n", "    allow_partial=False,\n", ")"]}, {"cell_type": "markdown", "id": "28fcfc94-0d4a-415c-9506-8ae7634253a2", "metadata": {}, "source": ["## 基于消息数量的修剪", "\n", "或者，我们可以基于**消息数量**来修剪聊天记录，通过设置 `token_counter=len`。在这种情况下，每条消息将被视为一个单独的 token，而 `max_tokens` 将控制", "最大消息数量。", "\n", "这是一个基于消息数量使用 `trim_messages` 时的良好默认配置。请根据实际使用场景调整 `max_tokens` 参数。"]}, {"cell_type": "code", "execution_count": 3, "id": "c8fdedae-0e6b-4901-a222-81fc95e265c2", "metadata": {}, "outputs": [{"data": {"text/plain": ["[SystemMessage(content=\"you're a good assistant, you always respond with a joke.\", additional_kwargs={}, response_metadata={}),\n", " HumanMessage(content='and who is harrison chasing anyways', additional_kwargs={}, response_metadata={}),\n", " AIMessage(content=\"Hmmm let me think.\\n\\nWhy, he's probably chasing after the last cup of coffee in the office!\", additional_kwargs={}, response_metadata={}),\n", " HumanMessage(content='what do you call a speechless parrot', additional_kwargs={}, response_metadata={})]"]}, "execution_count": 3, "metadata": {}, "output_type": "execute_result"}], "source": ["trim_messages(\n", "    messages,\n", "    # Keep the last <= n_count tokens of the messages.\n", "    strategy=\"last\",\n", "    # highlight-next-line\n", "    token_counter=len,\n", "    # When token_counter=len, each message\n", "    # will be counted as a single token.\n", "    # highlight-start\n", "    # Remember to adjust for your use case\n", "    max_tokens=5,\n", "    # highlight-end\n", "    # Most chat models expect that chat history starts with either:\n", "    # (1) a HumanMessage or\n", "    # (2) a SystemMessage followed by a HumanMessage\n", "    start_on=\"human\",\n", "    # Most chat models expect that chat history ends with either:\n", "    # (1) a HumanMessage or\n", "    # (2) a ToolMessage\n", "    end_on=(\"human\", \"tool\"),\n", "    # Usually, we want to keep the SystemMessage\n", "    # if it's present in the original history.\n", "    # The SystemMessage has special instructions for the model.\n", "    include_system=True,\n", ")"]}, {"attachments": {}, "cell_type": "markdown", "id": "9367857f-7f9a-4d17-9f9c-6ffc5aae909c", "metadata": {}, "source": ["## 高级用法", "\n", "你可以使用 `trim_messages` 作为基础构建模块，来创建更复杂的处理逻辑。", "\n", "如果我们希望允许拆分消息内容，可以指定 `allow_partial=True`："]}, {"cell_type": "code", "execution_count": 4, "id": "0265eba7-c8f3-4495-bcbb-17cd7ede3ece", "metadata": {}, "outputs": [{"data": {"text/plain": ["[SystemMessage(content=\"you're a good assistant, you always respond with a joke.\", additional_kwargs={}, response_metadata={}),\n", " AIMessage(content=\"\\nWhy, he's probably chasing after the last cup of coffee in the office!\", additional_kwargs={}, response_metadata={}),\n", " HumanMessage(content='what do you call a speechless parrot', additional_kwargs={}, response_metadata={})]"]}, "execution_count": 4, "metadata": {}, "output_type": "execute_result"}], "source": ["trim_messages(\n", "    messages,\n", "    max_tokens=56,\n", "    strategy=\"last\",\n", "    token_counter=count_tokens_approximately,\n", "    include_system=True,\n", "    allow_partial=True,\n", ")"]}, {"cell_type": "markdown", "id": "245bee9b-e515-4e89-8f2a-84bda9a25de8", "metadata": {}, "source": ["默认情况下，`SystemMessage`不会被包含在内，因此您可以通过设置`include_system=False`或直接省略`include_system`参数来排除它。"]}, {"cell_type": "code", "execution_count": 5, "id": "94351736-28a1-44a3-aac7-82356c81d171", "metadata": {}, "outputs": [{"data": {"text/plain": ["[AIMessage(content=\"Hmmm let me think.\\n\\nWhy, he's probably chasing after the last cup of coffee in the office!\", additional_kwargs={}, response_metadata={}),\n", " HumanMessage(content='what do you call a speechless parrot', additional_kwargs={}, response_metadata={})]"]}, "execution_count": 5, "metadata": {}, "output_type": "execute_result"}], "source": ["trim_messages(\n", "    messages,\n", "    max_tokens=45,\n", "    strategy=\"last\",\n", "    token_counter=count_tokens_approximately,\n", ")"]}, {"cell_type": "markdown", "id": "7f5d391d-235b-4091-b2de-c22866b478f3", "metadata": {}, "source": ["我们可以通过指定 `strategy=\"first\"` 来执行获取 *前* `max_tokens` 个标记的反向操作："]}, {"cell_type": "code", "execution_count": 6, "id": "5f56ae54-1a39-4019-9351-3b494c003d5b", "metadata": {}, "outputs": [{"data": {"text/plain": ["[SystemMessage(content=\"you're a good assistant, you always respond with a joke.\", additional_kwargs={}, response_metadata={}),\n", " HumanMessage(content=\"i wonder why it's called langchain\", additional_kwargs={}, response_metadata={})]"]}, "execution_count": 6, "metadata": {}, "output_type": "execute_result"}], "source": ["trim_messages(\n", "    messages,\n", "    max_tokens=45,\n", "    strategy=\"first\",\n", "    token_counter=count_tokens_approximately,\n", ")"]}, {"cell_type": "markdown", "id": "0625c094-380f-4485-b2d2-e5dfa83fe299", "metadata": {}, "source": ["## 使用 `ChatModel` 作为令牌计数器", "\n", "你可以将 ChatModel 作为令牌计数器使用。这将调用 `ChatModel.get_num_tokens_from_messages` 方法。以下演示如何配合 OpenAI 使用该功能："]}, {"cell_type": "code", "execution_count": 7, "id": "9ef35359-1b7a-4918-ab41-30bec69fb3dc", "metadata": {}, "outputs": [{"data": {"text/plain": ["[SystemMessage(content=\"you're a good assistant, you always respond with a joke.\", additional_kwargs={}, response_metadata={}),\n", " HumanMessage(content=\"i wonder why it's called langchain\", additional_kwargs={}, response_metadata={})]"]}, "execution_count": 7, "metadata": {}, "output_type": "execute_result"}], "source": ["from langchain_openai import ChatOpenAI\n", "\n", "trim_messages(\n", "    messages,\n", "    max_tokens=45,\n", "    strategy=\"first\",\n", "    token_counter=ChatOpenAI(model=\"gpt-4o\"),\n", ")"]}, {"cell_type": "markdown", "id": "ab70bf70-1e5a-4d51-b9b8-a823bf2cf532", "metadata": {}, "source": ["## 编写自定义令牌计数器", "\n", "我们可以编写一个自定义的令牌计数器函数，该函数接收消息列表并返回一个整数。"]}, {"cell_type": "code", "execution_count": 8, "id": "d930c089-e8e6-4980-9d39-11d41e794772", "metadata": {}, "outputs": [], "source": ["pip install -qU tiktoken"]}, {"cell_type": "code", "execution_count": 9, "id": "1c1c3b1e-2ece-49e7-a3b6-e69877c1633b", "metadata": {}, "outputs": [{"data": {"text/plain": ["[SystemMessage(content=\"you're a good assistant, you always respond with a joke.\", additional_kwargs={}, response_metadata={}),\n", " HumanMessage(content='what do you call a speechless parrot', additional_kwargs={}, response_metadata={})]"]}, "execution_count": 9, "metadata": {}, "output_type": "execute_result"}], "source": ["from typing import List\n", "\n", "import tiktoken\n", "from langchain_core.messages import BaseMessage, ToolMessage\n", "\n", "\n", "def str_token_counter(text: str) -> int:\n", "    enc = tiktoken.get_encoding(\"o200k_base\")\n", "    return len(enc.encode(text))\n", "\n", "\n", "def tiktoken_counter(messages: List[BaseMessage]) -> int:\n", "    \"\"\"Approximately reproduce https://github.com/openai/openai-cookbook/blob/main/examples/How_to_count_tokens_with_tiktoken.ipynb\n", "\n", "    For simplicity only supports str Message.contents.\n", "    \"\"\"\n", "    num_tokens = 3  # every reply is primed with <|start|>assistant<|message|>\n", "    tokens_per_message = 3\n", "    tokens_per_name = 1\n", "    for msg in messages:\n", "        if isinstance(msg, HumanMessage):\n", "            role = \"user\"\n", "        elif isinstance(msg, AIMessage):\n", "            role = \"assistant\"\n", "        elif isinstance(msg, ToolMessage):\n", "            role = \"tool\"\n", "        elif isinstance(msg, SystemMessage):\n", "            role = \"system\"\n", "        else:\n", "            raise ValueError(f\"Unsupported messages type {msg.__class__}\")\n", "        num_tokens += (\n", "            tokens_per_message\n", "            + str_token_counter(role)\n", "            + str_token_counter(msg.content)\n", "        )\n", "        if msg.name:\n", "            num_tokens += tokens_per_name + str_token_counter(msg.name)\n", "    return num_tokens\n", "\n", "\n", "trim_messages(\n", "    messages,\n", "    # highlight-next-line\n", "    token_counter=tiktoken_counter,\n", "    # Keep the last <= n_count tokens of the messages.\n", "    strategy=\"last\",\n", "    # When token_counter=len, each message\n", "    # will be counted as a single token.\n", "    # highlight-start\n", "    # Remember to adjust for your use case\n", "    max_tokens=45,\n", "    # highlight-end\n", "    # Most chat models expect that chat history starts with either:\n", "    # (1) a HumanMessage or\n", "    # (2) a SystemMessage followed by a HumanMessage\n", "    start_on=\"human\",\n", "    # Most chat models expect that chat history ends with either:\n", "    # (1) a HumanMessage or\n", "    # (2) a ToolMessage\n", "    end_on=(\"human\", \"tool\"),\n", "    # Usually, we want to keep the SystemMessage\n", "    # if it's present in the original history.\n", "    # The SystemMessage has special instructions for the model.\n", "    include_system=True,\n", ")"]}, {"cell_type": "markdown", "id": "4b2a672b-c007-47c5-9105-617944dc0a6a", "metadata": {}, "source": ["## 链式调用", "\n", "`trim_messages` 既可通过命令式（如上所示）也可通过声明式使用，便于在链式结构中与其他组件组合。"]}, {"cell_type": "code", "execution_count": 10, "id": "96aa29b2-01e0-437c-a1ab-02fb0141cb57", "metadata": {}, "outputs": [{"data": {"text/plain": ["AIMessage(content='A \"polly-no-wanna-cracker\"!', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 11, 'prompt_tokens': 32, 'total_tokens': 43, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-4o-2024-08-06', 'system_fingerprint': 'fp_90d33c15d4', 'finish_reason': 'stop', 'logprobs': None}, id='run-b1f8b63b-6bc2-4df4-b3b9-dfc4e3e675fe-0', usage_metadata={'input_tokens': 32, 'output_tokens': 11, 'total_tokens': 43, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}})"]}, "execution_count": 10, "metadata": {}, "output_type": "execute_result"}], "source": ["llm = ChatOpenAI(model=\"gpt-4o\")\n", "\n", "# Notice we don't pass in messages. This creates\n", "# a RunnableLambda that takes messages as input\n", "trimmer = trim_messages(\n", "    token_counter=llm,\n", "    # Keep the last <= n_count tokens of the messages.\n", "    strategy=\"last\",\n", "    # When token_counter=len, each message\n", "    # will be counted as a single token.\n", "    # Remember to adjust for your use case\n", "    max_tokens=45,\n", "    # Most chat models expect that chat history starts with either:\n", "    # (1) a HumanMessage or\n", "    # (2) a SystemMessage followed by a HumanMessage\n", "    start_on=\"human\",\n", "    # Most chat models expect that chat history ends with either:\n", "    # (1) a HumanMessage or\n", "    # (2) a ToolMessage\n", "    end_on=(\"human\", \"tool\"),\n", "    # Usually, we want to keep the SystemMessage\n", "    # if it's present in the original history.\n", "    # The SystemMessage has special instructions for the model.\n", "    include_system=True,\n", ")\n", "\n", "chain = trimmer | llm\n", "chain.invoke(messages)"]}, {"cell_type": "markdown", "id": "4d91d390-e7f7-467b-ad87-d100411d7a21", "metadata": {}, "source": ["查看LangSmith跟踪记录可以发现，在消息传递给模型之前会先进行裁剪处理：https://smith.langchain.com/public/65af12c4-c24d-4824-90f0-6547566e59bb/r", "\n", "仅看修剪器部分，我们可以发现它是一个可运行对象（Runnable），能像所有可运行对象一样被调用："]}, {"cell_type": "code", "execution_count": 11, "id": "1ff02d0a-353d-4fac-a77c-7c2c5262abd9", "metadata": {}, "outputs": [{"data": {"text/plain": ["[SystemMessage(content=\"you're a good assistant, you always respond with a joke.\", additional_kwargs={}, response_metadata={}),\n", " HumanMessage(content='what do you call a speechless parrot', additional_kwargs={}, response_metadata={})]"]}, "execution_count": 11, "metadata": {}, "output_type": "execute_result"}], "source": ["trimmer.invoke(messages)"]}, {"cell_type": "markdown", "id": "dc4720c8-4062-4ebc-9385-58411202ce6e", "metadata": {}, "source": ["## 与 ChatMessageHistory 配合使用", "\n", "在处理可能无限增长的[聊天记录](/docs/how_to/message_history/)时，消息截断功能尤为实用："]}, {"cell_type": "code", "execution_count": 11, "id": "a9517858-fc2f-4dc3-898d-bf98a0e905a0", "metadata": {}, "outputs": [{"data": {"text/plain": ["AIMessage(content='A \"polygon\"!', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 4, 'prompt_tokens': 32, 'total_tokens': 36, 'completion_tokens_details': {'reasoning_tokens': 0}}, 'model_name': 'gpt-4o-2024-05-13', 'system_fingerprint': 'fp_c17d3befe7', 'finish_reason': 'stop', 'logprobs': None}, id='run-71d9fce6-bb0c-4bb3-acc8-d5eaee6ae7bc-0', usage_metadata={'input_tokens': 32, 'output_tokens': 4, 'total_tokens': 36})"]}, "execution_count": 11, "metadata": {}, "output_type": "execute_result"}], "source": ["from langchain_core.chat_history import InMemoryChatMessageHistory\n", "from langchain_core.runnables.history import RunnableWithMessageHistory\n", "\n", "chat_history = InMemoryChatMessageHistory(messages=messages[:-1])\n", "\n", "\n", "def dummy_get_session_history(session_id):\n", "    if session_id != \"1\":\n", "        return InMemoryChatMessageHistory()\n", "    return chat_history\n", "\n", "\n", "trimmer = trim_messages(\n", "    max_tokens=45,\n", "    strategy=\"last\",\n", "    token_counter=llm,\n", "    # Usually, we want to keep the SystemMessage\n", "    # if it's present in the original history.\n", "    # The SystemMessage has special instructions for the model.\n", "    include_system=True,\n", "    # Most chat models expect that chat history starts with either:\n", "    # (1) a HumanMessage or\n", "    # (2) a SystemMessage followed by a HumanMessage\n", "    # start_on=\"human\" makes sure we produce a valid chat history\n", "    start_on=\"human\",\n", ")\n", "\n", "chain = trimmer | llm\n", "chain_with_history = RunnableWithMessageHistory(chain, dummy_get_session_history)\n", "chain_with_history.invoke(\n", "    [HumanMessage(\"what do you call a speechless parrot\")],\n", "    config={\"configurable\": {\"session_id\": \"1\"}},\n", ")"]}, {"cell_type": "markdown", "id": "556b7b4c-43cb-41de-94fc-1a41f4ec4d2e", "metadata": {}, "source": ["查看LangSmith跟踪记录可以发现，我们检索了所有消息，但在传递给模型之前，这些消息被精简为仅包含系统消息和最后一条人类消息：https://smith.langchain.com/public/17dd700b-9994-44ca-930c-116e00997315/r"]}, {"cell_type": "markdown", "id": "75dc7b84-b92f-44e7-8beb-ba22398e4efb", "metadata": {}, "source": ["## API 参考", "\n", "要查看所有参数的完整描述，请前往API参考文档：[https://python.langchain.com/api_reference/core/messages/langchain_core.messages.utils.trim_messages.html](https://python.langchain.com/api_reference/core/messages/langchain_core.messages.utils.trim_messages.html)"]}], "metadata": {"kernelspec": {"display_name": "Python 3 (ipykernel)", "language": "python", "name": "python3"}, "language_info": {"codemirror_mode": {"name": "ipython", "version": 3}, "file_extension": ".py", "mimetype": "text/x-python", "name": "python", "nbconvert_exporter": "python", "pygments_lexer": "ipython3", "version": "3.12.3"}}, "nbformat": 4, "nbformat_minor": 5}